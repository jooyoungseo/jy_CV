@online{2024_microsoft,
  title        = {Seeing {{AI}} - {{An}} App for Visually Impaired People That Narrates the World around You},
  author       = {{Microsoft}},
  date         = {2024},
  url          = {https://www.microsoft.com/en-us/garage/wall-of-fame/seeing-ai/},
  urldate      = {2024-04-22},
  langid       = {american},
  organization = {Microsoft Garage},
  file         = {C:\Users\jseo1005\Zotero\storage\IXGSV3QI\seeing-ai.html}
}

@online{accessibilityatpennstateChartsAccessibility2014,
  title    = {Charts \& {{Accessibility}}},
  author   = {{Accessibility at Penn State}},
  date     = {2014},
  url      = {https://accessibility.psu.edu/images/charts/},
  urldate  = {2024-04-21},
  abstract = {Page Content Synopsis Text Description Repeat Data in Tables Color in Charts Charts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide se…},
  langid   = {american},
  file     = {C:\Users\jseo1005\Zotero\storage\T2Z3QREU\charts.html}
}

@online{AccessibleCOVID19Data,
  title   = {Accessible {{COVID-19}} Data},
  url     = {https://covid.ski.org/?fbclid=IwAR0kqAZIeQkyelOjMpRA_NrKVM8gKYGEVSZeFgT0vSe61f8aLE0z4oB8DzI},
  urldate = {2022-08-21},
  file    = {C:\Users\jseo1005\Zotero\storage\D2ILW7QE\covid.ski.org.html}
}

@online{AccessibleGraphs,
  title    = {Accessible {{Graphs}}},
  url      = {https://accessiblegraphs.org/},
  urldate  = {2023-09-11},
  abstract = {Helping blind people see graphs using sound and touch},
  langid   = {english},
  file     = {C:\Users\jseo1005\Zotero\storage\DST4J3U2\accessiblegraphs.org.html}
}

@software{AccessibleGraphsProject2023,
  title        = {The {{Accessible Graphs}} Project},
  date         = {2023-01-17T09:12:00Z},
  origdate     = {2019-10-10T21:52:42Z},
  url          = {https://github.com/hasadna/accessible-graphs},
  urldate      = {2023-09-11},
  abstract     = {The Accessible Graphs project},
  organization = {The Public Knowledge Workshop}
}

@article{ACMDSeminarFully2017,
  title        = {{{ACMD Seminar}}: {{Towards Fully Accessible Data Visualisation}}},
  shorttitle   = {{{ACMD Seminar}}},
  date         = {2017-06-06T09:43-04:00},
  journaltitle = {NIST},
  url          = {https://www.nist.gov/itl/math/acmd-seminar-towards-fully-accessible-data-visualisation},
  urldate      = {2022-08-21},
  abstract     = {Volker SorgeSchool of Computer Science, University of Birmingham, UK},
  langid       = {english},
  annotation   = {Last Modified: 2019-11-15T19:42-05:00},
  file         = {C:\Users\jseo1005\Zotero\storage\8ZZW8M5Y\acmd-seminar-towards-fully-accessible-data-visualisation.html}
}

@online{airaWeReAira2024,
  title    = {We’re {{Aira}}, a {{Visual Interpreting Service}}.},
  author   = {{Aira}},
  date     = {2024},
  url      = {https://airaio.kinsta.cloud/},
  urldate  = {2024-04-22},
  abstract = {Aira is live, on-demand visual interpreting, a productivity tool that connects you to actual humans who describe your visual surroundings using your smartphone.},
  langid   = {american},
  file     = {C:\Users\jseo1005\Zotero\storage\W63UZR8X\aira.io.html}
}

@article{alamEnablingAccessibleCharts2022,
  title    = {Enabling {{Accessible Charts Through Interactive Natural Language Interface}} for {{People}} with {{Visual Impairments}}},
  author   = {Alam, Md Zubair Ibne},
  date     = {2022-11-18},
  url      = {http://hdl.handle.net/10315/40987},
  urldate  = {2024-03-25},
  abstract = {Web-based data visualizations have become very popular for exploring data and communicating insights. Newspapers, journals, and reports regularly publish visualizations to tell compelling stories with data. Unfortunately, most visualizations are inaccessible to readers with visual impairments. For many charts on the web, there are no accompanying alternative (alt) texts, and even if such texts exist they do not adequately describe important insights from charts. To address the problem, we first interviewed 15 blind users to understand their challenges and requirements for reading data visualizations. Based on the insights from these interviews, we developed \textbackslash seechart, an interactive tool that automatically deconstructs charts from web pages and then converts them to accessible visualizations for blind people by enabling them to hear the chart summary as well as to interact through data points using the keyboard. Our evaluation with 14 blind participants suggests the efficacy of SeeChart in understanding key insights from charts and fulfilling their information needs while reducing their required time and cognitive burden.},
  langid   = {english},
  file     = {C:\Users\jseo1005\Zotero\storage\BJ87T5GI\Alam - 2022 - Enabling Accessible Charts Through Interactive Nat.pdf}
}

@online{alamSeeChartEnablingAccessible2023,
  title       = {{{SeeChart}}: {{Enabling Accessible Visualizations Through Interactive Natural Language Interface For People}} with {{Visual Impairments}}},
  shorttitle  = {{{SeeChart}}},
  author      = {Alam, Md Zubair Ibne and Islam, Shehnaz and Hoque, Enamul},
  date        = {2023-02-15},
  eprint      = {2302.07742},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  doi         = {10.1145/3581641.3584099},
  url         = {http://arxiv.org/abs/2302.07742},
  urldate     = {2023-02-16},
  abstract    = {Web-based data visualizations have become very popular for exploring data and communicating insights. Newspapers, journals, and reports regularly publish visualizations to tell compelling stories with data. Unfortunately, most visualizations are inaccessible to readers with visual impairments. For many charts on the web, there are no accompanying alternative (alt) texts, and even if such texts exist they do not adequately describe important insights from charts. To address the problem, we first interviewed 15 blind users to understand their challenges and requirements for reading data visualizations. Based on the insights from these interviews, we developed SeeChart, an interactive tool that automatically deconstructs charts from web pages and then converts them to accessible visualizations for blind people by enabling them to hear the chart summary as well as to interact through data points using the keyboard. Our evaluation with 14 blind participants suggests the efficacy of SeeChart in understanding key insights from charts and fulfilling their information needs while reducing their required time and cognitive burden.},
  pubstate    = {preprint},
  keywords    = {Computer Science - Human-Computer Interaction},
  file        = {C:\Users\jseo1005\Zotero\storage\6M9NTXPQ\Alam et al. - 2023 - SeeChart Enabling Accessible Visualizations Throu.pdf}
}

@online{americanprintinghousefortheblindAnnualReports2021,
  title  = {Annual {{Reports}}},
  author = {{American Printing House for the Blind}},
  date   = {2021},
  url    = {https://www.aph.org/app/uploads/2022/04/annual-report-fy2021.pdf},
  langid = {english},
  file   = {C:\Users\jseo1005\Zotero\storage\UU2U8PQE\Limitless Possibility.pdf}
}

@online{anthropicIntroducingNextGeneration2024,
  title    = {Introducing the next Generation of {{Claude}}},
  author   = {{Anthropic}},
  date     = {2024},
  url      = {https://www.anthropic.com/news/claude-3-family},
  urldate  = {2024-04-20},
  abstract = {Today, we're announcing the Claude 3 model family, which sets new industry benchmarks across a wide range of cognitive tasks. The family includes three state-of-the-art models in ascending order of capability: Claude 3 Haiku, Claude 3 Sonnet, and Claude 3 Opus.},
  langid   = {english},
  file     = {C:\Users\jseo1005\Zotero\storage\XGZNJTIL\claude-3-family.html}
}

@online{AudioGraphsApple,
  title   = {Audio {{Graphs}} | {{Apple Developer Documentation}}},
  url     = {https://developer.apple.com/documentation/accessibility/audio_graphs},
  urldate = {2022-08-21},
  file    = {C:\Users\jseo1005\Zotero\storage\3Y4533ZN\audio_graphs.html}
}

@inproceedings{aultEvaluationLongDescriptions2002,
  title     = {Evaluation of {{Long Descriptions}} of {{Statistical Graphics}} for {{Blind}} and {{Low Vision Web Users}}},
  booktitle = {Computers {{Helping People}} with {{Special Needs}}},
  author    = {Ault, H. K. and Deloge, J. W. and Lapp, R. W. and Morgan, M. J. and Barnett, J. R.},
  editor    = {Miesenberger, Klaus and Klaus, Joachim and Zagler, Wolfgang},
  date      = {2002},
  series    = {Lecture {{Notes}} in {{Computer Science}}},
  pages     = {517--526},
  publisher = {Springer},
  location  = {Berlin, Heidelberg},
  doi       = {10.1007/3-540-45491-8_99},
  abstract  = {The objective of this research was to maximize not only accessibility but also user comprehension of web pages, particularly those containing tabular and graphical information. Based on literature and interviews with blind and low vision students and their teachers, the research team developed guidelines for web developers to describe charts and graphs commonly used in statistical applications. A usability study was then performed to evaluate the effectiveness of these new guidelines. Accessibility and comprehension for both blind and low vision users were increased when web pages were developed following the new guidelines.},
  isbn      = {978-3-540-45491-5},
  langid    = {english},
  keywords  = {Accessibility Guideline,Blind User,Lesson Plan,Screen Reader,Worcester Polytechnic Institute},
  file      = {C:\Users\jseo1005\Zotero\storage\ZPH9N8YJ\3-540-45491-8_99.pdf}
}

@article{bachChallengesOpportunitiesData2023,
  title        = {Challenges and {{Opportunities}} in {{Data Visualization Education}}: {{A Call}} to {{Action}}},
  shorttitle   = {Challenges and {{Opportunities}} in {{Data Visualization Education}}},
  author       = {Bach, Benjamin and Keck, Mandy and Rajabiyazdi, Fateme and Losev, Tatiana and Meirelles, Isabel and Dykes, Jason and Laramee, Robert S. and AlKadi, Mashael and Stoiber, Christina and Huron, Samuel and Perin, Charles and Morais, Luiz and Aigner, Wolfgang and Kosminsky, Doris and Boucher, Magdalena and Knudsen, Søren and Manataki, Areti and Aerts, Jan and Hinrichs, Uta and Roberts, Jonathan C. and Carpendale, Sheelagh},
  date         = {2023},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  shortjournal = {IEEE Trans. Visual. Comput. Graphics},
  pages        = {1--12},
  issn         = {1077-2626, 1941-0506, 2160-9306},
  doi          = {10.1109/TVCG.2023.3327378},
  url          = {https://ieeexplore.ieee.org/document/10310184/},
  urldate      = {2024-01-15},
  langid       = {english},
  file         = {C:\Users\jseo1005\Zotero\storage\VE6QQJDA\Bach et al. - 2023 - Challenges and Opportunities in Data Visualization Education A Call to Action.pdf}
}

@article{bacieroTouchScopePassiveHapticDevice,
  title        = {{{TouchScope}}: {{A Passive-Haptic Device}} to {{Investigate Tactile Perception Using}} a {{Refreshable Braille Display}}},
  shorttitle   = {{{TouchScope}}},
  author       = {Baciero, Ana and Perea, Manuel and Duñabeitia, Jon Andoni and Gómez, Pablo},
  journaltitle = {Journal of Cognition},
  shortjournal = {J Cogn},
  volume       = {6},
  number       = {1},
  eprint       = {37152833},
  eprinttype   = {pmid},
  pages        = {21},
  issn         = {2514-4820},
  doi          = {10.5334/joc.271},
  url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10162198/},
  urldate      = {2023-11-14},
  abstract     = {The sense of touch is underrepresented in cognitive psychology research. One of the reasons is that controlling the timing of stimulus presentation, which is a hallmark of cognitive research, is significantly more difficult for tactile stimuli than visual or auditory stimuli. In the present work, we present a system to display tactile stimuli (braille cells) and collect response time with the capability for static and dynamic (passive haptic) stimuli prsentation that will contribute to the development of tactile research. While the system requires some construction, it can be put together with commercially available materials. Here, we present the step-by-step instructions for constructing the tool, the code used to control it, and some basic experiments to validate it. The data from the experiments show that the device can be used for a variety of tactile perception experiments.},
  pmcid        = {PMC10162198},
  file         = {C:\Users\jseo1005\Zotero\storage\SQ7V4KRS\Baciero et al. - TouchScope A Passive-Haptic Device to Investigate Tactile Perception Using a Refreshable Braille Di.pdf}
}

@inproceedings{bakerEducationalExperiencesBlind2019,
  title      = {Educational {{Experiences}} of {{Blind Programmers}}},
  booktitle  = {Proceedings of the 50th {{ACM Technical Symposium}} on {{Computer Science Education}}},
  author     = {Baker, Catherine M. and Bennett, Cynthia L. and Ladner, Richard E.},
  date       = {2019-02-22},
  pages      = {759--765},
  publisher  = {ACM},
  location   = {Minneapolis MN USA},
  doi        = {10.1145/3287324.3287410},
  url        = {https://dl.acm.org/doi/10.1145/3287324.3287410},
  urldate    = {2024-04-04},
  eventtitle = {{{SIGCSE}} '19: {{The}} 50th {{ACM Technical Symposium}} on {{Computer Science Education}}},
  isbn       = {978-1-4503-5890-3},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\L6DWQQVW\Baker et al. - 2019 - Educational Experiences of Blind Programmers.pdf}
}

@thesis{bakerUnderstandingImprovingBlind2017,
  type       = {Thesis},
  title      = {Understanding and {{Improving Blind Students}}’ {{Access}} to {{Visual Information}} in {{Computer Science Education}}},
  author     = {Baker, Catherine Marie},
  date       = {2017-08},
  url        = {https://digital.lib.washington.edu:443/researchworks/handle/1773/40540},
  urldate    = {2024-04-14},
  abstract   = {Teaching people with disabilities tech skills empowers them to create solutions to problems they encounter and prepares them for careers. However, computer science is typically taught in a highly visual manner which can present barriers for people who are blind. The goal of this dissertation is to understand and decrease those barriers. The first projects I present looked at the barriers that blind students face. I first present the results of my survey and interviews with blind students with degrees in computer science or related fields. This work highlighted the many barriers that these blind students faced. I then followed-up on one of the barriers mentioned, access to technology, by doing a preliminary accessibility evaluation of six popular integrated development environments (IDEs) and code editors. I found that half were unusable and all had some inaccessible portions. As access to visual information is a barrier in computer science education, I present three projects I have done to decrease this barrier. The first project is Tactile Graphics with a Voice (TGV). This project investigated an alternative to Braille labels for those who do not know Braille and showed that TGV was a potential alternative. The next project was StructJumper, which created a modified abstract syntax tree that blind programmers could use to navigate through code with their screen reader. The evaluation showed that users could navigate more quickly and easily determine the relationships of lines of code when they were using StructJumper compared to when they were not. Finally, I present a tool for dynamic graphs (the type with nodes and edges) which had two different modes for handling focus changes when moving between graphs. I found that the modes support different approaches for exploring the graphs and therefore preferences are mixed based on the user’s preferred approach. However, both modes had similar accuracy in completing the tasks. These projects are a first step towards the goal of making computer science education more accessible to blind students. By identifying the barriers that exist and creating solutions to overcome them, we can support increasing the number of blind students in computer science.},
  langid     = {american},
  annotation = {Accepted: 2017-10-26T20:48:55Z},
  file       = {C:\Users\jseo1005\Zotero\storage\A4MQ5YMJ\Baker - 2017 - Understanding and Improving Blind Students’ Access to Visual Information in Computer Science Educati.pdf}
}

@article{baldwinTangibleDesktopMultimodal2017,
  title        = {The {{Tangible Desktop}}: {{A Multimodal Approach}} to {{Nonvisual Computing}}},
  shorttitle   = {The {{Tangible Desktop}}},
  author       = {Baldwin, Mark S. and Hayes, Gillian R. and Haimson, Oliver L. and Mankoff, Jennifer and Hudson, Scott E.},
  date         = {2017-08-11},
  journaltitle = {ACM Transactions on Accessible Computing},
  shortjournal = {ACM Trans. Access. Comput.},
  volume       = {10},
  number       = {3},
  pages        = {9:1--9:28},
  issn         = {1936-7228},
  doi          = {10.1145/3075222},
  url          = {https://dl.acm.org/doi/10.1145/3075222},
  urldate      = {2023-11-28},
  abstract     = {Audio-only interfaces, facilitated through text-to-speech screen reading software, have been the primary mode of computer interaction for blind and low-vision computer users for more than four decades. During this time, the advances that have made visual interfaces faster and easier to use, from direct manipulation to skeuomorphic design, have not been paralleled in nonvisual computing environments. The screen reader–dependent community is left with no alternatives to engage with our rapidly advancing technological infrastructure. In this article, we describe our efforts to understand the problems that exist with audio-only interfaces. Based on observing screen reader use for 4 months at a computer training school for blind and low-vision adults, we identify three problem areas within audio-only interfaces: ephemerality, linear interaction, and unidirectional communication. We then evaluated a multimodal approach to computer interaction called the Tangible Desktop that addresses these problems by moving semantic information from the auditory to the tactile channel. Our evaluation demonstrated that among novice screen reader users, Tangible Desktop improved task completion times by an average of 6 minutes when compared to traditional audio-only computer systems.},
  keywords     = {Accessibility,assistive technology,blindness,haptic,hardware,tangible,vibrotactile feedback,visual impairment},
  file         = {C:\Users\jseo1005\Zotero\storage\DEU8ER2J\Baldwin et al. - 2017 - The Tangible Desktop A Multimodal Approach to Nonvisual Computing.pdf}
}

@online{bangMultitaskMultilingualMultimodal2023,
  title       = {A {{Multitask}}, {{Multilingual}}, {{Multimodal Evaluation}} of {{ChatGPT}} on {{Reasoning}}, {{Hallucination}}, and {{Interactivity}}},
  author      = {Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and Do, Quyet V. and Xu, Yan and Fung, Pascale},
  date        = {2023-11-28},
  eprint      = {2302.04023},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2302.04023},
  url         = {http://arxiv.org/abs/2302.04023},
  urldate     = {2024-04-20},
  abstract    = {This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 23 data sets covering 8 different common NLP application tasks. We evaluate the multitask, multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that ChatGPT is 63.41\% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. It is, for example, better at deductive than inductive reasoning. ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8\% ROUGE-1 on summarization and 2\% ChrF++ on machine translation, in a multi-turn "prompt engineering" fashion. We also release codebase for evaluation set extraction.},
  pubstate    = {preprint},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file        = {C:\Users\jseo1005\Zotero\storage\NAWSTLMS\2302.html}
}

@inproceedings{banovicUncoveringInformationNeeds2013,
  title      = {Uncovering Information Needs for Independent Spatial Learning for Users Who Are Visually Impaired},
  booktitle  = {Proceedings of the 15th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author     = {Banovic, Nikola and Franz, Rachel L. and Truong, Khai N. and Mankoff, Jennifer and Dey, Anind K.},
  date       = {2013-10-21},
  pages      = {1--8},
  publisher  = {ACM},
  location   = {Bellevue Washington},
  doi        = {10.1145/2513383.2513445},
  url        = {https://dl.acm.org/doi/10.1145/2513383.2513445},
  urldate    = {2022-08-21},
  eventtitle = {{{ASSETS}} '13: {{The}} 15th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  isbn       = {978-1-4503-2405-2},
  langid     = {english}
}

@inproceedings{belleAltTexifyPipelineGenerate2022b,
  title      = {Alt-{{Texify}}: {{A Pipeline}} to {{Generate Alt-text}} from {{SVG Visualizations}}:},
  shorttitle = {Alt-{{Texify}}},
  booktitle  = {Proceedings of the 17th {{International Conference}} on {{Evaluation}} of {{Novel Approaches}} to {{Software Engineering}}},
  author     = {Belle, Aspen and Goh, Vanessa and Kumar, Akshay and Pranjatno, Richard and Yip, Pui and Wickramaratne, Umayangani and Obie, Humphrey},
  date       = {2022},
  pages      = {275--281},
  publisher  = {{SCITEPRESS - Science and Technology Publications}},
  location   = {Online Streaming, --- Select a Country ---},
  doi        = {10.5220/0010994600003176},
  url        = {https://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0010994600003176},
  urldate    = {2024-02-08},
  eventtitle = {17th {{International Conference}} on {{Evaluation}} of {{Novel Approaches}} to {{Software Engineering}}},
  isbn       = {978-989-758-568-5},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\G6ZTBCIH\Belle et al. - 2022 - Alt-Texify A Pipeline to Generate Alt-text from S.pdf}
}

@online{bemyeyesAnnouncingBeMy2023,
  title    = {Announcing ‘{{Be My AI}},’ {{Soon Available}} for {{Hundreds}} of {{Thousands}} of {{Be My Eyes Users}}},
  author   = {{Be My Eyes}},
  date     = {2023},
  url      = {https://www.bemyeyes.com/blog/announcing-be-my-ai},
  urldate  = {2024-04-20},
  abstract = {Be My Eyes’ AI assistant, powered by GPT-4, is rolling out to hundreds of thousands of iOS and Android users over the next several weeks.},
  langid   = {english},
  file     = {C:\Users\jseo1005\Zotero\storage\ZUAL4S9B\announcing-be-my-ai.html}
}

@online{bemyeyesBeMyEyes2024,
  title    = {Be {{My Eyes}} - {{See}} the World Together},
  author   = {{Be My Eyes}},
  date     = {2024},
  url      = {https://www.bemyeyes.com/},
  urldate  = {2024-04-22},
  abstract = {Whether you need a pair of sharp eyes or have some sight to lend, Be My Eyes is a simple, free tool to support people see the world better, together.},
  langid   = {english},
  file     = {C:\Users\jseo1005\Zotero\storage\4QEJR7EE\www.bemyeyes.com.html}
}

@online{bendelHowCanGenerative2024,
  title       = {How {{Can Generative AI Enhance}} the {{Well-being}} of {{Blind}}?},
  author      = {Bendel, Oliver},
  date        = {2024-02-02},
  eprint      = {2402.07919},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2402.07919},
  url         = {http://arxiv.org/abs/2402.07919},
  urldate     = {2024-04-20},
  abstract    = {This paper examines the question of how generative AI can improve the well-being of blind or visually impaired people. It refers to a current example, the Be My Eyes app, in which the Be My AI feature was integrated in 2023, which is based on GPT-4 from OpenAI. The author's tests are described and evaluated. There is also an ethical and social discussion. The power of the tool, which can analyze still images in an amazing way, is demonstrated. Those affected gain a new independence and a new perception of their environment. At the same time, they are dependent on the world view and morality of the provider or developer, who prescribe or deny them certain descriptions. An outlook makes it clear that the analysis of moving images will mean a further leap forward. It is fair to say that generative AI can fundamentally improve the well-being of blind and visually impaired people and will change it in various ways.},
  pubstate    = {preprint},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,I.2,K.3},
  file        = {C:\Users\jseo1005\Zotero\storage\IA8DSCLS\2402.html}
}

@inproceedings{bighamVizWizNearlyRealtime2010,
  title      = {{{VizWiz}}: Nearly Real-Time Answers to Visual Questions},
  shorttitle = {{{VizWiz}}},
  booktitle  = {Proceedings of the 23nd Annual {{ACM}} Symposium on {{User}} Interface Software and Technology},
  author     = {Bigham, Jeffrey P. and Jayant, Chandrika and Ji, Hanjie and Little, Greg and Miller, Andrew and Miller, Robert C. and Miller, Robin and Tatarowicz, Aubrey and White, Brandyn and White, Samual and Yeh, Tom},
  date       = {2010-10-03},
  series     = {{{UIST}} '10},
  pages      = {333--342},
  publisher  = {Association for Computing Machinery},
  location   = {New York, NY, USA},
  doi        = {10.1145/1866029.1866080},
  url        = {https://doi.org/10.1145/1866029.1866080},
  urldate    = {2024-04-20},
  abstract   = {The lack of access to visual information like text labels, icons, and colors can cause frustration and decrease independence for blind people. Current access technology uses automatic approaches to address some problems in this space, but the technology is error-prone, limited in scope, and quite expensive. In this paper, we introduce VizWiz, a talking application for mobile phones that offers a new alternative to answering visual questions in nearly real-time - asking multiple people on the web. To support answering questions quickly, we introduce a general approach for intelligently recruiting human workers in advance called quikTurkit so that workers are available when new questions arrive. A field deployment with 11 blind participants illustrates that blind people can effectively use VizWiz to cheaply answer questions in their everyday lives, highlighting issues that automatic approaches will need to address to be useful. Finally, we illustrate the potential of using VizWiz as part of the participatory design of advanced tools by using it to build and evaluate VizWiz::LocateIt, an interactive mobile tool that helps blind people solve general visual search problems.},
  isbn       = {978-1-4503-0271-5},
  keywords   = {blind users,non-visual interfaces,real-time human computation}
}

@incollection{blancoOlliExtensibleVisualization2022,
  title      = {Olli: {{An Extensible Visualization Library}} for {{Screen Reader Accessibility}}},
  shorttitle = {Olli},
  booktitle  = {{{IEEE VIS Posters}}},
  author     = {Blanco, Matt and Zong, Jonathan and Satyanarayan, Arvind},
  date       = {2022-10-19T00:00:00+00:00},
  url        = {http://vis.csail.mit.edu/pubs/olli/},
  urldate    = {2023-11-06},
  abstract   = {Though recent research has explored the design of rich screen reader visualization experiences, accessible visualizations for blind and low vision users remain rare on the web. While some visualization toolkits offer accessible solutions, toolkit-specific implementations can present idiosyncratic user experiences that limit learnability. We present Olli, an open source library that converts visualizations into a keyboard-navigable structure accessible to screen readers. Using an extensible adapter design pattern, Olli is agnostic to the specific toolkit used to author the visualization. Olli renders a chart as an accessible tree view following the HTML Accessible Rich Internet Applications (ARIA) standard. Olli helps visualization developers easily create accessible visualizations across visualization toolkits.},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\D5X9Y53T\Blanco et al. - 2022 - Olli An Extensible Visualization Library for Scre.pdf}
}

@article{blattmannStableVideoDiffusion,
  title  = {Stable {{Video Diffusion}}: {{Scaling Latent Video Diffusion Models}} to {{Large Datasets}}},
  author = {Blattmann, Andreas and Dockhorn, Tim and Kulal, Sumith and Mendelevitch, Daniel and Kilian, Maciej and Lorenz, Dominik and Levi, Yam and English, Zion and Voleti, Vikram and Letts, Adam and Jampani, Varun and Rombach, Robin and Ai, Stability},
  langid = {english},
  file   = {C:\Users\jseo1005\Zotero\storage\GH3D9JFN\Blattmann et al. - Stable Video Diffusion Scaling Latent Video Diffu.pdf}
}

@inproceedings{boucherEducationalDataComics2023,
  title      = {Educational {{Data Comics}}: {{What}} Can {{Comics}} Do for {{Education}} in {{Visualization}}?},
  shorttitle = {Educational {{Data Comics}}},
  booktitle  = {2023 {{IEEE VIS Workshop}} on {{Visualization Education}}, {{Literacy}}, and {{Activities}} ({{EduVis}})},
  author     = {Boucher, Magdalena and Bach, Benjamin and Stoiber, Christina and Wang, Zezhong and Aigner, Wolfgang},
  date       = {2023-10-22},
  pages      = {34--40},
  publisher  = {IEEE},
  location   = {Melbourne, Australia},
  doi        = {10.1109/EduVis60792.2023.00012},
  url        = {https://ieeexplore.ieee.org/document/10344064/},
  urldate    = {2024-01-15},
  abstract   = {This paper discusses the potential of comics for explaining concepts with and around data visualization. With the increasing spread of visualizations and the democratization of access to visualization tools, we see a growing need for easily approachable resources for learning visualization techniques, applications, design processes, etc. Comics are a promising medium for such explanation as they concisely combine graphical and textual content in a sequential manner and they provide fast visual access to specific parts of the explanations. Based on a first literature review and our extensive experience with the subject, we survey works at the respective intersections of comics, visualization and education: data comics, educational comics, and visualization education. We report on five potentials of comics to create and share educational material, to engage wide and potentially diverse audiences, and to support educational activities. For each potential we list, we describe open questions for future research. Our discussion aims to inform both the application of comics by educators and their extension and study by researchers.},
  eventtitle = {2023 {{IEEE VIS Workshop}} on {{Visualization Education}}, {{Literacy}}, and {{Activities}} ({{EduVis}})},
  isbn       = {9798350330304},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\NQPFU5ZU\Boucher et al. - 2023 - Educational Data Comics What can Comics do for Education in Visualization.pdf}
}

@article{bovairAcquisitionPerformanceTextEditing1990,
  title        = {The {{Acquisition}} and {{Performance}} of {{Text-Editing Skill}}: {{A Cognitive Complexity Analysis}}},
  shorttitle   = {The {{Acquisition}} and {{Performance}} of {{Text-Editing Skill}}},
  author       = {Bovair, Susan and Kieras, David E. and Polson, Peter G.},
  date         = {1990-03-01},
  journaltitle = {Human–Computer Interaction},
  volume       = {5},
  number       = {1},
  pages        = {1--48},
  publisher    = {Taylor \& Francis},
  issn         = {0737-0024},
  doi          = {10.1207/s15327051hci0501_1},
  url          = {https://www.tandfonline.com/doi/abs/10.1207/s15327051hci0501_1},
  urldate      = {2023-09-07},
  abstract     = {Kieras and Polson (1985) proposed an approach for making quantitative predictions on ease of learning and ease of use of a system, based on a production system version of the goals, operators, methods, and selection rules (GOMS) model of Card, Moran, and Newel1 (1983). This article describes the principles for constructing such models and obtaining predictions of learning and execution time. A production rule model for a simulated text editor is described in detail and is compared to experimental data on learning and performance. The model accounted well for both learning and execution time and for the details of the increase in speed with practice. The relationship between the performance model and the Keystroke-Level Model of Card et al. (1983) is discussed. The results provide strong support for the original proposal that production rule models can make quantitative predictions for both ease of learning and ease of use.},
  file         = {C:\Users\jseo1005\Zotero\storage\FUDR4HX3\Bovair et al. - 1990 - The Acquisition and Performance of Text-Editing Skill A Cognitive Complexity Analysis.pdf}
}

@online{BraillePatterns,
  title   = {Braille {{Patterns}}},
  url     = {https://unicode.org/charts/nameslist/c_2800.html},
  urldate = {2023-01-12},
  file    = {C:\Users\jseo1005\Zotero\storage\EAIRJ2XZ\c_2800.html}
}

@article{brewsterVisualizationToolsBlind2002,
  title        = {Visualization Tools for Blind People Using Multiple Modalities},
  author       = {Brewster, S.},
  date         = {2002-01-01},
  journaltitle = {Disability and Rehabilitation},
  volume       = {24},
  number       = {11-12},
  eprint       = {12182801},
  eprinttype   = {pmid},
  pages        = {613--621},
  publisher    = {Taylor \& Francis},
  issn         = {0963-8288},
  doi          = {10.1080/09638280110111388},
  url          = {https://doi.org/10.1080/09638280110111388},
  urldate      = {2023-11-28},
  abstract     = {Purpose : There are many problems when blind people need to access visualizations such as graphs and tables. Current speech or raised-paper technology does not provide a good solution. Our approach is to use non-speech sounds and haptics to allow a richer and more flexible form of access to graphs and tables. Method : Two experiments are reported that test out designs for both sound and haptic graph solutions. In the audio case a standard speech interface is compared to one with non-speech sounds added. The haptic experiment compares two different graph designs to see which was the most effective. Results : Our results for the sound graphs showed a significant decrease in subjective workload, reduced time taken to complete tasks and reduced errors as compared to a standard speech interface. For the haptic graphs reductions in workload and some of the problems that can occur when using such graphs are shown. Conclusions : Using non-speech sound and haptics can significantly improve interaction with visualizations such as graphs. This multimodal approach makes the most of the senses our users have to provide access to information in more flexible ways.},
  file         = {C:\Users\jseo1005\Zotero\storage\65EPHUUY\Brewster - 2002 - Visualization tools for blind people using multiple modalities.pdf}
}

@article{brookeSUSRetrospective2013,
  title        = {{{SUS}}: A Retrospective},
  shorttitle   = {{{SUS}}},
  author       = {Brooke, John},
  date         = {2013-01-01},
  journaltitle = {Journal of Usability Studies},
  shortjournal = {Journal of Usability Studies},
  volume       = {8},
  pages        = {29--40}
}

@article{brookeSUSRetrospective2013a,
  title        = {{{SUS}}: A Retrospective},
  shorttitle   = {{{SUS}}},
  author       = {Brooke, John},
  date         = {2013-02-01},
  journaltitle = {Journal of Usability Studies},
  shortjournal = {J. Usability Studies},
  volume       = {8},
  number       = {2},
  pages        = {29--40},
  abstract     = {Rather more than 25 years ago, as part of a usability engineering program, I developed a questionnaire---the System Usability Scale (SUS)---that could be used to take a quick measurement of how people perceived the usability of computer systems on which they were working. This proved to be an extremely simple and reliable tool for use when doing usability evaluations, and I decided, with the blessing of engineering management at Digital Equipment Co. Ltd (DEC; where I developed SUS), that it was probably something that could be used by other organizations (the benefit for us being that if they did use it, we potentially had something we could use to compare their systems against ours). So, in 1986, I made SUS freely available to a number of colleagues, with permission to pass it on to anybody else who might find it useful, and over the next few years occasionally heard of evaluations of systems where researchers and usability engineers had used it with some success.}
}

@inproceedings{brownVizTouchAutomaticallyGenerated2012a,
  title      = {{{VizTouch}}: Automatically Generated Tactile Visualizations of Coordinate Spaces},
  shorttitle = {{{VizTouch}}},
  booktitle  = {Proceedings of the {{Sixth International Conference}} on {{Tangible}}, {{Embedded}} and {{Embodied Interaction}}},
  author     = {Brown, Craig and Hurst, Amy},
  date       = {2012-02-19},
  pages      = {131--138},
  publisher  = {ACM},
  location   = {Kingston Ontario Canada},
  doi        = {10.1145/2148131.2148160},
  url        = {https://dl.acm.org/doi/10.1145/2148131.2148160},
  urldate    = {2023-09-09},
  eventtitle = {{{TEI}}'12: {{Sixth International Conference}} on {{Tangible}}, {{Embedded}}, and {{Embodied Interaction}}},
  isbn       = {978-1-4503-1174-8},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\WEMJHXZ2\Brown and Hurst - 2012 - VizTouch automatically generated tactile visualiz.pdf}
}

@online{caiLeveragingLargeLanguage2023,
  title       = {Leveraging {{Large Language Models}} for {{Scalable Vector Graphics-Driven Image Understanding}}},
  author      = {Cai, Mu and Huang, Zeyi and Li, Yuheng and Wang, Haohan and Lee, Yong Jae},
  date        = {2023-06-09},
  eprint      = {2306.06094},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/2306.06094},
  urldate     = {2024-02-06},
  abstract    = {Recently, large language models (LLMs) have made significant advancements in natural language understanding and generation. However, their potential in computer vision remains largely unexplored. In this paper, we introduce a new, exploratory approach that enables LLMs to process images using the Scalable Vector Graphics (SVG) format. By leveraging the XML-based textual descriptions of SVG representations instead of raster images, we aim to bridge the gap between the visual and textual modalities, allowing LLMs to directly understand and manipulate images without the need for parameterized visual components. Our method facilitates simple image classification, generation, and in-context learning using only LLM capabilities. We demonstrate the promise of our approach across discriminative and generative tasks, highlighting its (i) robustness against distribution shift, (ii) substantial improvements achieved by tapping into the in-context learning abilities of LLMs, and (iii) image understanding and generation capabilities with human guidance. Our code, data, and models can be found here https://github.com/mu-cai/svg-llm.},
  pubstate    = {preprint},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file        = {C\:\\Users\\jseo1005\\Zotero\\storage\\X83C2XSY\\Cai et al. - 2023 - Leveraging Large Language Models for Scalable Vect.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\IKKXHUP6\\2306.html}
}

@inproceedings{cervoneAdaptableAccessibilityFeatures2019,
  title     = {Adaptable {{Accessibility Features}} for {{Mathematics}} on the {{Web}}},
  booktitle = {Proceedings of the 16th {{International Web}} for {{All Conference}}},
  author    = {Cervone, Davide and Sorge, Volker},
  date      = {2019-05-13},
  series    = {{{W4A}} '19},
  pages     = {1--4},
  publisher = {Association for Computing Machinery},
  location  = {New York, NY, USA},
  doi       = {10.1145/3315002.3317567},
  url       = {https://dl.acm.org/doi/10.1145/3315002.3317567},
  urldate   = {2023-12-10},
  abstract  = {Accessibility of mathematics is still a challenging problem and providing the right level of support to a reader depends on many factors, such as their particular assistive technology needs, their level of expertise, and the subject area they are working in. We present work towards making math accessibility more adaptable to the reader's personal needs that is implemented in the MathJax library for rendering mathematics on the web. While MathJax provided accessibility support for several years, the new version 3 has both more new features and means of personalization. In particular, it provides adaptable combinations of highlighting, colorization, and magnification techniques. Both Braille and speech output can be generated, with different speech rule sets allowing readers to flexibly change presentation and adaptation for better interpretation of formulas in different subject areas, like Physics, Chemistry, and Logic.},
  isbn      = {978-1-4503-6716-5},
  keywords  = {Mathematics,MathJax,STEM Accessibility},
  file      = {C:\Users\jseo1005\Zotero\storage\HZSM4IP8\Cervone and Sorge - 2019 - Adaptable Accessibility Features for Mathematics o.pdf}
}

@book{charltonNothingUsUs1998,
  title      = {Nothing {{About Us Without Us}}: {{Disability Oppression}} and {{Empowerment}}},
  shorttitle = {Nothing {{About Us Without Us}}},
  author     = {Charlton, James I.},
  date       = {1998},
  eprint     = {ohqff8DBt9gC},
  eprinttype = {googlebooks},
  publisher  = {University of California Press},
  abstract   = {James Charlton has produced a ringing indictment of disability oppression, which, he says, is rooted in degradation, dependency, and powerlessness and is experienced in some form by five hundred million persons throughout the world who have physical, sensory, cognitive, or developmental disabilities. Nothing About Us Without Us is the first book in the literature on disability to provide a theoretical overview of disability oppression that shows its similarities to, and differences from, racism, sexism, and colonialism. Charlton's analysis is illuminated by interviews he conducted over a ten-year period with disability rights activists throughout the Third World, Europe, and the United States. Charlton finds an antidote for dependency and powerlessness in the resistance to disability oppression that is emerging worldwide. His interviews contain striking stories of self-reliance and empowerment evoking the new consciousness of disability rights activists. As a latecomer among the world's liberation movements, the disability rights movement will gain visibility and momentum from Charlton's elucidation of its history and its political philosophy of self-determination, which is captured in the title of his book. Nothing About Us Without Us expresses the conviction of people with disabilities that they know what is best for them. Charlton's combination of personal involvement and theoretical awareness assures greater understanding of the disability rights movement.},
  isbn       = {978-0-520-22481-0},
  langid     = {english},
  pagetotal  = {222},
  keywords   = {Medical / Health Care Delivery}
}

@online{ChartsContentComponents,
  title   = {Charts - {{Content}} - {{Components}} - {{Human Interface Guidelines}} - {{Design}} - {{Apple Developer}}},
  url     = {https://developer.apple.com/design/human-interface-guidelines/components/content/charts},
  urldate = {2022-09-27},
  file    = {C:\Users\jseo1005\Zotero\storage\NQVXZWEW\charts.html}
}

@online{cherukuruVisualsExaminingExperiences2022,
  title       = {Beyond {{Visuals}} : {{Examining}} the {{Experiences}} of {{Geoscience Professionals With Vision Disabilities}} in {{Accessing Data Visualizations}}},
  shorttitle  = {Beyond {{Visuals}}},
  author      = {Cherukuru, Nihanth W. and Bailey, David A. and Fourment, Tiffany and Hatheway, Becca and Holland, Marika M. and Rehme, Matt},
  date        = {2022-07-26},
  eprint      = {2207.13220},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2207.13220},
  url         = {http://arxiv.org/abs/2207.13220},
  urldate     = {2022-10-20},
  abstract    = {Data visualizations are ubiquitous in all disciplines and have become the primary means of analysing data and communicating insights. However, the predominant reliance on visual encoding of data continues to create accessibility barriers for people who are blind/vision impaired resulting in their under representation in Science, Technology, Engineering and Mathematics (STEM) disciplines. This research study seeks to understand the experiences of professionals who are blind/vision impaired in one such STEM discipline (geosciences) in accessing data visualizations. In-depth, semi-structured interviews with seven professionals were conducted to examine the accessibility barriers and areas for improvement to inform accessibility research pertaining to data visualizations through a socio-technical lens. A reflexive thematic analysis revealed the negative impact of visualizations in influencing their career path, lack of data exploration tools for research, barriers in accessing works of peers and mismatched pace of visualization and accessibility research. The article also includes recommendations from the participants to address some of these accessibility barriers.},
  pubstate    = {preprint},
  keywords    = {Computer Science - Computers and Society,Computer Science - Graphics,Computer Science - Human-Computer Interaction},
  file        = {C:\Users\jseo1005\Zotero\storage\B2F2B2R2\Cherukuru et al. - 2022 - Beyond Visuals  Examining the Experiences of Geos.pdf}
}

@inproceedings{choiTactileDisplayBraille2004,
  title      = {Tactile Display as a {{Braille}} Display for the Visually Disabled},
  booktitle  = {2004 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}}) ({{IEEE Cat}}. {{No}}.{{04CH37566}})},
  author     = {Choi, H.R. and Lee, S.W. and Jung, K.M. and Koo, J.C. and Lee, S.I. and Choi, H.G. and Jeon, J.W. and Nam, J.D.},
  date       = {2004-09},
  volume     = {2},
  pages      = {1985-1990 vol.2},
  doi        = {10.1109/IROS.2004.1389689},
  abstract   = {Tactile sensation is one of the most important sensory functions along with the auditory sensation for the visually impaired because it replaces the visual sensation of the persons with sight. In this paper, we present a tactile display device as a dynamic Braille display that is the unique tool for exchanging information among them. The proposed tactile cell of the Braille display is based on the dielectric elastomer and it has advantageous features over the existing ones with respect to intrinsic softness, ease of fabrication, cost effectiveness and miniaturization. We introduce a new idea for actuation and describe the actuating mechanism of the Braille pin in details capable of realizing the enhanced spatial density of the tactile cells. Finally, results of psychophysical experiments are given and its effectiveness is confirmed.},
  eventtitle = {2004 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}}) ({{IEEE Cat}}. {{No}}.{{04CH37566}})},
  keywords   = {Actuators,Auditory displays,Engineering management,Fabrication,Humans,Lungs,Mechanical engineering,Pins,Psychology,Skin},
  file       = {C\:\\Users\\jseo1005\\Zotero\\storage\\W28ZJ4ZI\\Choi et al. - 2004 - Tactile display as a Braille display for the visua.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\QT327JMP\\1389689.html}
}

@article{choiVisualizingNonVisual2019,
  title        = {Visualizing for the {{Non}}‐{{Visual}}: {{Enabling}} the {{Visually Impaired}} to {{Use Visualization}}},
  shorttitle   = {Visualizing for the {{Non}}‐{{Visual}}},
  author       = {Choi, Jinho and Jung, Sanghun and Park, Deok Gun and Choo, Jaegul and Elmqvist, Niklas},
  date         = {2019-06},
  journaltitle = {Computer Graphics Forum},
  volume       = {38},
  number       = {3},
  pages        = {249--260},
  publisher    = {Wiley-Blackwell},
  issn         = {01677055},
  doi          = {10.1111/cgf.13686},
  url          = {https://proxy2.library.illinois.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=bsu&AN=137771620&site=eds-live&scope=site},
  urldate      = {2023-09-04},
  abstract     = {The majority of visualizations on the web are still stored as raster images, making them inaccessible to visually impaired users. We propose a deep‐neural‐network‐based approach that automatically recognizes key elements in a visualization, including a visualization type, graphical elements, labels, legends, and most importantly, the original data conveyed in the visualization. We leverage such extracted information to provide visually impaired people with the reading of the extracted information. Based on interviews with visually impaired users, we built a Google Chrome extension designed to work with screen reader software to automatically decode charts on a webpage using our pipeline. We compared the performance of the back‐end algorithm with existing methods and evaluated the utility using qualitative feedback from visually impaired users.},
  keywords     = {CCS Concepts,Data modeling,Data visualization,Google Chrome (Computer software),Human‐centered computing → Visual analytics,People with visual disabilities,Visual analytics,Visualization,Visualization toolkits,Work design},
  file         = {C:\Users\jseo1005\Zotero\storage\CR795AQV\Choi et al. - 2019 - Visualizing for the Non‐Visual Enabling the Visua.pdf}
}

@article{choiVisualizingNonVisualEnabling2019,
  title        = {Visualizing for the {{Non-Visual}}: {{Enabling}} the {{Visually Impaired}} to {{Use Visualization}}},
  shorttitle   = {Visualizing for the {{Non-Visual}}},
  author       = {Choi, Jinho and Jung, Sanghun and Park, Deok Gun and Choo, Jaegul and Elmqvist, Niklas},
  date         = {2019},
  journaltitle = {Computer Graphics Forum},
  volume       = {38},
  number       = {3},
  pages        = {249--260},
  issn         = {1467-8659},
  doi          = {10.1111/cgf.13686},
  url          = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13686},
  urldate      = {2022-08-22},
  abstract     = {The majority of visualizations on the web are still stored as raster images, making them inaccessible to visually impaired users. We propose a deep-neural-network-based approach that automatically recognizes key elements in a visualization, including a visualization type, graphical elements, labels, legends, and most importantly, the original data conveyed in the visualization. We leverage such extracted information to provide visually impaired people with the reading of the extracted information. Based on interviews with visually impaired users, we built a Google Chrome extension designed to work with screen reader software to automatically decode charts on a webpage using our pipeline. We compared the performance of the back-end algorithm with existing methods and evaluated the utility using qualitative feedback from visually impaired users.},
  langid       = {english},
  keywords     = {• Human-centered computing → Visual analytics,CCS Concepts,Visualization toolkits},
  file         = {C:\Users\jseo1005\Zotero\storage\ZBUN9DYE\cgf.html}
}

@article{choiVisualizingNonVisualEnabling2019a,
  title        = {Visualizing for the {{Non-Visual}}: {{Enabling}} the {{Visually Impaired}} to {{Use Visualization}}},
  shorttitle   = {Visualizing for the {{Non-Visual}}},
  author       = {Choi, Jinho and Jung, Sanghun and Park, Deok Gun and Choo, Jaegul and Elmqvist, Niklas},
  date         = {2019},
  journaltitle = {Computer Graphics Forum},
  volume       = {38},
  number       = {3},
  pages        = {249--260},
  issn         = {1467-8659},
  doi          = {10.1111/cgf.13686},
  url          = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13686},
  urldate      = {2023-11-14},
  abstract     = {The majority of visualizations on the web are still stored as raster images, making them inaccessible to visually impaired users. We propose a deep-neural-network-based approach that automatically recognizes key elements in a visualization, including a visualization type, graphical elements, labels, legends, and most importantly, the original data conveyed in the visualization. We leverage such extracted information to provide visually impaired people with the reading of the extracted information. Based on interviews with visually impaired users, we built a Google Chrome extension designed to work with screen reader software to automatically decode charts on a webpage using our pipeline. We compared the performance of the back-end algorithm with existing methods and evaluated the utility using qualitative feedback from visually impaired users.},
  langid       = {english},
  keywords     = {• Human-centered computing → Visual analytics,CCS Concepts,Visualization toolkits},
  file         = {C:\Users\jseo1005\Zotero\storage\Z8TQENML\Choi et al. - 2019 - Visualizing for the Non-Visual Enabling the Visually Impaired to Use Visualization.pdf}
}

@article{chunduryUnderstandingSensorySubstitution2022,
  title        = {Towards {{Understanding Sensory Substitution}} for {{Accessible Visualization}}: {{An Interview Study}}},
  shorttitle   = {Towards {{Understanding Sensory Substitution}} for {{Accessible Visualization}}},
  author       = {Chundury, Pramod and Patnaik, Biswaksen and Reyazuddin, Yasmin and Tang, Christine and Lazar, Jonathan and Elmqvist, Niklas},
  date         = {2022-01},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  volume       = {28},
  number       = {1},
  pages        = {1084--1094},
  issn         = {1941-0506},
  doi          = {10.1109/TVCG.2021.3114829},
  url          = {https://ieeexplore.ieee.org/abstract/document/9552177},
  urldate      = {2023-11-28},
  abstract     = {For all its potential in supporting data analysis, particularly in exploratory situations, visualization also creates barriers: accessibility for blind and visually impaired individuals. Regardless of how effective a visualization is, providing equal access for blind users requires a paradigm shift for the visualization research community. To enact such a shift, it is not sufficient to treat visualization accessibility as merely another technical problem to overcome. Instead, supporting the millions of blind and visually impaired users around the world who have equally valid needs for data analysis as sighted individuals requires a respectful, equitable, and holistic approach that includes all users from the onset. In this paper, we draw on accessibility research methodologies to make inroads towards such an approach. We first identify the people who have specific insight into how blind people perceive the world: orientation and mobility (O\&M) experts, who are instructors that teach blind individuals how to navigate the physical world using non-visual senses. We interview 10 O\&M experts—all of them blind—to understand how best to use sensory substitution other than the visual sense for conveying spatial layouts. Finally, we investigate our qualitative findings using thematic analysis. While blind people in general tend to use both sound and touch to understand their surroundings, we focused on auditory affordances and how they can be used to make data visualizations accessible—using sonification and auralization. However, our experts recommended supporting a combination of senses—sound and touch—to make charts accessible as blind individuals may be more familiar with exploring tactile charts. We report results on both sound and touch affordances, and conclude by discussing implications for accessible visualization for blind individuals.},
  eventtitle   = {{{IEEE Transactions}} on {{Visualization}} and {{Computer Graphics}}},
  file         = {C\:\\Users\\jseo1005\\Zotero\\storage\\D49SRMC6\\Chundury et al. - 2022 - Towards Understanding Sensory Substitution for Accessible Visualization An Interview Study.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\JFPHYWHC\\9552177.html}
}

@inproceedings{ciuhaVisualizationConcurrentTones2010,
  title      = {Visualization of Concurrent Tones in Music with Colours},
  booktitle  = {Proceedings of the International Conference on {{Multimedia}} - {{MM}} '10},
  author     = {Ciuha, Peter and Klemenc, Bojan and Solina, Franc},
  date       = {2010},
  pages      = {1677},
  publisher  = {ACM Press},
  location   = {Firenze, Italy},
  doi        = {10.1145/1873951.1874320},
  url        = {http://dl.acm.org/citation.cfm?doid=1873951.1874320},
  urldate    = {2022-08-21},
  eventtitle = {The International Conference},
  isbn       = {978-1-60558-933-6},
  langid     = {english}
}

@article{clarkDualCodingTheory1991,
  title        = {Dual Coding Theory and Education},
  author       = {Clark, James M. and Paivio, Allan},
  date         = {1991-09-01},
  journaltitle = {Educational Psychology Review},
  shortjournal = {Educ Psychol Rev},
  volume       = {3},
  number       = {3},
  pages        = {149--210},
  issn         = {1573-336X},
  doi          = {10.1007/BF01320076},
  url          = {https://doi.org/10.1007/BF01320076},
  urldate      = {2023-01-17},
  abstract     = {Dual coding theory (DCT) explains human behavior and experience in terms of dynamic associative processes that operate on a rich network of modality-specific verbal and nonverbal (or imagery) representations. We first describe the underlying premises of the theory and then show how the basic DCT mechanisms can be used to model diverse educational phenomena. The research demonstrates that concreteness, imagery, and verbal associative processes play major roles in various educational domains: the representation and comprehension of knowledge, learning and memory of school material, effective instruction, individual differences, achievement motivation and test anxiety, and the learning of motor skills. DCT also has important implications for the science and practice of educational psychology — specifically, for educational research and teacher education. We show not only that DCT provides a unified explanation for diverse topics in education, but also that its mechanistic framework accommodates theories cast in terms of strategies and other high-level psychological processes. Although much additional research needs to be done, the concrete models that DCT offers for the behavior and experience of students, teachers, and educational psychologists further our understanding of educational phenomena and strengthen related pedagogical practices.},
  langid       = {english},
  keywords     = {imagery,unified educational theory,verbal processes},
  file         = {C:\Users\jseo1005\Zotero\storage\7WPYLRAX\Clark and Paivio - 1991 - Dual coding theory and education.pdf}
}

@online{cloudsightImageRecognitionAPI2024,
  title   = {Image {{Recognition API}} \& {{General Purpose Computer Vision}} and {{Captioning}} - {{CloudSight AI}}},
  author  = {{CloudSight}},
  date    = {2024},
  url     = {https://cloudsight.ai/},
  urldate = {2024-04-20},
  file    = {C:\Users\jseo1005\Zotero\storage\2VGT662D\cloudsight.ai.html}
}

@incollection{dealmeidavasconcellosChapter18Interactive2005,
  title     = {Chapter 18 {{Interactive}} Mapping for People Who Are Blind or Visually Impaired},
  booktitle = {Modern {{Cartography Series}}},
  author    = {family=Almeida (Vasconcellos), given=Regina Araujo, prefix=de, useprefix=true and Tsuji, Bruce},
  editor    = {Taylor, D. R. Fraser},
  date      = {2005-01-01},
  series    = {Cybercartography},
  volume    = {4},
  pages     = {411--431},
  publisher = {Academic Press},
  doi       = {10.1016/S1363-0814(05)80021-8},
  url       = {https://www.sciencedirect.com/science/article/pii/S1363081405800218},
  urldate   = {2023-11-14},
  abstract  = {Static and interactive tactile maps are discussed in the context of providing survey and mobility information to people who are visually impaired or blind. The heterogeneous nature of visual impairment is examined, as is the nature of tactile perception. Technologies associated with tactile maps are reviewed and the application of interactive tactile maps for populations, in addition to those who are visually impaired, is also considered. Cybercartography has considerable potential in this respect.},
  file      = {C:\Users\jseo1005\Zotero\storage\57ZTZR38\S1363081405800218.html}
}

@inproceedings{degreefInterdependentVariablesRemotely2021,
  title      = {Interdependent {{Variables}}: {{Remotely Designing Tactile Graphics}} for an {{Accessible Workflow}}},
  shorttitle = {Interdependent {{Variables}}},
  booktitle  = {The 23rd {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author     = {De Greef, Lilian and Moritz, Dominik and Bennett, Cynthia},
  date       = {2021-10-17},
  pages      = {1--6},
  publisher  = {ACM},
  location   = {Virtual Event USA},
  doi        = {10.1145/3441852.3476468},
  url        = {https://dl.acm.org/doi/10.1145/3441852.3476468},
  urldate    = {2023-09-06},
  abstract   = {In this experience report, we offer a case study of blind and sighted colleagues creating an accessible workflow to collaborate on a data visualization-focused project. We outline our process for making the project’s shared data representations accessible through incorporating both handmade and machine-embossed tactile graphics. We also share lessons and strategies for considering team needs and addressing contextual constraints like remote collaboration during the COVID-19 pandemic. More broadly, this report contributes to ongoing research into the ways accessibility is interdependent by arguing that access work must be a collective responsibility and properly supported with recognition, resources, and infrastructure.},
  eventtitle = {{{ASSETS}} '21: {{The}} 23rd {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  isbn       = {978-1-4503-8306-6},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\YGPSINZ3\De Greef et al. - 2021 - Interdependent Variables Remotely Designing Tacti.pdf}
}

@online{DesmosGraphingCalculator,
  title        = {Desmos | {{Graphing Calculator}}},
  url          = {https://www.desmos.com/calculator},
  urldate      = {2023-01-02},
  abstract     = {Explore math with our beautiful, free online graphing calculator. Graph functions, plot points, visualize algebraic equations, add sliders, animate graphs, and more.},
  langid       = {english},
  organization = {Desmos},
  file         = {C:\Users\jseo1005\Zotero\storage\VTL3FFZK\calculator.html}
}

@online{Diagcess,
  title        = {Diagcess},
  url          = {https://www.npmjs.com/package/diagcess},
  urldate      = {2022-08-21},
  abstract     = {A diagram explorer for progressiveaccee.com style diagram annotations.. Latest version: 1.1.4, last published: 7 months ago. Start using diagcess in your project by running `npm i diagcess`. There are no other projects in the npm registry using diagcess.},
  langid       = {english},
  organization = {npm},
  file         = {C:\Users\jseo1005\Zotero\storage\NJM4U6JH\diagcess.html}
}

@article{dowWizardOzSupport2005,
  title        = {Wizard of {{Oz}} Support throughout an Iterative Design Process},
  author       = {Dow, S. and MacIntyre, B. and Lee, J. and Oezbek, C. and Bolter, J.D. and Gandy, M.},
  date         = {2005-10},
  journaltitle = {IEEE Pervasive Computing},
  volume       = {4},
  number       = {4},
  pages        = {18--26},
  issn         = {1558-2590},
  doi          = {10.1109/MPRV.2005.93},
  abstract     = {The Wizard of Oz prototyping approach, widely used in human-computer interaction research, is particularly useful in exploring user interfaces for pervasive, ubiquitous, or mixed-reality systems that combine complex sensing and intelligent control logic. The vast design space for such nontraditional interfaces provides many possibilities for user interaction through one or more modalities and often requires challenging hardware and software implementations. The WOz method helps designers avoid getting locked into a particular design or working under an incorrect set of assumptions about user preferences, because it lets them explore and evaluate designs before investing the considerable development time needed to build a complete prototype.},
  eventtitle   = {{{IEEE Pervasive Computing}}},
  keywords     = {audio tours,Computational modeling,design process,HCI methods,Intelligent control,Intelligent sensors,Intelligent systems,Iterative methods,mixed reality,Process design,Prototypes,prototyping,Sensor systems,ubiquitous computing,User interfaces,Virtual reality,Wizard of Oz},
  file         = {C\:\\Users\\jseo1005\\Zotero\\storage\\FEII3JJW\\Dow et al. - 2005 - Wizard of Oz support throughout an iterative desig.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\AXPNQFXY\\1541964.html}
}

@inproceedings{ebelVisualizingEventSequence2021,
  title     = {Visualizing {{Event Sequence Data}} for {{User Behavior Evaluation}} of {{In-Vehicle Information Systems}}},
  booktitle = {13th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  author    = {Ebel, Patrick and Lingenfelder, Christoph and Vogelsang, Andreas},
  date      = {2021-09-20},
  series    = {{{AutomotiveUI}} '21},
  pages     = {219--229},
  publisher = {Association for Computing Machinery},
  location  = {New York, NY, USA},
  doi       = {10.1145/3409118.3475140},
  url       = {https://dl.acm.org/doi/10.1145/3409118.3475140},
  urldate   = {2023-09-07},
  abstract  = {With modern In-Vehicle Information Systems (IVISs) becoming more capable and complex than ever, their evaluation becomes increasingly difficult. The analysis of large amounts of user behavior data can help to cope with this complexity and can support UX experts in designing IVISs that serve customer needs and are safe to operate while driving. We, therefore, propose a Multi-level User Behavior Visualization Framework providing effective visualizations of user behavior data that is collected via telematics from production vehicles. Our approach visualizes user behavior data on three different levels: (1) The Task Level View aggregates event sequence data generated through touchscreen interactions to visualize user flows. (2) The Flow Level View allows comparing the individual flows based on a chosen metric. (3) The Sequence Level View provides detailed insights into touch interactions, glance, and driving behavior. Our case study proves that UX experts consider our approach a useful addition to their design process.},
  isbn      = {978-1-4503-8063-8},
  file      = {C:\Users\jseo1005\Zotero\storage\27AG4E2E\Ebel et al. - 2021 - Visualizing Event Sequence Data for User Behavior Evaluation of In-Vehicle Information Systems.pdf}
}

@online{EducationalDataComics,
  title   = {Educational {{Data Comics}}: {{What}} Can {{Comics}} Do for {{Education}} in {{Visualization}}? | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url     = {https://ieeexplore.ieee.org/abstract/document/10344064},
  urldate = {2024-01-15},
  file    = {C:\Users\jseo1005\Zotero\storage\35E26565\10344064.html}
}

@article{edwardsHowAltText2023,
  title        = {How the {{Alt Text Gets Made}}: {{What Roles}} and {{Processes}} of {{Alt Text Creation Can Teach Us AboutInclusive Imagery}}},
  shorttitle   = {How the {{Alt Text Gets Made}}},
  author       = {Edwards, Emory J. and Gilbert, Michael and Blank, Emily and Branham, Stacy M.},
  date         = {2023-06-30},
  journaltitle = {ACM Transactions on Accessible Computing},
  shortjournal = {ACM Trans. Access. Comput.},
  volume       = {16},
  number       = {2},
  pages        = {1--28},
  issn         = {1936-7228, 1936-7236},
  doi          = {10.1145/3587469},
  url          = {https://dl.acm.org/doi/10.1145/3587469},
  urldate      = {2024-01-19},
  abstract     = {Many studies within Accessible Computing have investigated image accessibility, from what should be included in alternative text (alt text), to possible automated, human-in-the-loop, or crowdsourced approaches to alt text generation. However, the processes through which practitioners make alt text               in situ               have rarely been discussed. Through interviews with three artists and three accessibility practitioners working with Google, as well as 25 end users, we identify four processes of alt text creation used by this company—The User-Evaluation Process, The Lone Writer Process, The Team Write-A-Thon Process, and The Artist-Writer Process—and unpack their potential strengths and weaknesses as they relate to access and inclusive imagery. We conclude with a discussion of what alt text researchers and industry professionals can learn from considering alt text               in situ               , including opportunities to support user feedback, cross-contributor consistency, and organizational or technical changes to production processes.},
  langid       = {english},
  file         = {C:\Users\jseo1005\Zotero\storage\MF7ID7TT\Edwards et al. - 2023 - How the Alt Text Gets Made What Roles and Process.pdf}
}

@article{elavskyDataNavigatorAccessibilityCentered2023,
  title        = {Data {{Navigator}}: {{An Accessibility-Centered Data Navigation Toolkit}}},
  shorttitle   = {Data {{Navigator}}},
  author       = {Elavsky, Frank and Nadolskis, Lucas and Moritz, Dominik},
  date         = {2023},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  shortjournal = {IEEE Trans. Visual. Comput. Graphics},
  pages        = {1--11},
  issn         = {1077-2626, 1941-0506, 2160-9306},
  doi          = {10.1109/TVCG.2023.3327393},
  url          = {https://ieeexplore.ieee.org/document/10301522/},
  urldate      = {2023-11-06},
  abstract     = {Making data visualizations accessible for people with disabilities remains a significant challenge in current practitioner efforts. Existing visualizations often lack an underlying navigable structure, fail to engage necessary input modalities, and rely heavily on visual-only rendering practices. These limitations exclude people with disabilities, especially users of assistive technologies. To address these challenges, we present Data Navigator: a system built on a dynamic graph structure, enabling developers to construct navigable lists, trees, graphs, and flows as well as spatial, diagrammatic, and geographic relations. Data Navigator supports a wide range of input modalities: screen reader, keyboard, speech, gesture detection, and even fabricated assistive devices. We present 3 case examples with Data Navigator, demonstrating we can provide accessible navigation structures on top of raster images, integrate with existing toolkits at scale, and rapidly develop novel prototypes. Data Navigator is a step towards making accessible data visualizations easier to design and implement.},
  langid       = {english},
  file         = {C:\Users\jseo1005\Zotero\storage\BJTRTD75\Elavsky et al. - 2023 - Data Navigator An Accessibility-Centered Data Navigation Toolkit.pdf}
}

@article{elavskyDataNavigatorAccessibilityCentered2024,
  title        = {Data {{Navigator}}: {{An Accessibility-Centered Data Navigation Toolkit}}},
  shorttitle   = {Data {{Navigator}}},
  author       = {Elavsky, Frank and Nadolskis, Lucas and Moritz, Dominik},
  date         = {2024-01-01},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  volume       = {30},
  number       = {01},
  pages        = {803--813},
  publisher    = {IEEE Computer Society},
  issn         = {1077-2626},
  doi          = {10.1109/TVCG.2023.3327393},
  url          = {https://www.computer.org/csdl/journal/tg/2024/01/10301522/1RFC0Gz2dEY},
  urldate      = {2024-02-25},
  abstract     = {Making data visualizations accessible for people with disabilities remains a significant challenge in current practitioner efforts. Existing visualizations often lack an underlying navigable structure, fail to engage necessary input modalities, and rely heavily on visual-only rendering practices. These limitations exclude people with disabilities, especially users of assistive technologies. To address these challenges, we present Data Navigator: a system built on a dynamic graph structure, enabling developers to construct navigable lists, trees, graphs, and flows as well as spatial, diagrammatic, and geographic relations. Data Navigator supports a wide range of input modalities: screen reader, keyboard, speech, gesture detection, and even fabricated assistive devices. We present 3 case examples with Data Navigator, demonstrating we can provide accessible navigation structures on top of raster images, integrate with existing toolkits at scale, and rapidly develop novel prototypes. Data Navigator is a step towards making accessible data visualizations easier to design and implement.},
  langid       = {english},
  file         = {C:\Users\jseo1005\Zotero\storage\3J8CVA3Z\Elavsky et al. - 2024 - Data Navigator An Accessibility-Centered Data Navigation Toolkit.pdf}
}

@article{elavskyHowAccessibleMy2022,
  title        = {How Accessible Is My Visualization? {{Evaluating}} Visualization Accessibility with {{Chartability}}},
  shorttitle   = {How Accessible Is My Visualization?},
  author       = {Elavsky, Frank and Bennett, Cynthia and Moritz, Dominik},
  date         = {2022-06},
  journaltitle = {Computer Graphics Forum},
  shortjournal = {Computer Graphics Forum},
  volume       = {41},
  number       = {3},
  pages        = {57--70},
  issn         = {0167-7055, 1467-8659},
  doi          = {10.1111/cgf.14522},
  url          = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.14522},
  urldate      = {2022-10-17},
  abstract     = {Novices and experts have struggled to evaluate the accessibility of data visualizations because there are no common shared guidelines across environments, platforms, and contexts in which data visualizations are authored. Between non-specifc standards bodies like WCAG, emerging research, and guidelines from specifc communities of practice, it is hard to organize knowledge on how to evaluate accessible data visualizations. We present Chartability, a set of heuristics synthesized from these various sources which enables designers, developers, researchers, and auditors to evaluate data-driven visualizations and interfaces for visual, motor, vestibular, neurological, and cognitive accessibility. In this paper, we outline our process of making a set of heuristics and accessibility principles for Chartability and highlight key features in the auditing process. Working with participants on real projects, we found that data practitioners with a novice level of accessibility skills were more confdent and found auditing to be easier after using Chartability. Expert accessibility practitioners were eager to integrate Chartability into their own work. Refecting on Chartability’s development and the preliminary user evaluation, we discuss tradeoffs of open projects, working with high-risk evaluations like auditing projects in the wild, and challenge future research projects at the intersection of visualization and accessibility to consider the broad intersections of disabilities.},
  langid       = {english},
  file         = {C:\Users\jseo1005\Zotero\storage\3GJLUDSM\Elavsky et al. - 2022 - How accessible is my visualization Evaluating vis.pdf}
}

@inproceedings{engelAnalysisTactileChart2017,
  title     = {Analysis of {{Tactile Chart Design}}},
  booktitle = {Proceedings of the 10th {{International Conference}} on {{PErvasive Technologies Related}} to {{Assistive Environments}}},
  author    = {Engel, Christin and Weber, Gerhard},
  date      = {2017-06-21},
  series    = {{{PETRA}} '17},
  pages     = {197--200},
  publisher = {Association for Computing Machinery},
  location  = {New York, NY, USA},
  doi       = {10.1145/3056540.3064955},
  url       = {https://dl.acm.org/doi/10.1145/3056540.3064955},
  urldate   = {2023-11-13},
  abstract  = {Tactile charts are widely used by blind people to get access to visual charts. They are often a transcription of visual charts. However, design guidelines for tactile charts are not sufficient for effective tactile chart design. An effective design supports the reader understanding the chart's underlying data. We explore how design can improve the readability of tactile charts and can support the user by getting insights from the data. We analyzed 69 tactile charts, including bar charts, line charts, pie charts, area charts, and scatter plots. The charts are taken from publications, guidelines and transcriber's institutes. In particular, we studied how axes and tick marks are designed and how labels and legends are used. Furthermore, we looked into the style of chart elements as well as design considerations for specific chart types. Based on the findings, we derived basic design guidelines for bar charts. The presented study is the first stage of our research which aims to develop design guidelines for tactile charts.},
  isbn      = {978-1-4503-5227-7},
  keywords  = {accessible charts,design guidelines,effective design,information visualization,tactile chart design,tactile graphics},
  file      = {C:\Users\jseo1005\Zotero\storage\IWD9X3XZ\Engel and Weber - 2017 - Analysis of Tactile Chart Design.pdf}
}

@inproceedings{engelAnalysisTactileChart2017a,
  title     = {Analysis of {{Tactile Chart Design}}},
  booktitle = {Proceedings of the 10th {{International Conference}} on {{PErvasive Technologies Related}} to {{Assistive Environments}}},
  author    = {Engel, Christin and Weber, Gerhard},
  date      = {2017-06-21},
  series    = {{{PETRA}} '17},
  pages     = {197--200},
  publisher = {Association for Computing Machinery},
  location  = {New York, NY, USA},
  doi       = {10.1145/3056540.3064955},
  url       = {https://dl.acm.org/doi/10.1145/3056540.3064955},
  urldate   = {2023-11-13},
  abstract  = {Tactile charts are widely used by blind people to get access to visual charts. They are often a transcription of visual charts. However, design guidelines for tactile charts are not sufficient for effective tactile chart design. An effective design supports the reader understanding the chart's underlying data. We explore how design can improve the readability of tactile charts and can support the user by getting insights from the data. We analyzed 69 tactile charts, including bar charts, line charts, pie charts, area charts, and scatter plots. The charts are taken from publications, guidelines and transcriber's institutes. In particular, we studied how axes and tick marks are designed and how labels and legends are used. Furthermore, we looked into the style of chart elements as well as design considerations for specific chart types. Based on the findings, we derived basic design guidelines for bar charts. The presented study is the first stage of our research which aims to develop design guidelines for tactile charts.},
  isbn      = {978-1-4503-5227-7},
  keywords  = {accessible charts,design guidelines,effective design,information visualization,tactile chart design,tactile graphics},
  file      = {C:\Users\jseo1005\Zotero\storage\H8I6Q824\Engel and Weber - 2017 - Analysis of Tactile Chart Design.pdf}
}

@inproceedings{engelSVGPlottAccessibleTool2019,
  title      = {{{SVGPlott}}: An Accessible Tool to Generate Highly Adaptable, Accessible Audio-Tactile Charts for and from Blind and Visually Impaired People},
  shorttitle = {{{SVGPlott}}},
  booktitle  = {Proceedings of the 12th {{ACM International Conference}} on {{PErvasive Technologies Related}} to {{Assistive Environments}}},
  author     = {Engel, Christin and Müller, Emma Franziska and Weber, Gerhard},
  date       = {2019-06-05},
  series     = {{{PETRA}} '19},
  pages      = {186--195},
  publisher  = {Association for Computing Machinery},
  location   = {New York, NY, USA},
  doi        = {10.1145/3316782.3316793},
  url        = {https://dl.acm.org/doi/10.1145/3316782.3316793},
  urldate    = {2024-02-21},
  abstract   = {Charts, such as bar or line charts, are used in many different areas, newspapers, in education, and other areas of life. Accessing charts or understanding data is a main requirement in various professions. Audio-tactile charts can be explored by touch. Producing tactile charts takes a high effort and the design is challenging. Especially, blind and visually impaired people are excluded from chart creation. That is why we investigated several studies and developed a concept for a tool that aims to improve the creation process as well as the quality of audio-tactile charts. We present our concept and describe ideas and properties for user input, input data, graphical user interface, the rendering process as well as the output files in detail. Afterwards, we present our current implementation that includes an accessible graphical user interface which supports well-designed default parameters for chart generation. Most properties are customizable while the GUI supports a live preview of the current chart. In addition, an accessible legend and description will be generated. We support the generation of grouped and stacked bar charts, line charts, and scatterplots in an accessible SVG format. We evaluated the resulting charts, the usability and accessibility of the GUI within two pilot studies with blind and sighted people where we find implications for further investigations.},
  isbn       = {978-1-4503-6232-0},
  keywords   = {accessible graphics,audio-tactile charts,automation tool,blind and visually impaired people,data visualization,effective design,tactile charts},
  file       = {C:\Users\jseo1005\Zotero\storage\9PU9DQPY\Engel et al. - 2019 - SVGPlott an accessible tool to generate highly ad.pdf}
}

@online{ExperienceLearnEducational,
  title   = {Experience + {{Learn}} / {{Educational Media}} / {{Effective Practices}} for {{Description}} of {{Science Content}} within {{Digital Talking Books}} / {{NCAM}}},
  url     = {http://ncamftp.wgbh.org/ncam-old-site/experience_learn/educational_media/stemdx.html},
  urldate = {2023-08-28}
}

@article{fanAccessibilityDataVisualizations2022,
  title        = {The {{Accessibility}} of {{Data Visualizations}} on the {{Web}} for {{Screen Reader Users}}: {{Practices}} and {{Experiences During COVID-19}}},
  shorttitle   = {The {{Accessibility}} of {{Data Visualizations}} on the {{Web}} for {{Screen Reader Users}}},
  author       = {Fan, Danyang and Siu, Alexa F. and Rao, Hrishikesh V. and Kim, Gene S-H and Vazquez, Xavier and Greco, Lucy and O’Modhrain, Sile and Follmer, Sean},
  date         = {2022-08-18},
  journaltitle = {ACM Transactions on Accessible Computing},
  shortjournal = {ACM Trans. Access. Comput.},
  pages        = {3557899},
  issn         = {1936-7228, 1936-7236},
  doi          = {10.1145/3557899},
  url          = {https://dl.acm.org/doi/10.1145/3557899},
  urldate      = {2023-01-12},
  abstract     = {Data visualization has become an increasingly important means of efective data communication and has played a vital role in broadcasting the progression of COVID-19. Accessible data representations, on the other hand, have lagged behind, leaving areas of information out of reach for many blind and visually impaired (BVI) users. In this work, we sought to understand (1) the accessibility of current implementations of visualizations on the web; (2) BVI users’ preferences and current experiences when accessing data-driven media; (3) how accessible data representations on the web address these users’ access needs and help them navigate, interpret, and gain insights from the data; and (4) the practical challenges that limit BVI users’ access and use of data representations. To answer these questions, we conducted a mixed-methods study consisting of an accessibility audit of 87 data visualizations on the web to identify accessibility issues, an online survey of 127 screen reader users to understand lived experiences and preferences, and a remote contextual inquiry with 12 of the survey respondents to observe how they navigate, interpret and gain insights from accessible data representations. Our observations during this critical period of time provide an understanding of the widespread accessibility issues encountered across online data visualizations, the impact that data accessibility inequities have on the BVI community, the ways screen reader users sought access to data-driven information and made use of online visualizations to form insights, and the pressing need to make larger strides towards improving data literacy, building conidence, and enriching methods of access. Based on our indings, we provide recommendations for researchers and practitioners to broaden data accessibility on the web. CCS Concepts: · Human-centered computing → Empirical studies in accessibility; Visualization application domains.},
  langid       = {english},
  file         = {C:\Users\jseo1005\Zotero\storage\Y6Q8L9IR\Fan et al. - 2022 - The Accessibility of Data Visualizations on the We.pdf}
}

@article{fanAccessibilityDataVisualizations2023,
  title        = {The {{Accessibility}} of {{Data Visualizations}} on the {{Web}} for {{Screen Reader Users}}: {{Practices}} and {{Experiences During COVID-19}}},
  shorttitle   = {The {{Accessibility}} of {{Data Visualizations}} on the {{Web}} for {{Screen Reader Users}}},
  author       = {Fan, Danyang and Fay Siu, Alexa and Rao, Hrishikesh and Kim, Gene Sung-Ho and Vazquez, Xavier and Greco, Lucy and O'Modhrain, Sile and Follmer, Sean},
  date         = {2023-03-29},
  journaltitle = {ACM Transactions on Accessible Computing},
  shortjournal = {ACM Trans. Access. Comput.},
  volume       = {16},
  number       = {1},
  pages        = {4:1--4:29},
  issn         = {1936-7228},
  doi          = {10.1145/3557899},
  url          = {https://dl.acm.org/doi/10.1145/3557899},
  urldate      = {2023-08-27},
  abstract     = {Data visualization has become an increasingly important means of effective data communication and has played a vital role in broadcasting the progression of COVID-19. Accessible data representations, however, have lagged behind, leaving areas of information out of reach for many blind and visually impaired (BVI) users. In this work, we sought to understand (1) the accessibility of current implementations of visualizations on the web; (2) BVI users’ preferences and current experiences when accessing data-driven media; (3) how accessible data representations on the web address these users’ access needs and help them navigate, interpret, and gain insights from the data; and (4) the practical challenges that limit BVI users’ access and use of data representations. To answer these questions, we conducted a mixed-methods study consisting of an accessibility audit of 87 data visualizations on the web to identify accessibility issues, an online survey of 127 screen reader users to understand lived experiences and preferences, and a remote contextual inquiry with 12 of the survey respondents to observe how they navigate, interpret, and gain insights from accessible data representations. Our observations during this critical period of time provide an understanding of the widespread accessibility issues encountered across online data visualizations, the impact that data accessibility inequities have on the BVI community, the ways screen reader users sought access to data-driven information and made use of online visualizations to form insights, and the pressing need to make larger strides towards improving data literacy, building confidence, and enriching methods of access. Based on our findings, we provide recommendations for researchers and practitioners to broaden data accessibility on the web.},
  keywords     = {Accessibility,accessible data visualization,audit,blind,data visualization,user experience,visually impaired,web accessibility},
  file         = {C:\Users\jseo1005\Zotero\storage\L48LCCW3\Fan et al. - 2023 - The Accessibility of Data Visualizations on the We.pdf}
}

@inproceedings{fanSlideToneTiltTone1DOF2022,
  title      = {Slide-{{Tone}} and {{Tilt-Tone}}: 1-{{DOF Haptic Techniques}} for {{Conveying Shape Characteristics}} of {{Graphs}} to {{Blind Users}}},
  shorttitle = {Slide-{{Tone}} and {{Tilt-Tone}}},
  booktitle  = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author     = {Fan, Danyang and Siu, Alexa Fay and Law, Wing-Sum Adrienne and Zhen, Raymond Ruihong and O'Modhrain, Sile and Follmer, Sean},
  date       = {2022-04-29},
  pages      = {1--19},
  publisher  = {ACM},
  location   = {New Orleans LA USA},
  doi        = {10.1145/3491102.3517790},
  url        = {https://dl.acm.org/doi/10.1145/3491102.3517790},
  urldate    = {2023-01-12},
  eventtitle = {{{CHI}} '22: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn       = {978-1-4503-9157-3},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\KTLG8G4D\Fan et al. - 2022 - Slide-Tone and Tilt-Tone 1-DOF Haptic Techniques .pdf}
}

@inproceedings{farihaMiningFrequentPatterns2013,
  title     = {Mining {{Frequent Patterns}} from {{Human Interactions}} in {{Meetings Using Directed Acyclic Graphs}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  author    = {Fariha, Anna and Ahmed, Chowdhury Farhan and Leung, Carson Kai-Sang and Abdullah, S. M. and Cao, Longbing},
  editor    = {Pei, Jian and Tseng, Vincent S. and Cao, Longbing and Motoda, Hiroshi and Xu, Guandong},
  date      = {2013},
  series    = {Lecture {{Notes}} in {{Computer Science}}},
  pages     = {38--49},
  publisher = {Springer},
  location  = {Berlin, Heidelberg},
  doi       = {10.1007/978-3-642-37453-1_4},
  abstract  = {In modern life, interactions between human beings frequently occur in meetings, where topics are discussed. Semantic knowledge of meetings can be revealed by discovering interaction patterns from these meetings. An existing method mines interaction patterns from meetings using tree structures. However, such a tree-based method may not capture all kinds of triggering relations between interactions, and it may not distinguish a participant of a certain rank from another participant of a different rank in a meeting. Hence, the tree-based method may not be able to find all interaction patterns such as those about correlated interaction. In this paper, we propose to mine interaction patterns from meetings using an alternative data structure—namely, a directed acyclic graph (DAG). Specifically, a DAG captures both temporal and triggering relations between interactions in meetings. Moreover, to distinguish one participant of a certain rank from another, we assign weights to nodes in the DAG. As such, a meeting can be modeled as a weighted DAG, from which weighted frequent interaction patterns can be discovered. Experimental results showed the effectiveness of our proposed DAG-based method for mining interaction patterns from meetings.},
  isbn      = {978-3-642-37453-1},
  langid    = {english},
  keywords  = {Data mining,Directed Acyclic Graphs,Frequent patterns,Human interaction,Modeling meetings},
  file      = {C:\Users\jseo1005\Zotero\storage\M2HRTAMH\Fariha et al. - 2013 - Mining Frequent Patterns from Human Interactions i.pdf}
}

@article{fengChartUnderstandingLarge2023,
  title     = {Chart {{Understanding}} with {{Large Language Model}}},
  author    = {Feng, John},
  date      = {2023-12-12},
  publisher = {Engineering Archive},
  doi       = {10.31224/3401},
  url       = {https://engrxiv.org/preprint/view/3401},
  urldate   = {2024-03-25},
  langid    = {english},
  file      = {C:\Users\jseo1005\Zotero\storage\WBH6E445\Feng - 2023 - Chart Understanding with Large Language Model.pdf}
}

@inproceedings{fitzpatrickProducingAccessibleStatistics2017,
  title      = {Producing {{Accessible Statistics Diagrams}} in {{R}}},
  booktitle  = {Proceedings of the 14th {{International Web}} for {{All Conference}}},
  author     = {Fitzpatrick, Donal and Godfrey, A. Jonathan R. and Sorge, Volker},
  date       = {2017-04-02},
  pages      = {1--4},
  publisher  = {ACM},
  location   = {Perth Western Australia Australia},
  doi        = {10.1145/3058555.3058564},
  url        = {https://dl.acm.org/doi/10.1145/3058555.3058564},
  urldate    = {2022-08-21},
  abstract   = {Blind people are at risk of being left behind in the infor­ mation age if efforts are not made to improve the access to information that is not traditionally conveyed in text, whether that text be accessed in braille, audio, or a com­ puter’s screen reading software. Most graphics summarise a scene or some aspect of data that the author hopes will in­ form their audience; good statistical graphics are commonly used to great effect for the sighted world, but are practi­ cally useless to a blind audience. Our work aims to provide an accessible way for blind users to easily, efficiently, and most importantly accurately, explore and query the data contained in diagrams such as bar charts, box plots, time series, and many more. We employ the statistical software environment R to compute rich semantics for these diagrams and make them web accessible by supporting screen reading and interactive exploration.},
  eventtitle = {{{W4A}} '17: {{Web For All}} 2017 - {{The Future}} of {{Accessible Work}}},
  isbn       = {978-1-4503-4900-0},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\ASU5R8I9\Fitzpatrick et al. - 2017 - Producing Accessible Statistics Diagrams in R.pdf}
}

@inproceedings{fitzpatrickProducingAccessibleStatistics2017a,
  title     = {Producing {{Accessible Statistics Diagrams}} in {{R}}},
  booktitle = {Proceedings of the 14th {{International Web}} for {{All Conference}}},
  author    = {Fitzpatrick, Donal and Godfrey, A. Jonathan R. and Sorge, Volker},
  date      = {2017-04-02},
  series    = {{{W4A}} '17},
  pages     = {1--4},
  publisher = {Association for Computing Machinery},
  location  = {New York, NY, USA},
  doi       = {10.1145/3058555.3058564},
  url       = {https://dl.acm.org/doi/10.1145/3058555.3058564},
  urldate   = {2023-11-13},
  abstract  = {Blind people are at risk of being left behind in the information age if efforts are not made to improve the access to information that is not traditionally conveyed in text, whether that text be accessed in braille, audio, or a computer's screen reading software. Most graphics summarise a scene or some aspect of data that the author hopes will inform their audience; good statistical graphics are commonly used to great effect for the sighted world, but are practically useless to a blind audience. Our work aims to provide an accessible way for blind users to easily, efficiently, and most importantly accurately, explore and query the data contained in diagrams such as bar charts, box plots, time series, and many more. We employ the statistical software environment R to compute rich semantics for these diagrams and make them web accessible by supporting screen reading and interactive exploration.},
  isbn      = {978-1-4503-4900-0},
  keywords  = {accessible diagrams,Statistics,STEM Accessibility},
  file      = {C:\Users\jseo1005\Zotero\storage\8S2IXJ4G\Fitzpatrick et al. - 2017 - Producing Accessible Statistics Diagrams in R.pdf}
}

@inproceedings{fitzpatrickProducingAccessibleStatistics2017b,
  title     = {Producing {{Accessible Statistics Diagrams}} in {{R}}},
  booktitle = {Proceedings of the 14th {{International Web}} for {{All Conference}}},
  author    = {Fitzpatrick, Donal and Godfrey, A. Jonathan R. and Sorge, Volker},
  date      = {2017-04-02},
  series    = {{{W4A}} '17},
  pages     = {1--4},
  publisher = {Association for Computing Machinery},
  location  = {New York, NY, USA},
  doi       = {10.1145/3058555.3058564},
  url       = {https://doi.org/10.1145/3058555.3058564},
  urldate   = {2024-01-24},
  abstract  = {Blind people are at risk of being left behind in the information age if efforts are not made to improve the access to information that is not traditionally conveyed in text, whether that text be accessed in braille, audio, or a computer's screen reading software. Most graphics summarise a scene or some aspect of data that the author hopes will inform their audience; good statistical graphics are commonly used to great effect for the sighted world, but are practically useless to a blind audience. Our work aims to provide an accessible way for blind users to easily, efficiently, and most importantly accurately, explore and query the data contained in diagrams such as bar charts, box plots, time series, and many more. We employ the statistical software environment R to compute rich semantics for these diagrams and make them web accessible by supporting screen reading and interactive exploration.},
  isbn      = {978-1-4503-4900-0},
  keywords  = {accessible diagrams,Statistics,STEM Accessibility},
  file      = {C:\Users\jseo1005\Zotero\storage\KKLCSZLP\Fitzpatrick et al. - 2017 - Producing Accessible Statistics Diagrams in R.pdf}
}

@online{FormInputBindings,
  title   = {Form {{Input Bindings}} | {{Vue}}.Js},
  url     = {https://vuejs.org/guide/essentials/forms.html#select},
  urldate = {2022-09-24}
}

@inproceedings{gargBraille8UnifiedBraille2016,
  title      = {Braille-8 — {{The}} Unified Braille {{Unicode}} System: {{Presenting}} an Ideal Unified System around 8-Dot {{Braille Unicode}} for the Braille Users World-Over},
  shorttitle = {Braille-8 — {{The}} Unified Braille {{Unicode}} System},
  booktitle  = {2016 {{IEEE International Conference}} on {{Advanced Networks}} and {{Telecommunications Systems}} ({{ANTS}})},
  author     = {Garg, Anupam Kumar},
  date       = {2016-11},
  pages      = {1--6},
  doi        = {10.1109/ANTS.2016.7947839},
  abstract   = {Traditional Braille is a 6-dot code that can represent maximum 64 unique symbols with each braille cell. This is grossly insufficient to represent even ordinary English text (comprising 26 small letters, 26 capital letters, 10 digits, and 14 basic punctuations) - let alone math and science symbols. Thus a braille user has to enter 2 (and sometime 3 or 4) braille cells to enter one character or symbol. This makes braille writing very slow and tedious. Incidentally, 8-dot Braille Unicode was introduced to facilitate the Computer Braille that could represent all 95 computer characters with one braille cell itself. Since 8-dot braille can represent maximum 256 unique symbols, it has huge potential to provide the ultimate solution to all woes faced by braille users while writing texts (in English or in native languages) as well as mathematical and technical text. This paper presents a comprehensive unified braille Unicode system providing a detailed mapping of 8-dot braille Unicode pattern to represent the transcribing codes (in English or any other language) as well as the math, science, and computer symbols/characters - mostly with one braille cell itself.},
  eventtitle = {2016 {{IEEE International Conference}} on {{Advanced Networks}} and {{Telecommunications Systems}} ({{ANTS}})},
  keywords   = {blind,braille pattern,braille standard,Braille Unicode,braille user,Braille-8,computer braille,Computers,Context,eight-dot braille code,Encoding,Geometry,Set theory,Standards,visually challenged,Writing},
  file       = {C\:\\Users\\jseo1005\\Zotero\\storage\\WE3NJT7N\\Garg - 2016 - Braille-8 — The unified braille Unicode system Pr.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\LBYE5IYN\\authors.html}
}

@online{geminiteamGeminiFamilyHighly2024,
  title       = {Gemini: {{A Family}} of {{Highly Capable Multimodal Models}}},
  shorttitle  = {Gemini},
  author      = {Gemini Team and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M. and Hauth, Anja and Millican, Katie and Silver, David and Johnson, Melvin and Antonoglou, Ioannis and Schrittwieser, Julian and Glaese, Amelia and Chen, Jilin and Pitler, Emily and Lillicrap, Timothy and Lazaridou, Angeliki and Firat, Orhan and Molloy, James and Isard, Michael and Barham, Paul R. and Hennigan, Tom and Lee, Benjamin and Viola, Fabio and Reynolds, Malcolm and Xu, Yuanzhong and Doherty, Ryan and Collins, Eli and Meyer, Clemens and Rutherford, Eliza and Moreira, Erica and Ayoub, Kareem and Goel, Megha and Krawczyk, Jack and Du, Cosmo and Chi, Ed and Cheng, Heng-Tze and Ni, Eric and Shah, Purvi and Kane, Patrick and Chan, Betty and Faruqui, Manaal and Severyn, Aliaksei and Lin, Hanzhao and Li, YaGuang and Cheng, Yong and Ittycheriah, Abe and Mahdieh, Mahdis and Chen, Mia and Sun, Pei and Tran, Dustin and Bagri, Sumit and Lakshminarayanan, Balaji and Liu, Jeremiah and Orban, Andras and Güra, Fabian and Zhou, Hao and Song, Xinying and Boffy, Aurelien and Ganapathy, Harish and Zheng, Steven and Choe, HyunJeong and Weisz, Ágoston and Zhu, Tao and Lu, Yifeng and Gopal, Siddharth and Kahn, Jarrod and Kula, Maciej and Pitman, Jeff and Shah, Rushin and Taropa, Emanuel and Merey, Majd Al and Baeuml, Martin and Chen, Zhifeng and Shafey, Laurent El and Zhang, Yujing and Sercinoglu, Olcan and Tucker, George and Piqueras, Enrique and Krikun, Maxim and Barr, Iain and Savinov, Nikolay and Danihelka, Ivo and Roelofs, Becca and White, Anaïs and Andreassen, Anders and family=Glehn, given=Tamara, prefix=von, useprefix=true and Yagati, Lakshman and Kazemi, Mehran and Gonzalez, Lucas and Khalman, Misha and Sygnowski, Jakub and Frechette, Alexandre and Smith, Charlotte and Culp, Laura and Proleev, Lev and Luan, Yi and Chen, Xi and Lottes, James and Schucher, Nathan and Lebron, Federico and Rrustemi, Alban and Clay, Natalie and Crone, Phil and Kocisky, Tomas and Zhao, Jeffrey and Perz, Bartek and Yu, Dian and Howard, Heidi and Bloniarz, Adam and Rae, Jack W. and Lu, Han and Sifre, Laurent and Maggioni, Marcello and Alcober, Fred and Garrette, Dan and Barnes, Megan and Thakoor, Shantanu and Austin, Jacob and Barth-Maron, Gabriel and Wong, William and Joshi, Rishabh and Chaabouni, Rahma and Fatiha, Deeni and Ahuja, Arun and Tomar, Gaurav Singh and Senter, Evan and Chadwick, Martin and Kornakov, Ilya and Attaluri, Nithya and Iturrate, Iñaki and Liu, Ruibo and Li, Yunxuan and Cogan, Sarah and Chen, Jeremy and Jia, Chao and Gu, Chenjie and Zhang, Qiao and Grimstad, Jordan and Hartman, Ale Jakse and Garcia, Xavier and Pillai, Thanumalayan Sankaranarayana and Devlin, Jacob and Laskin, Michael and Casas, Diego de Las and Valter, Dasha and Tao, Connie and Blanco, Lorenzo and Badia, Adrià Puigdomènech and Reitter, David and Chen, Mianna and Brennan, Jenny and Rivera, Clara and Brin, Sergey and Iqbal, Shariq and Surita, Gabriela and Labanowski, Jane and Rao, Abhi and Winkler, Stephanie and Parisotto, Emilio and Gu, Yiming and Olszewska, Kate and Addanki, Ravi and Miech, Antoine and Louis, Annie and Teplyashin, Denis and Brown, Geoff and Catt, Elliot and Balaguer, Jan and Xiang, Jackie and Wang, Pidong and Ashwood, Zoe and Briukhov, Anton and Webson, Albert and Ganapathy, Sanjay and Sanghavi, Smit and Kannan, Ajay and Chang, Ming-Wei and Stjerngren, Axel and Djolonga, Josip and Sun, Yuting and Bapna, Ankur and Aitchison, Matthew and Pejman, Pedram and Michalewski, Henryk and Yu, Tianhe and Wang, Cindy and Love, Juliette and Ahn, Junwhan and Bloxwich, Dawn and Han, Kehang and Humphreys, Peter and Sellam, Thibault and Bradbury, James and Godbole, Varun and Samangooei, Sina and Damoc, Bogdan and Kaskasoli, Alex and Arnold, Sébastien M. R. and Vasudevan, Vijay and Agrawal, Shubham and Riesa, Jason and Lepikhin, Dmitry and Tanburn, Richard and Srinivasan, Srivatsan and Lim, Hyeontaek and Hodkinson, Sarah and Shyam, Pranav and Ferret, Johan and Hand, Steven and Garg, Ankush and Paine, Tom Le and Li, Jian and Li, Yujia and Giang, Minh and Neitz, Alexander and Abbas, Zaheer and York, Sarah and Reid, Machel and Cole, Elizabeth and Chowdhery, Aakanksha and Das, Dipanjan and Rogozińska, Dominika and Nikolaev, Vitaliy and Sprechmann, Pablo and Nado, Zachary and Zilka, Lukas and Prost, Flavien and He, Luheng and Monteiro, Marianne and Mishra, Gaurav and Welty, Chris and Newlan, Josh and Jia, Dawei and Allamanis, Miltiadis and Hu, Clara Huiyi and family=Liedekerke, given=Raoul, prefix=de, useprefix=true and Gilmer, Justin and Saroufim, Carl and Rijhwani, Shruti and Hou, Shaobo and Shrivastava, Disha and Baddepudi, Anirudh and Goldin, Alex and Ozturel, Adnan and Cassirer, Albin and Xu, Yunhan and Sohn, Daniel and Sachan, Devendra and Amplayo, Reinald Kim and Swanson, Craig and Petrova, Dessie and Narayan, Shashi and Guez, Arthur and Brahma, Siddhartha and Landon, Jessica and Patel, Miteyan and Zhao, Ruizhe and Villela, Kevin and Wang, Luyu and Jia, Wenhao and Rahtz, Matthew and Giménez, Mai and Yeung, Legg and Keeling, James and Georgiev, Petko and Mincu, Diana and Wu, Boxi and Haykal, Salem and Saputro, Rachel and Vodrahalli, Kiran and Qin, James and Cankara, Zeynep and Sharma, Abhanshu and Fernando, Nick and Hawkins, Will and Neyshabur, Behnam and Kim, Solomon and Hutter, Adrian and Agrawal, Priyanka and Castro-Ros, Alex and family=Driessche, given=George, prefix=van den, useprefix=false and Wang, Tao and Yang, Fan and Chang, Shuo-yiin and Komarek, Paul and McIlroy, Ross and Lučić, Mario and Zhang, Guodong and Farhan, Wael and Sharman, Michael and Natsev, Paul and Michel, Paul and Bansal, Yamini and Qiao, Siyuan and Cao, Kris and Shakeri, Siamak and Butterfield, Christina and Chung, Justin and Rubenstein, Paul Kishan and Agrawal, Shivani and Mensch, Arthur and Soparkar, Kedar and Lenc, Karel and Chung, Timothy and Pope, Aedan and Maggiore, Loren and Kay, Jackie and Jhakra, Priya and Wang, Shibo and Maynez, Joshua and Phuong, Mary and Tobin, Taylor and Tacchetti, Andrea and Trebacz, Maja and Robinson, Kevin and Katariya, Yash and Riedel, Sebastian and Bailey, Paige and Xiao, Kefan and Ghelani, Nimesh and Aroyo, Lora and Slone, Ambrose and Houlsby, Neil and Xiong, Xuehan and Yang, Zhen and Gribovskaya, Elena and Adler, Jonas and Wirth, Mateo and Lee, Lisa and Li, Music and Kagohara, Thais and Pavagadhi, Jay and Bridgers, Sophie and Bortsova, Anna and Ghemawat, Sanjay and Ahmed, Zafarali and Liu, Tianqi and Powell, Richard and Bolina, Vijay and Iinuma, Mariko and Zablotskaia, Polina and Besley, James and Chung, Da-Woon and Dozat, Timothy and Comanescu, Ramona and Si, Xiance and Greer, Jeremy and Su, Guolong and Polacek, Martin and Kaufman, Raphaël Lopez and Tokumine, Simon and Hu, Hexiang and Buchatskaya, Elena and Miao, Yingjie and Elhawaty, Mohamed and Siddhant, Aditya and Tomasev, Nenad and Xing, Jinwei and Greer, Christina and Miller, Helen and Ashraf, Shereen and Roy, Aurko and Zhang, Zizhao and Ma, Ada and Filos, Angelos and Besta, Milos and Blevins, Rory and Klimenko, Ted and Yeh, Chih-Kuan and Changpinyo, Soravit and Mu, Jiaqi and Chang, Oscar and Pajarskas, Mantas and Muir, Carrie and Cohen, Vered and Lan, Charline Le and Haridasan, Krishna and Marathe, Amit and Hansen, Steven and Douglas, Sholto and Samuel, Rajkumar and Wang, Mingqiu and Austin, Sophia and Lan, Chang and Jiang, Jiepu and Chiu, Justin and Lorenzo, Jaime Alonso and Sjösund, Lars Lowe and Cevey, Sébastien and Gleicher, Zach and Avrahami, Thi and Boral, Anudhyan and Srinivasan, Hansa and Selo, Vittorio and May, Rhys and Aisopos, Konstantinos and Hussenot, Léonard and Soares, Livio Baldini and Baumli, Kate and Chang, Michael B. and Recasens, Adrià and Caine, Ben and Pritzel, Alexander and Pavetic, Filip and Pardo, Fabio and Gergely, Anita and Frye, Justin and Ramasesh, Vinay and Horgan, Dan and Badola, Kartikeya and Kassner, Nora and Roy, Subhrajit and Dyer, Ethan and Campos, Víctor Campos and Tomala, Alex and Tang, Yunhao and Badawy, Dalia El and White, Elspeth and Mustafa, Basil and Lang, Oran and Jindal, Abhishek and Vikram, Sharad and Gong, Zhitao and Caelles, Sergi and Hemsley, Ross and Thornton, Gregory and Feng, Fangxiaoyu and Stokowiec, Wojciech and Zheng, Ce and Thacker, Phoebe and Ünlü, Çağlar and Zhang, Zhishuai and Saleh, Mohammad and Svensson, James and Bileschi, Max and Patil, Piyush and Anand, Ankesh and Ring, Roman and Tsihlas, Katerina and Vezer, Arpi and Selvi, Marco and Shevlane, Toby and Rodriguez, Mikel and Kwiatkowski, Tom and Daruki, Samira and Rong, Keran and Dafoe, Allan and FitzGerald, Nicholas and Gu-Lemberg, Keren and Khan, Mina and Hendricks, Lisa Anne and Pellat, Marie and Feinberg, Vladimir and Cobon-Kerr, James and Sainath, Tara and Rauh, Maribeth and Hashemi, Sayed Hadi and Ives, Richard and Hasson, Yana and Noland, Eric and Cao, Yuan and Byrd, Nathan and Hou, Le and Wang, Qingze and Sottiaux, Thibault and Paganini, Michela and Lespiau, Jean-Baptiste and Moufarek, Alexandre and Hassan, Samer and Shivakumar, Kaushik and family=Amersfoort, given=Joost, prefix=van, useprefix=true and Mandhane, Amol and Joshi, Pratik and Goyal, Anirudh and Tung, Matthew and Brock, Andrew and Sheahan, Hannah and Misra, Vedant and Li, Cheng and Rakićević, Nemanja and Dehghani, Mostafa and Liu, Fangyu and Mittal, Sid and Oh, Junhyuk and Noury, Seb and Sezener, Eren and Huot, Fantine and Lamm, Matthew and De Cao, Nicola and Chen, Charlie and Mudgal, Sidharth and Stella, Romina and Brooks, Kevin and Vasudevan, Gautam and Liu, Chenxi and Chain, Mainak and Melinkeri, Nivedita and Cohen, Aaron and Wang, Venus and Seymore, Kristie and Zubkov, Sergey and Goel, Rahul and Yue, Summer and Krishnakumaran, Sai and Albert, Brian and Hurley, Nate and Sano, Motoki and Mohananey, Anhad and Joughin, Jonah and Filonov, Egor and Kępa, Tomasz and Eldawy, Yomna and Lim, Jiawern and Rishi, Rahul and Badiezadegan, Shirin and Bos, Taylor and Chang, Jerry and Jain, Sanil and Padmanabhan, Sri Gayatri Sundara and Puttagunta, Subha and Krishna, Kalpesh and Baker, Leslie and Kalb, Norbert and Bedapudi, Vamsi and Kurzrok, Adam and Lei, Shuntong and Yu, Anthony and Litvin, Oren and Zhou, Xiang and Wu, Zhichun and Sobell, Sam and Siciliano, Andrea and Papir, Alan and Neale, Robby and Bragagnolo, Jonas and Toor, Tej and Chen, Tina and Anklin, Valentin and Wang, Feiran and Feng, Richie and Gholami, Milad and Ling, Kevin and Liu, Lijuan and Walter, Jules and Moghaddam, Hamid and Kishore, Arun and Adamek, Jakub and Mercado, Tyler and Mallinson, Jonathan and Wandekar, Siddhinita and Cagle, Stephen and Ofek, Eran and Garrido, Guillermo and Lombriser, Clemens and Mukha, Maksim and Sun, Botu and Mohammad, Hafeezul Rahman and Matak, Josip and Qian, Yadi and Peswani, Vikas and Janus, Pawel and Yuan, Quan and Schelin, Leif and David, Oana and Garg, Ankur and He, Yifan and Duzhyi, Oleksii and Älgmyr, Anton and Lottaz, Timothée and Li, Qi and Yadav, Vikas and Xu, Luyao and Chinien, Alex and Shivanna, Rakesh and Chuklin, Aleksandr and Li, Josie and Spadine, Carrie and Wolfe, Travis and Mohamed, Kareem and Das, Subhabrata and Dai, Zihang and He, Kyle and family=Dincklage, given=Daniel, prefix=von, useprefix=true and Upadhyay, Shyam and Maurya, Akanksha and Chi, Luyan and Krause, Sebastian and Salama, Khalid and Rabinovitch, Pam G. and M, Pavan Kumar Reddy and Selvan, Aarush and Dektiarev, Mikhail and Ghiasi, Golnaz and Guven, Erdem and Gupta, Himanshu and Liu, Boyi and Sharma, Deepak and Shtacher, Idan Heimlich and Paul, Shachi and Akerlund, Oscar and Aubet, François-Xavier and Huang, Terry and Zhu, Chen and Zhu, Eric and Teixeira, Elico and Fritze, Matthew and Bertolini, Francesco and Marinescu, Liana-Eleonora and Bölle, Martin and Paulus, Dominik and Gupta, Khyatti and Latkar, Tejasi and Chang, Max and Sanders, Jason and Wilson, Roopa and Wu, Xuewei and Tan, Yi-Xuan and Thiet, Lam Nguyen and Doshi, Tulsee and Lall, Sid and Mishra, Swaroop and Chen, Wanming and Luong, Thang and Benjamin, Seth and Lee, Jasmine and Andrejczuk, Ewa and Rabiej, Dominik and Ranjan, Vipul and Styrc, Krzysztof and Yin, Pengcheng and Simon, Jon and Harriott, Malcolm Rose and Bansal, Mudit and Robsky, Alexei and Bacon, Geoff and Greene, David and Mirylenka, Daniil and Zhou, Chen and Sarvana, Obaid and Goyal, Abhimanyu and Andermatt, Samuel and Siegler, Patrick and Horn, Ben and Israel, Assaf and Pongetti, Francesco and Chen, Chih-Wei "Louis" and Selvatici, Marco and Silva, Pedro and Wang, Kathie and Tolins, Jackson and Guu, Kelvin and Yogev, Roey and Cai, Xiaochen and Agostini, Alessandro and Shah, Maulik and Nguyen, Hung and Donnaile, Noah Ó and Pereira, Sébastien and Friso, Linda and Stambler, Adam and Kurzrok, Adam and Kuang, Chenkai and Romanikhin, Yan and Geller, Mark and Yan, Z. J. and Jang, Kane and Lee, Cheng-Chun and Fica, Wojciech and Malmi, Eric and Tan, Qijun and Banica, Dan and Balle, Daniel and Pham, Ryan and Huang, Yanping and Avram, Diana and Shi, Hongzhi and Singh, Jasjot and Hidey, Chris and Ahuja, Niharika and Saxena, Pranab and Dooley, Dan and Potharaju, Srividya Pranavi and O'Neill, Eileen and Gokulchandran, Anand and Foley, Ryan and Zhao, Kai and Dusenberry, Mike and Liu, Yuan and Mehta, Pulkit and Kotikalapudi, Ragha and Safranek-Shrader, Chalence and Goodman, Andrew and Kessinger, Joshua and Globen, Eran and Kolhar, Prateek and Gorgolewski, Chris and Ibrahim, Ali and Song, Yang and Eichenbaum, Ali and Brovelli, Thomas and Potluri, Sahitya and Lahoti, Preethi and Baetu, Cip and Ghorbani, Ali and Chen, Charles and Crawford, Andy and Pal, Shalini and Sridhar, Mukund and Gurita, Petru and Mujika, Asier and Petrovski, Igor and Cedoz, Pierre-Louis and Li, Chenmei and Chen, Shiyuan and Santo, Niccolò Dal and Goyal, Siddharth and Punjabi, Jitesh and Kappaganthu, Karthik and Kwak, Chester and LV, Pallavi and Velury, Sarmishta and Choudhury, Himadri and Hall, Jamie and Shah, Premal and Figueira, Ricardo and Thomas, Matt and Lu, Minjie and Zhou, Ting and Kumar, Chintu and Jurdi, Thomas and Chikkerur, Sharat and Ma, Yenai and Yu, Adams and Kwak, Soo and Ähdel, Victor and Rajayogam, Sujeevan and Choma, Travis and Liu, Fei and Barua, Aditya and Ji, Colin and Park, Ji Ho and Hellendoorn, Vincent and Bailey, Alex and Bilal, Taylan and Zhou, Huanjie and Khatir, Mehrdad and Sutton, Charles and Rzadkowski, Wojciech and Macintosh, Fiona and Shagin, Konstantin and Medina, Paul and Liang, Chen and Zhou, Jinjing and Shah, Pararth and Bi, Yingying and Dankovics, Attila and Banga, Shipra and Lehmann, Sabine and Bredesen, Marissa and Lin, Zifan and Hoffmann, John Eric and Lai, Jonathan and Chung, Raynald and Yang, Kai and Balani, Nihal and Bražinskas, Arthur and Sozanschi, Andrei and Hayes, Matthew and Alcalde, Héctor Fernández and Makarov, Peter and Chen, Will and Stella, Antonio and Snijders, Liselotte and Mandl, Michael and Kärrman, Ante and Nowak, Paweł and Wu, Xinyi and Dyck, Alex and Vaidyanathan, Krishnan and R, Raghavender and Mallet, Jessica and Rudominer, Mitch and Johnston, Eric and Mittal, Sushil and Udathu, Akhil and Christensen, Janara and Verma, Vishal and Irving, Zach and Santucci, Andreas and Elsayed, Gamaleldin and Davoodi, Elnaz and Georgiev, Marin and Tenney, Ian and Hua, Nan and Cideron, Geoffrey and Leurent, Edouard and Alnahlawi, Mahmoud and Georgescu, Ionut and Wei, Nan and Zheng, Ivy and Scandinaro, Dylan and Jiang, Heinrich and Snoek, Jasper and Sundararajan, Mukund and Wang, Xuezhi and Ontiveros, Zack and Karo, Itay and Cole, Jeremy and Rajashekhar, Vinu and Tumeh, Lara and Ben-David, Eyal and Jain, Rishub and Uesato, Jonathan and Datta, Romina and Bunyan, Oskar and Wu, Shimu and Zhang, John and Stanczyk, Piotr and Zhang, Ye and Steiner, David and Naskar, Subhajit and Azzam, Michael and Johnson, Matthew and Paszke, Adam and Chiu, Chung-Cheng and Elias, Jaume Sanchez and Mohiuddin, Afroz and Muhammad, Faizan and Miao, Jin and Lee, Andrew and Vieillard, Nino and Park, Jane and Zhang, Jiageng and Stanway, Jeff and Garmon, Drew and Karmarkar, Abhijit and Dong, Zhe and Lee, Jong and Kumar, Aviral and Zhou, Luowei and Evens, Jonathan and Isaac, William and Irving, Geoffrey and Loper, Edward and Fink, Michael and Arkatkar, Isha and Chen, Nanxin and Shafran, Izhak and Petrychenko, Ivan and Chen, Zhe and Jia, Johnson and Levskaya, Anselm and Zhu, Zhenkai and Grabowski, Peter and Mao, Yu and Magni, Alberto and Yao, Kaisheng and Snaider, Javier and Casagrande, Norman and Palmer, Evan and Suganthan, Paul and Castaño, Alfonso and Giannoumis, Irene and Kim, Wooyeol and Rybiński, Mikołaj and Sreevatsa, Ashwin and Prendki, Jennifer and Soergel, David and Goedeckemeyer, Adrian and Gierke, Willi and Jafari, Mohsen and Gaba, Meenu and Wiesner, Jeremy and Wright, Diana Gage and Wei, Yawen and Vashisht, Harsha and Kulizhskaya, Yana and Hoover, Jay and Le, Maigo and Li, Lu and Iwuanyanwu, Chimezie and Liu, Lu and Ramirez, Kevin and Khorlin, Andrey and Cui, Albert and LIN, Tian and Wu, Marcus and Aguilar, Ricardo and Pallo, Keith and Chakladar, Abhishek and Perng, Ginger and Abellan, Elena Allica and Zhang, Mingyang and Dasgupta, Ishita and Kushman, Nate and Penchev, Ivo and Repina, Alena and Wu, Xihui and family=Weide, given=Tom, prefix=van der, useprefix=true and Ponnapalli, Priya and Kaplan, Caroline and Simsa, Jiri and Li, Shuangfeng and Dousse, Olivier and Yang, Fan and Piper, Jeff and Ie, Nathan and Pasumarthi, Rama and Lintz, Nathan and Vijayakumar, Anitha and Andor, Daniel and Valenzuela, Pedro and Lui, Minnie and Paduraru, Cosmin and Peng, Daiyi and Lee, Katherine and Zhang, Shuyuan and Greene, Somer and Nguyen, Duc Dung and Kurylowicz, Paula and Hardin, Cassidy and Dixon, Lucas and Janzer, Lili and Choo, Kiam and Feng, Ziqiang and Zhang, Biao and Singhal, Achintya and Du, Dayou and McKinnon, Dan and Antropova, Natasha and Bolukbasi, Tolga and Keller, Orgad and Reid, David and Finchelstein, Daniel and Raad, Maria Abi and Crocker, Remi and Hawkins, Peter and Dadashi, Robert and Gaffney, Colin and Franko, Ken and Bulanova, Anna and Leblond, Rémi and Chung, Shirley and Askham, Harry and Cobo, Luis C. and Xu, Kelvin and Fischer, Felix and Xu, Jun and Sorokin, Christina and Alberti, Chris and Lin, Chu-Cheng and Evans, Colin and Dimitriev, Alek and Forbes, Hannah and Banarse, Dylan and Tung, Zora and Omernick, Mark and Bishop, Colton and Sterneck, Rachel and Jain, Rohan and Xia, Jiawei and Amid, Ehsan and Piccinno, Francesco and Wang, Xingyu and Banzal, Praseem and Mankowitz, Daniel J. and Polozov, Alex and Krakovna, Victoria and Brown, Sasha and Bateni, MohammadHossein and Duan, Dennis and Firoiu, Vlad and Thotakuri, Meghana and Natan, Tom and Geist, Matthieu and family=Girgin, given=Ser, prefix=tan, useprefix=false and Li, Hui and Ye, Jiayu and Roval, Ofir and Tojo, Reiko and Kwong, Michael and Lee-Thorp, James and Yew, Christopher and Sinopalnikov, Danila and Ramos, Sabela and Mellor, John and Sharma, Abhishek and Wu, Kathy and Miller, David and Sonnerat, Nicolas and Vnukov, Denis and Greig, Rory and Beattie, Jennifer and Caveness, Emily and Bai, Libin and Eisenschlos, Julian and Korchemniy, Alex and Tsai, Tomy and Jasarevic, Mimi and Kong, Weize and Dao, Phuong and Zheng, Zeyu and Liu, Frederick and Yang, Fan and Zhu, Rui and Teh, Tian Huey and Sanmiya, Jason and Gladchenko, Evgeny and Trdin, Nejc and Toyama, Daniel and Rosen, Evan and Tavakkol, Sasan and Xue, Linting and Elkind, Chen and Woodman, Oliver and Carpenter, John and Papamakarios, George and Kemp, Rupert and Kafle, Sushant and Grunina, Tanya and Sinha, Rishika and Talbert, Alice and Wu, Diane and Owusu-Afriyie, Denese and Du, Cosmo and Thornton, Chloe and Pont-Tuset, Jordi and Narayana, Pradyumna and Li, Jing and Fatehi, Saaber and Wieting, John and Ajmeri, Omar and Uria, Benigno and Ko, Yeongil and Knight, Laura and Héliou, Amélie and Niu, Ning and Gu, Shane and Pang, Chenxi and Li, Yeqing and Levine, Nir and Stolovich, Ariel and Santamaria-Fernandez, Rebeca and Goenka, Sonam and Yustalim, Wenny and Strudel, Robin and Elqursh, Ali and Deck, Charlie and Lee, Hyo and Li, Zonglin and Levin, Kyle and Hoffmann, Raphael and Holtmann-Rice, Dan and Bachem, Olivier and Arora, Sho and Koh, Christy and Yeganeh, Soheil Hassas and Põder, Siim and Tariq, Mukarram and Sun, Yanhua and Ionita, Lucian and Seyedhosseini, Mojtaba and Tafti, Pouya and Liu, Zhiyu and Gulati, Anmol and Liu, Jasmine and Ye, Xinyu and Chrzaszcz, Bart and Wang, Lily and Sethi, Nikhil and Li, Tianrun and Brown, Ben and Singh, Shreya and Fan, Wei and Parisi, Aaron and Stanton, Joe and Koverkathu, Vinod and Choquette-Choo, Christopher A. and Li, Yunjie and Lu, T. J. and Ittycheriah, Abe and Shroff, Prakash and Varadarajan, Mani and Bahargam, Sanaz and Willoughby, Rob and Gaddy, David and Desjardins, Guillaume and Cornero, Marco and Robenek, Brona and Mittal, Bhavishya and Albrecht, Ben and Shenoy, Ashish and Moiseev, Fedor and Jacobsson, Henrik and Ghaffarkhah, Alireza and Rivière, Morgane and Walton, Alanna and Crepy, Clément and Parrish, Alicia and Zhou, Zongwei and Farabet, Clement and Radebaugh, Carey and Srinivasan, Praveen and family=Salm, given=Claudia, prefix=van der, useprefix=true and Fidjeland, Andreas and Scellato, Salvatore and Latorre-Chimoto, Eri and Klimczak-Plucińska, Hanna and Bridson, David and family=Cesare, given=Dario, prefix=de, useprefix=true and Hudson, Tom and Mendolicchio, Piermaria and Walker, Lexi and Morris, Alex and Mauger, Matthew and Guseynov, Alexey and Reid, Alison and Odoom, Seth and Loher, Lucia and Cotruta, Victor and Yenugula, Madhavi and Grewe, Dominik and Petrushkina, Anastasia and Duerig, Tom and Sanchez, Antonio and Yadlowsky, Steve and Shen, Amy and Globerson, Amir and Webb, Lynette and Dua, Sahil and Li, Dong and Bhupatiraju, Surya and Hurt, Dan and Qureshi, Haroon and Agarwal, Ananth and Shani, Tomer and Eyal, Matan and Khare, Anuj and Belle, Shreyas Rammohan and Wang, Lei and Tekur, Chetan and Kale, Mihir Sanjay and Wei, Jinliang and Sang, Ruoxin and Saeta, Brennan and Liechty, Tyler and Sun, Yi and Zhao, Yao and Lee, Stephan and Nayak, Pandu and Fritz, Doug and Vuyyuru, Manish Reddy and Aslanides, John and Vyas, Nidhi and Wicke, Martin and Ma, Xiao and Eltyshev, Evgenii and Martin, Nina and Cate, Hardie and Manyika, James and Amiri, Keyvan and Kim, Yelin and Xiong, Xi and Kang, Kai and Luisier, Florian and Tripuraneni, Nilesh and Madras, David and Guo, Mandy and Waters, Austin and Wang, Oliver and Ainslie, Joshua and Baldridge, Jason and Zhang, Han and Pruthi, Garima and Bauer, Jakob and Yang, Feng and Mansour, Riham and Gelman, Jason and Xu, Yang and Polovets, George and Liu, Ji and Cai, Honglong and Chen, Warren and Sheng, XiangHai and Xue, Emily and Ozair, Sherjil and Angermueller, Christof and Li, Xiaowei and Sinha, Anoop and Wang, Weiren and Wiesinger, Julia and Koukoumidis, Emmanouil and Tian, Yuan and Iyer, Anand and Gurumurthy, Madhu and Goldenson, Mark and Shah, Parashar and Blake, M. K. and Yu, Hongkun and Urbanowicz, Anthony and Palomaki, Jennimaria and Fernando, Chrisantha and Durden, Ken and Mehta, Harsh and Momchev, Nikola and Rahimtoroghi, Elahe and Georgaki, Maria and Raul, Amit and Ruder, Sebastian and Redshaw, Morgan and Lee, Jinhyuk and Zhou, Denny and Jalan, Komal and Li, Dinghua and Hechtman, Blake and Schuh, Parker and Nasr, Milad and Milan, Kieran and Mikulik, Vladimir and Franco, Juliana and Green, Tim and Nguyen, Nam and Kelley, Joe and Mahendru, Aroma and Hu, Andrea and Howland, Joshua and Vargas, Ben and Hui, Jeffrey and Bansal, Kshitij and Rao, Vikram and Ghiya, Rakesh and Wang, Emma and Ye, Ke and Sarr, Jean Michel and Preston, Melanie Moranski and Elish, Madeleine and Li, Steve and Kaku, Aakash and Gupta, Jigar and Pasupat, Ice and Juan, Da-Cheng and Someswar, Milan and M., Tejvi and Chen, Xinyun and Amini, Aida and Fabrikant, Alex and Chu, Eric and Dong, Xuanyi and Muthal, Amruta and Buthpitiya, Senaka and Jauhari, Sarthak and Hua, Nan and Khandelwal, Urvashi and Hitron, Ayal and Ren, Jie and Rinaldi, Larissa and Drath, Shahar and Dabush, Avigail and Jiang, Nan-Jiang and Godhia, Harshal and Sachs, Uli and Chen, Anthony and Fan, Yicheng and Taitelbaum, Hagai and Noga, Hila and Dai, Zhuyun and Wang, James and Liang, Chen and Hamer, Jenny and Ferng, Chun-Sung and Elkind, Chenel and Atias, Aviel and Lee, Paulina and Listík, Vít and Carlen, Mathias and family=Kerkhof, given=Jan, prefix=van de, useprefix=true and Pikus, Marcin and Zaher, Krunoslav and Müller, Paul and Zykova, Sasha and Stefanec, Richard and Gatsko, Vitaly and Hirnschall, Christoph and Sethi, Ashwin and Xu, Xingyu Federico and Ahuja, Chetan and Tsai, Beth and Stefanoiu, Anca and Feng, Bo and Dhandhania, Keshav and Katyal, Manish and Gupta, Akshay and Parulekar, Atharva and Pitta, Divya and Zhao, Jing and Bhatia, Vivaan and Bhavnani, Yashodha and Alhadlaq, Omar and Li, Xiaolin and Danenberg, Peter and Tu, Dennis and Pine, Alex and Filippova, Vera and Ghosh, Abhipso and Limonchik, Ben and Urala, Bhargava and Lanka, Chaitanya Krishna and Clive, Derik and Sun, Yi and Li, Edward and Wu, Hao and Hongtongsak, Kevin and Li, Ianna and Thakkar, Kalind and Omarov, Kuanysh and Majmundar, Kushal and Alverson, Michael and Kucharski, Michael and Patel, Mohak and Jain, Mudit and Zabelin, Maksim and Pelagatti, Paolo and Kohli, Rohan and Kumar, Saurabh and Kim, Joseph and Sankar, Swetha and Shah, Vineet and Ramachandruni, Lakshmi and Zeng, Xiangkai and Bariach, Ben and Weidinger, Laura and Subramanya, Amar and Hsiao, Sissie and Hassabis, Demis and Kavukcuoglu, Koray and Sadovsky, Adam and Le, Quoc and Strohman, Trevor and Wu, Yonghui and Petrov, Slav and Dean, Jeffrey and Vinyals, Oriol},
  date        = {2024-04-02},
  eprint      = {2312.11805},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2312.11805},
  url         = {http://arxiv.org/abs/2312.11805},
  urldate     = {2024-04-20},
  abstract    = {This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.},
  pubstate    = {preprint},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file        = {C:\Users\jseo1005\Zotero\storage\27P3C2JD\2312.html}
}

@online{geminiteamGeminiFamilyHighly2024a,
  title       = {Gemini: {{A Family}} of {{Highly Capable Multimodal Models}}},
  shorttitle  = {Gemini},
  author      = {Gemini Team and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M. and Hauth, Anja and Millican, Katie and Silver, David and Johnson, Melvin and Antonoglou, Ioannis and Schrittwieser, Julian and Glaese, Amelia and Chen, Jilin and Pitler, Emily and Lillicrap, Timothy and Lazaridou, Angeliki and Firat, Orhan and Molloy, James and Isard, Michael and Barham, Paul R. and Hennigan, Tom and Lee, Benjamin and Viola, Fabio and Reynolds, Malcolm and Xu, Yuanzhong and Doherty, Ryan and Collins, Eli and Meyer, Clemens and Rutherford, Eliza and Moreira, Erica and Ayoub, Kareem and Goel, Megha and Krawczyk, Jack and Du, Cosmo and Chi, Ed and Cheng, Heng-Tze and Ni, Eric and Shah, Purvi and Kane, Patrick and Chan, Betty and Faruqui, Manaal and Severyn, Aliaksei and Lin, Hanzhao and Li, YaGuang and Cheng, Yong and Ittycheriah, Abe and Mahdieh, Mahdis and Chen, Mia and Sun, Pei and Tran, Dustin and Bagri, Sumit and Lakshminarayanan, Balaji and Liu, Jeremiah and Orban, Andras and Güra, Fabian and Zhou, Hao and Song, Xinying and Boffy, Aurelien and Ganapathy, Harish and Zheng, Steven and Choe, HyunJeong and Weisz, Ágoston and Zhu, Tao and Lu, Yifeng and Gopal, Siddharth and Kahn, Jarrod and Kula, Maciej and Pitman, Jeff and Shah, Rushin and Taropa, Emanuel and Merey, Majd Al and Baeuml, Martin and Chen, Zhifeng and Shafey, Laurent El and Zhang, Yujing and Sercinoglu, Olcan and Tucker, George and Piqueras, Enrique and Krikun, Maxim and Barr, Iain and Savinov, Nikolay and Danihelka, Ivo and Roelofs, Becca and White, Anaïs and Andreassen, Anders and family=Glehn, given=Tamara, prefix=von, useprefix=true and Yagati, Lakshman and Kazemi, Mehran and Gonzalez, Lucas and Khalman, Misha and Sygnowski, Jakub and Frechette, Alexandre and Smith, Charlotte and Culp, Laura and Proleev, Lev and Luan, Yi and Chen, Xi and Lottes, James and Schucher, Nathan and Lebron, Federico and Rrustemi, Alban and Clay, Natalie and Crone, Phil and Kocisky, Tomas and Zhao, Jeffrey and Perz, Bartek and Yu, Dian and Howard, Heidi and Bloniarz, Adam and Rae, Jack W. and Lu, Han and Sifre, Laurent and Maggioni, Marcello and Alcober, Fred and Garrette, Dan and Barnes, Megan and Thakoor, Shantanu and Austin, Jacob and Barth-Maron, Gabriel and Wong, William and Joshi, Rishabh and Chaabouni, Rahma and Fatiha, Deeni and Ahuja, Arun and Tomar, Gaurav Singh and Senter, Evan and Chadwick, Martin and Kornakov, Ilya and Attaluri, Nithya and Iturrate, Iñaki and Liu, Ruibo and Li, Yunxuan and Cogan, Sarah and Chen, Jeremy and Jia, Chao and Gu, Chenjie and Zhang, Qiao and Grimstad, Jordan and Hartman, Ale Jakse and Garcia, Xavier and Pillai, Thanumalayan Sankaranarayana and Devlin, Jacob and Laskin, Michael and Casas, Diego de Las and Valter, Dasha and Tao, Connie and Blanco, Lorenzo and Badia, Adrià Puigdomènech and Reitter, David and Chen, Mianna and Brennan, Jenny and Rivera, Clara and Brin, Sergey and Iqbal, Shariq and Surita, Gabriela and Labanowski, Jane and Rao, Abhi and Winkler, Stephanie and Parisotto, Emilio and Gu, Yiming and Olszewska, Kate and Addanki, Ravi and Miech, Antoine and Louis, Annie and Teplyashin, Denis and Brown, Geoff and Catt, Elliot and Balaguer, Jan and Xiang, Jackie and Wang, Pidong and Ashwood, Zoe and Briukhov, Anton and Webson, Albert and Ganapathy, Sanjay and Sanghavi, Smit and Kannan, Ajay and Chang, Ming-Wei and Stjerngren, Axel and Djolonga, Josip and Sun, Yuting and Bapna, Ankur and Aitchison, Matthew and Pejman, Pedram and Michalewski, Henryk and Yu, Tianhe and Wang, Cindy and Love, Juliette and Ahn, Junwhan and Bloxwich, Dawn and Han, Kehang and Humphreys, Peter and Sellam, Thibault and Bradbury, James and Godbole, Varun and Samangooei, Sina and Damoc, Bogdan and Kaskasoli, Alex and Arnold, Sébastien M. R. and Vasudevan, Vijay and Agrawal, Shubham and Riesa, Jason and Lepikhin, Dmitry and Tanburn, Richard and Srinivasan, Srivatsan and Lim, Hyeontaek and Hodkinson, Sarah and Shyam, Pranav and Ferret, Johan and Hand, Steven and Garg, Ankush and Paine, Tom Le and Li, Jian and Li, Yujia and Giang, Minh and Neitz, Alexander and Abbas, Zaheer and York, Sarah and Reid, Machel and Cole, Elizabeth and Chowdhery, Aakanksha and Das, Dipanjan and Rogozińska, Dominika and Nikolaev, Vitaliy and Sprechmann, Pablo and Nado, Zachary and Zilka, Lukas and Prost, Flavien and He, Luheng and Monteiro, Marianne and Mishra, Gaurav and Welty, Chris and Newlan, Josh and Jia, Dawei and Allamanis, Miltiadis and Hu, Clara Huiyi and family=Liedekerke, given=Raoul, prefix=de, useprefix=true and Gilmer, Justin and Saroufim, Carl and Rijhwani, Shruti and Hou, Shaobo and Shrivastava, Disha and Baddepudi, Anirudh and Goldin, Alex and Ozturel, Adnan and Cassirer, Albin and Xu, Yunhan and Sohn, Daniel and Sachan, Devendra and Amplayo, Reinald Kim and Swanson, Craig and Petrova, Dessie and Narayan, Shashi and Guez, Arthur and Brahma, Siddhartha and Landon, Jessica and Patel, Miteyan and Zhao, Ruizhe and Villela, Kevin and Wang, Luyu and Jia, Wenhao and Rahtz, Matthew and Giménez, Mai and Yeung, Legg and Keeling, James and Georgiev, Petko and Mincu, Diana and Wu, Boxi and Haykal, Salem and Saputro, Rachel and Vodrahalli, Kiran and Qin, James and Cankara, Zeynep and Sharma, Abhanshu and Fernando, Nick and Hawkins, Will and Neyshabur, Behnam and Kim, Solomon and Hutter, Adrian and Agrawal, Priyanka and Castro-Ros, Alex and family=Driessche, given=George, prefix=van den, useprefix=false and Wang, Tao and Yang, Fan and Chang, Shuo-yiin and Komarek, Paul and McIlroy, Ross and Lučić, Mario and Zhang, Guodong and Farhan, Wael and Sharman, Michael and Natsev, Paul and Michel, Paul and Bansal, Yamini and Qiao, Siyuan and Cao, Kris and Shakeri, Siamak and Butterfield, Christina and Chung, Justin and Rubenstein, Paul Kishan and Agrawal, Shivani and Mensch, Arthur and Soparkar, Kedar and Lenc, Karel and Chung, Timothy and Pope, Aedan and Maggiore, Loren and Kay, Jackie and Jhakra, Priya and Wang, Shibo and Maynez, Joshua and Phuong, Mary and Tobin, Taylor and Tacchetti, Andrea and Trebacz, Maja and Robinson, Kevin and Katariya, Yash and Riedel, Sebastian and Bailey, Paige and Xiao, Kefan and Ghelani, Nimesh and Aroyo, Lora and Slone, Ambrose and Houlsby, Neil and Xiong, Xuehan and Yang, Zhen and Gribovskaya, Elena and Adler, Jonas and Wirth, Mateo and Lee, Lisa and Li, Music and Kagohara, Thais and Pavagadhi, Jay and Bridgers, Sophie and Bortsova, Anna and Ghemawat, Sanjay and Ahmed, Zafarali and Liu, Tianqi and Powell, Richard and Bolina, Vijay and Iinuma, Mariko and Zablotskaia, Polina and Besley, James and Chung, Da-Woon and Dozat, Timothy and Comanescu, Ramona and Si, Xiance and Greer, Jeremy and Su, Guolong and Polacek, Martin and Kaufman, Raphaël Lopez and Tokumine, Simon and Hu, Hexiang and Buchatskaya, Elena and Miao, Yingjie and Elhawaty, Mohamed and Siddhant, Aditya and Tomasev, Nenad and Xing, Jinwei and Greer, Christina and Miller, Helen and Ashraf, Shereen and Roy, Aurko and Zhang, Zizhao and Ma, Ada and Filos, Angelos and Besta, Milos and Blevins, Rory and Klimenko, Ted and Yeh, Chih-Kuan and Changpinyo, Soravit and Mu, Jiaqi and Chang, Oscar and Pajarskas, Mantas and Muir, Carrie and Cohen, Vered and Lan, Charline Le and Haridasan, Krishna and Marathe, Amit and Hansen, Steven and Douglas, Sholto and Samuel, Rajkumar and Wang, Mingqiu and Austin, Sophia and Lan, Chang and Jiang, Jiepu and Chiu, Justin and Lorenzo, Jaime Alonso and Sjösund, Lars Lowe and Cevey, Sébastien and Gleicher, Zach and Avrahami, Thi and Boral, Anudhyan and Srinivasan, Hansa and Selo, Vittorio and May, Rhys and Aisopos, Konstantinos and Hussenot, Léonard and Soares, Livio Baldini and Baumli, Kate and Chang, Michael B. and Recasens, Adrià and Caine, Ben and Pritzel, Alexander and Pavetic, Filip and Pardo, Fabio and Gergely, Anita and Frye, Justin and Ramasesh, Vinay and Horgan, Dan and Badola, Kartikeya and Kassner, Nora and Roy, Subhrajit and Dyer, Ethan and Campos, Víctor Campos and Tomala, Alex and Tang, Yunhao and Badawy, Dalia El and White, Elspeth and Mustafa, Basil and Lang, Oran and Jindal, Abhishek and Vikram, Sharad and Gong, Zhitao and Caelles, Sergi and Hemsley, Ross and Thornton, Gregory and Feng, Fangxiaoyu and Stokowiec, Wojciech and Zheng, Ce and Thacker, Phoebe and Ünlü, Çağlar and Zhang, Zhishuai and Saleh, Mohammad and Svensson, James and Bileschi, Max and Patil, Piyush and Anand, Ankesh and Ring, Roman and Tsihlas, Katerina and Vezer, Arpi and Selvi, Marco and Shevlane, Toby and Rodriguez, Mikel and Kwiatkowski, Tom and Daruki, Samira and Rong, Keran and Dafoe, Allan and FitzGerald, Nicholas and Gu-Lemberg, Keren and Khan, Mina and Hendricks, Lisa Anne and Pellat, Marie and Feinberg, Vladimir and Cobon-Kerr, James and Sainath, Tara and Rauh, Maribeth and Hashemi, Sayed Hadi and Ives, Richard and Hasson, Yana and Noland, Eric and Cao, Yuan and Byrd, Nathan and Hou, Le and Wang, Qingze and Sottiaux, Thibault and Paganini, Michela and Lespiau, Jean-Baptiste and Moufarek, Alexandre and Hassan, Samer and Shivakumar, Kaushik and family=Amersfoort, given=Joost, prefix=van, useprefix=true and Mandhane, Amol and Joshi, Pratik and Goyal, Anirudh and Tung, Matthew and Brock, Andrew and Sheahan, Hannah and Misra, Vedant and Li, Cheng and Rakićević, Nemanja and Dehghani, Mostafa and Liu, Fangyu and Mittal, Sid and Oh, Junhyuk and Noury, Seb and Sezener, Eren and Huot, Fantine and Lamm, Matthew and De Cao, Nicola and Chen, Charlie and Mudgal, Sidharth and Stella, Romina and Brooks, Kevin and Vasudevan, Gautam and Liu, Chenxi and Chain, Mainak and Melinkeri, Nivedita and Cohen, Aaron and Wang, Venus and Seymore, Kristie and Zubkov, Sergey and Goel, Rahul and Yue, Summer and Krishnakumaran, Sai and Albert, Brian and Hurley, Nate and Sano, Motoki and Mohananey, Anhad and Joughin, Jonah and Filonov, Egor and Kępa, Tomasz and Eldawy, Yomna and Lim, Jiawern and Rishi, Rahul and Badiezadegan, Shirin and Bos, Taylor and Chang, Jerry and Jain, Sanil and Padmanabhan, Sri Gayatri Sundara and Puttagunta, Subha and Krishna, Kalpesh and Baker, Leslie and Kalb, Norbert and Bedapudi, Vamsi and Kurzrok, Adam and Lei, Shuntong and Yu, Anthony and Litvin, Oren and Zhou, Xiang and Wu, Zhichun and Sobell, Sam and Siciliano, Andrea and Papir, Alan and Neale, Robby and Bragagnolo, Jonas and Toor, Tej and Chen, Tina and Anklin, Valentin and Wang, Feiran and Feng, Richie and Gholami, Milad and Ling, Kevin and Liu, Lijuan and Walter, Jules and Moghaddam, Hamid and Kishore, Arun and Adamek, Jakub and Mercado, Tyler and Mallinson, Jonathan and Wandekar, Siddhinita and Cagle, Stephen and Ofek, Eran and Garrido, Guillermo and Lombriser, Clemens and Mukha, Maksim and Sun, Botu and Mohammad, Hafeezul Rahman and Matak, Josip and Qian, Yadi and Peswani, Vikas and Janus, Pawel and Yuan, Quan and Schelin, Leif and David, Oana and Garg, Ankur and He, Yifan and Duzhyi, Oleksii and Älgmyr, Anton and Lottaz, Timothée and Li, Qi and Yadav, Vikas and Xu, Luyao and Chinien, Alex and Shivanna, Rakesh and Chuklin, Aleksandr and Li, Josie and Spadine, Carrie and Wolfe, Travis and Mohamed, Kareem and Das, Subhabrata and Dai, Zihang and He, Kyle and family=Dincklage, given=Daniel, prefix=von, useprefix=true and Upadhyay, Shyam and Maurya, Akanksha and Chi, Luyan and Krause, Sebastian and Salama, Khalid and Rabinovitch, Pam G. and M, Pavan Kumar Reddy and Selvan, Aarush and Dektiarev, Mikhail and Ghiasi, Golnaz and Guven, Erdem and Gupta, Himanshu and Liu, Boyi and Sharma, Deepak and Shtacher, Idan Heimlich and Paul, Shachi and Akerlund, Oscar and Aubet, François-Xavier and Huang, Terry and Zhu, Chen and Zhu, Eric and Teixeira, Elico and Fritze, Matthew and Bertolini, Francesco and Marinescu, Liana-Eleonora and Bölle, Martin and Paulus, Dominik and Gupta, Khyatti and Latkar, Tejasi and Chang, Max and Sanders, Jason and Wilson, Roopa and Wu, Xuewei and Tan, Yi-Xuan and Thiet, Lam Nguyen and Doshi, Tulsee and Lall, Sid and Mishra, Swaroop and Chen, Wanming and Luong, Thang and Benjamin, Seth and Lee, Jasmine and Andrejczuk, Ewa and Rabiej, Dominik and Ranjan, Vipul and Styrc, Krzysztof and Yin, Pengcheng and Simon, Jon and Harriott, Malcolm Rose and Bansal, Mudit and Robsky, Alexei and Bacon, Geoff and Greene, David and Mirylenka, Daniil and Zhou, Chen and Sarvana, Obaid and Goyal, Abhimanyu and Andermatt, Samuel and Siegler, Patrick and Horn, Ben and Israel, Assaf and Pongetti, Francesco and Chen, Chih-Wei "Louis" and Selvatici, Marco and Silva, Pedro and Wang, Kathie and Tolins, Jackson and Guu, Kelvin and Yogev, Roey and Cai, Xiaochen and Agostini, Alessandro and Shah, Maulik and Nguyen, Hung and Donnaile, Noah Ó and Pereira, Sébastien and Friso, Linda and Stambler, Adam and Kurzrok, Adam and Kuang, Chenkai and Romanikhin, Yan and Geller, Mark and Yan, Z. J. and Jang, Kane and Lee, Cheng-Chun and Fica, Wojciech and Malmi, Eric and Tan, Qijun and Banica, Dan and Balle, Daniel and Pham, Ryan and Huang, Yanping and Avram, Diana and Shi, Hongzhi and Singh, Jasjot and Hidey, Chris and Ahuja, Niharika and Saxena, Pranab and Dooley, Dan and Potharaju, Srividya Pranavi and O'Neill, Eileen and Gokulchandran, Anand and Foley, Ryan and Zhao, Kai and Dusenberry, Mike and Liu, Yuan and Mehta, Pulkit and Kotikalapudi, Ragha and Safranek-Shrader, Chalence and Goodman, Andrew and Kessinger, Joshua and Globen, Eran and Kolhar, Prateek and Gorgolewski, Chris and Ibrahim, Ali and Song, Yang and Eichenbaum, Ali and Brovelli, Thomas and Potluri, Sahitya and Lahoti, Preethi and Baetu, Cip and Ghorbani, Ali and Chen, Charles and Crawford, Andy and Pal, Shalini and Sridhar, Mukund and Gurita, Petru and Mujika, Asier and Petrovski, Igor and Cedoz, Pierre-Louis and Li, Chenmei and Chen, Shiyuan and Santo, Niccolò Dal and Goyal, Siddharth and Punjabi, Jitesh and Kappaganthu, Karthik and Kwak, Chester and LV, Pallavi and Velury, Sarmishta and Choudhury, Himadri and Hall, Jamie and Shah, Premal and Figueira, Ricardo and Thomas, Matt and Lu, Minjie and Zhou, Ting and Kumar, Chintu and Jurdi, Thomas and Chikkerur, Sharat and Ma, Yenai and Yu, Adams and Kwak, Soo and Ähdel, Victor and Rajayogam, Sujeevan and Choma, Travis and Liu, Fei and Barua, Aditya and Ji, Colin and Park, Ji Ho and Hellendoorn, Vincent and Bailey, Alex and Bilal, Taylan and Zhou, Huanjie and Khatir, Mehrdad and Sutton, Charles and Rzadkowski, Wojciech and Macintosh, Fiona and Shagin, Konstantin and Medina, Paul and Liang, Chen and Zhou, Jinjing and Shah, Pararth and Bi, Yingying and Dankovics, Attila and Banga, Shipra and Lehmann, Sabine and Bredesen, Marissa and Lin, Zifan and Hoffmann, John Eric and Lai, Jonathan and Chung, Raynald and Yang, Kai and Balani, Nihal and Bražinskas, Arthur and Sozanschi, Andrei and Hayes, Matthew and Alcalde, Héctor Fernández and Makarov, Peter and Chen, Will and Stella, Antonio and Snijders, Liselotte and Mandl, Michael and Kärrman, Ante and Nowak, Paweł and Wu, Xinyi and Dyck, Alex and Vaidyanathan, Krishnan and R, Raghavender and Mallet, Jessica and Rudominer, Mitch and Johnston, Eric and Mittal, Sushil and Udathu, Akhil and Christensen, Janara and Verma, Vishal and Irving, Zach and Santucci, Andreas and Elsayed, Gamaleldin and Davoodi, Elnaz and Georgiev, Marin and Tenney, Ian and Hua, Nan and Cideron, Geoffrey and Leurent, Edouard and Alnahlawi, Mahmoud and Georgescu, Ionut and Wei, Nan and Zheng, Ivy and Scandinaro, Dylan and Jiang, Heinrich and Snoek, Jasper and Sundararajan, Mukund and Wang, Xuezhi and Ontiveros, Zack and Karo, Itay and Cole, Jeremy and Rajashekhar, Vinu and Tumeh, Lara and Ben-David, Eyal and Jain, Rishub and Uesato, Jonathan and Datta, Romina and Bunyan, Oskar and Wu, Shimu and Zhang, John and Stanczyk, Piotr and Zhang, Ye and Steiner, David and Naskar, Subhajit and Azzam, Michael and Johnson, Matthew and Paszke, Adam and Chiu, Chung-Cheng and Elias, Jaume Sanchez and Mohiuddin, Afroz and Muhammad, Faizan and Miao, Jin and Lee, Andrew and Vieillard, Nino and Park, Jane and Zhang, Jiageng and Stanway, Jeff and Garmon, Drew and Karmarkar, Abhijit and Dong, Zhe and Lee, Jong and Kumar, Aviral and Zhou, Luowei and Evens, Jonathan and Isaac, William and Irving, Geoffrey and Loper, Edward and Fink, Michael and Arkatkar, Isha and Chen, Nanxin and Shafran, Izhak and Petrychenko, Ivan and Chen, Zhe and Jia, Johnson and Levskaya, Anselm and Zhu, Zhenkai and Grabowski, Peter and Mao, Yu and Magni, Alberto and Yao, Kaisheng and Snaider, Javier and Casagrande, Norman and Palmer, Evan and Suganthan, Paul and Castaño, Alfonso and Giannoumis, Irene and Kim, Wooyeol and Rybiński, Mikołaj and Sreevatsa, Ashwin and Prendki, Jennifer and Soergel, David and Goedeckemeyer, Adrian and Gierke, Willi and Jafari, Mohsen and Gaba, Meenu and Wiesner, Jeremy and Wright, Diana Gage and Wei, Yawen and Vashisht, Harsha and Kulizhskaya, Yana and Hoover, Jay and Le, Maigo and Li, Lu and Iwuanyanwu, Chimezie and Liu, Lu and Ramirez, Kevin and Khorlin, Andrey and Cui, Albert and LIN, Tian and Wu, Marcus and Aguilar, Ricardo and Pallo, Keith and Chakladar, Abhishek and Perng, Ginger and Abellan, Elena Allica and Zhang, Mingyang and Dasgupta, Ishita and Kushman, Nate and Penchev, Ivo and Repina, Alena and Wu, Xihui and family=Weide, given=Tom, prefix=van der, useprefix=true and Ponnapalli, Priya and Kaplan, Caroline and Simsa, Jiri and Li, Shuangfeng and Dousse, Olivier and Yang, Fan and Piper, Jeff and Ie, Nathan and Pasumarthi, Rama and Lintz, Nathan and Vijayakumar, Anitha and Andor, Daniel and Valenzuela, Pedro and Lui, Minnie and Paduraru, Cosmin and Peng, Daiyi and Lee, Katherine and Zhang, Shuyuan and Greene, Somer and Nguyen, Duc Dung and Kurylowicz, Paula and Hardin, Cassidy and Dixon, Lucas and Janzer, Lili and Choo, Kiam and Feng, Ziqiang and Zhang, Biao and Singhal, Achintya and Du, Dayou and McKinnon, Dan and Antropova, Natasha and Bolukbasi, Tolga and Keller, Orgad and Reid, David and Finchelstein, Daniel and Raad, Maria Abi and Crocker, Remi and Hawkins, Peter and Dadashi, Robert and Gaffney, Colin and Franko, Ken and Bulanova, Anna and Leblond, Rémi and Chung, Shirley and Askham, Harry and Cobo, Luis C. and Xu, Kelvin and Fischer, Felix and Xu, Jun and Sorokin, Christina and Alberti, Chris and Lin, Chu-Cheng and Evans, Colin and Dimitriev, Alek and Forbes, Hannah and Banarse, Dylan and Tung, Zora and Omernick, Mark and Bishop, Colton and Sterneck, Rachel and Jain, Rohan and Xia, Jiawei and Amid, Ehsan and Piccinno, Francesco and Wang, Xingyu and Banzal, Praseem and Mankowitz, Daniel J. and Polozov, Alex and Krakovna, Victoria and Brown, Sasha and Bateni, MohammadHossein and Duan, Dennis and Firoiu, Vlad and Thotakuri, Meghana and Natan, Tom and Geist, Matthieu and family=Girgin, given=Ser, prefix=tan, useprefix=false and Li, Hui and Ye, Jiayu and Roval, Ofir and Tojo, Reiko and Kwong, Michael and Lee-Thorp, James and Yew, Christopher and Sinopalnikov, Danila and Ramos, Sabela and Mellor, John and Sharma, Abhishek and Wu, Kathy and Miller, David and Sonnerat, Nicolas and Vnukov, Denis and Greig, Rory and Beattie, Jennifer and Caveness, Emily and Bai, Libin and Eisenschlos, Julian and Korchemniy, Alex and Tsai, Tomy and Jasarevic, Mimi and Kong, Weize and Dao, Phuong and Zheng, Zeyu and Liu, Frederick and Yang, Fan and Zhu, Rui and Teh, Tian Huey and Sanmiya, Jason and Gladchenko, Evgeny and Trdin, Nejc and Toyama, Daniel and Rosen, Evan and Tavakkol, Sasan and Xue, Linting and Elkind, Chen and Woodman, Oliver and Carpenter, John and Papamakarios, George and Kemp, Rupert and Kafle, Sushant and Grunina, Tanya and Sinha, Rishika and Talbert, Alice and Wu, Diane and Owusu-Afriyie, Denese and Du, Cosmo and Thornton, Chloe and Pont-Tuset, Jordi and Narayana, Pradyumna and Li, Jing and Fatehi, Saaber and Wieting, John and Ajmeri, Omar and Uria, Benigno and Ko, Yeongil and Knight, Laura and Héliou, Amélie and Niu, Ning and Gu, Shane and Pang, Chenxi and Li, Yeqing and Levine, Nir and Stolovich, Ariel and Santamaria-Fernandez, Rebeca and Goenka, Sonam and Yustalim, Wenny and Strudel, Robin and Elqursh, Ali and Deck, Charlie and Lee, Hyo and Li, Zonglin and Levin, Kyle and Hoffmann, Raphael and Holtmann-Rice, Dan and Bachem, Olivier and Arora, Sho and Koh, Christy and Yeganeh, Soheil Hassas and Põder, Siim and Tariq, Mukarram and Sun, Yanhua and Ionita, Lucian and Seyedhosseini, Mojtaba and Tafti, Pouya and Liu, Zhiyu and Gulati, Anmol and Liu, Jasmine and Ye, Xinyu and Chrzaszcz, Bart and Wang, Lily and Sethi, Nikhil and Li, Tianrun and Brown, Ben and Singh, Shreya and Fan, Wei and Parisi, Aaron and Stanton, Joe and Koverkathu, Vinod and Choquette-Choo, Christopher A. and Li, Yunjie and Lu, T. J. and Ittycheriah, Abe and Shroff, Prakash and Varadarajan, Mani and Bahargam, Sanaz and Willoughby, Rob and Gaddy, David and Desjardins, Guillaume and Cornero, Marco and Robenek, Brona and Mittal, Bhavishya and Albrecht, Ben and Shenoy, Ashish and Moiseev, Fedor and Jacobsson, Henrik and Ghaffarkhah, Alireza and Rivière, Morgane and Walton, Alanna and Crepy, Clément and Parrish, Alicia and Zhou, Zongwei and Farabet, Clement and Radebaugh, Carey and Srinivasan, Praveen and family=Salm, given=Claudia, prefix=van der, useprefix=true and Fidjeland, Andreas and Scellato, Salvatore and Latorre-Chimoto, Eri and Klimczak-Plucińska, Hanna and Bridson, David and family=Cesare, given=Dario, prefix=de, useprefix=true and Hudson, Tom and Mendolicchio, Piermaria and Walker, Lexi and Morris, Alex and Mauger, Matthew and Guseynov, Alexey and Reid, Alison and Odoom, Seth and Loher, Lucia and Cotruta, Victor and Yenugula, Madhavi and Grewe, Dominik and Petrushkina, Anastasia and Duerig, Tom and Sanchez, Antonio and Yadlowsky, Steve and Shen, Amy and Globerson, Amir and Webb, Lynette and Dua, Sahil and Li, Dong and Bhupatiraju, Surya and Hurt, Dan and Qureshi, Haroon and Agarwal, Ananth and Shani, Tomer and Eyal, Matan and Khare, Anuj and Belle, Shreyas Rammohan and Wang, Lei and Tekur, Chetan and Kale, Mihir Sanjay and Wei, Jinliang and Sang, Ruoxin and Saeta, Brennan and Liechty, Tyler and Sun, Yi and Zhao, Yao and Lee, Stephan and Nayak, Pandu and Fritz, Doug and Vuyyuru, Manish Reddy and Aslanides, John and Vyas, Nidhi and Wicke, Martin and Ma, Xiao and Eltyshev, Evgenii and Martin, Nina and Cate, Hardie and Manyika, James and Amiri, Keyvan and Kim, Yelin and Xiong, Xi and Kang, Kai and Luisier, Florian and Tripuraneni, Nilesh and Madras, David and Guo, Mandy and Waters, Austin and Wang, Oliver and Ainslie, Joshua and Baldridge, Jason and Zhang, Han and Pruthi, Garima and Bauer, Jakob and Yang, Feng and Mansour, Riham and Gelman, Jason and Xu, Yang and Polovets, George and Liu, Ji and Cai, Honglong and Chen, Warren and Sheng, XiangHai and Xue, Emily and Ozair, Sherjil and Angermueller, Christof and Li, Xiaowei and Sinha, Anoop and Wang, Weiren and Wiesinger, Julia and Koukoumidis, Emmanouil and Tian, Yuan and Iyer, Anand and Gurumurthy, Madhu and Goldenson, Mark and Shah, Parashar and Blake, M. K. and Yu, Hongkun and Urbanowicz, Anthony and Palomaki, Jennimaria and Fernando, Chrisantha and Durden, Ken and Mehta, Harsh and Momchev, Nikola and Rahimtoroghi, Elahe and Georgaki, Maria and Raul, Amit and Ruder, Sebastian and Redshaw, Morgan and Lee, Jinhyuk and Zhou, Denny and Jalan, Komal and Li, Dinghua and Hechtman, Blake and Schuh, Parker and Nasr, Milad and Milan, Kieran and Mikulik, Vladimir and Franco, Juliana and Green, Tim and Nguyen, Nam and Kelley, Joe and Mahendru, Aroma and Hu, Andrea and Howland, Joshua and Vargas, Ben and Hui, Jeffrey and Bansal, Kshitij and Rao, Vikram and Ghiya, Rakesh and Wang, Emma and Ye, Ke and Sarr, Jean Michel and Preston, Melanie Moranski and Elish, Madeleine and Li, Steve and Kaku, Aakash and Gupta, Jigar and Pasupat, Ice and Juan, Da-Cheng and Someswar, Milan and M., Tejvi and Chen, Xinyun and Amini, Aida and Fabrikant, Alex and Chu, Eric and Dong, Xuanyi and Muthal, Amruta and Buthpitiya, Senaka and Jauhari, Sarthak and Hua, Nan and Khandelwal, Urvashi and Hitron, Ayal and Ren, Jie and Rinaldi, Larissa and Drath, Shahar and Dabush, Avigail and Jiang, Nan-Jiang and Godhia, Harshal and Sachs, Uli and Chen, Anthony and Fan, Yicheng and Taitelbaum, Hagai and Noga, Hila and Dai, Zhuyun and Wang, James and Liang, Chen and Hamer, Jenny and Ferng, Chun-Sung and Elkind, Chenel and Atias, Aviel and Lee, Paulina and Listík, Vít and Carlen, Mathias and family=Kerkhof, given=Jan, prefix=van de, useprefix=true and Pikus, Marcin and Zaher, Krunoslav and Müller, Paul and Zykova, Sasha and Stefanec, Richard and Gatsko, Vitaly and Hirnschall, Christoph and Sethi, Ashwin and Xu, Xingyu Federico and Ahuja, Chetan and Tsai, Beth and Stefanoiu, Anca and Feng, Bo and Dhandhania, Keshav and Katyal, Manish and Gupta, Akshay and Parulekar, Atharva and Pitta, Divya and Zhao, Jing and Bhatia, Vivaan and Bhavnani, Yashodha and Alhadlaq, Omar and Li, Xiaolin and Danenberg, Peter and Tu, Dennis and Pine, Alex and Filippova, Vera and Ghosh, Abhipso and Limonchik, Ben and Urala, Bhargava and Lanka, Chaitanya Krishna and Clive, Derik and Sun, Yi and Li, Edward and Wu, Hao and Hongtongsak, Kevin and Li, Ianna and Thakkar, Kalind and Omarov, Kuanysh and Majmundar, Kushal and Alverson, Michael and Kucharski, Michael and Patel, Mohak and Jain, Mudit and Zabelin, Maksim and Pelagatti, Paolo and Kohli, Rohan and Kumar, Saurabh and Kim, Joseph and Sankar, Swetha and Shah, Vineet and Ramachandruni, Lakshmi and Zeng, Xiangkai and Bariach, Ben and Weidinger, Laura and Subramanya, Amar and Hsiao, Sissie and Hassabis, Demis and Kavukcuoglu, Koray and Sadovsky, Adam and Le, Quoc and Strohman, Trevor and Wu, Yonghui and Petrov, Slav and Dean, Jeffrey and Vinyals, Oriol},
  date        = {2024-04-02},
  eprint      = {2312.11805},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2312.11805},
  url         = {http://arxiv.org/abs/2312.11805},
  urldate     = {2024-04-24},
  abstract    = {This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.},
  pubstate    = {preprint},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file        = {C\:\\Users\\jseo1005\\Zotero\\storage\\CZVIG2F7\\Gemini Team et al. - 2024 - Gemini A Family of Highly Capable Multimodal Mode.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\STSN52WJ\\2312.html}
}

@inproceedings{giabbanelliGPTBasedModelsMeet2023,
  title      = {{{GPT-Based Models Meet Simulation}}: {{How}} to {{Efficiently}} Use {{Large-Scale Pre-Trained Language Models Across Simulation Tasks}}},
  shorttitle = {{{GPT-Based Models Meet Simulation}}},
  booktitle  = {2023 {{Winter Simulation Conference}} ({{WSC}})},
  author     = {Giabbanelli, Philippe J.},
  date       = {2023-12-10},
  pages      = {2920--2931},
  publisher  = {IEEE},
  location   = {San Antonio, TX, USA},
  doi        = {10.1109/WSC60868.2023.10408017},
  url        = {https://ieeexplore.ieee.org/document/10408017/},
  urldate    = {2024-03-25},
  abstract   = {The disruptive technology provided by large-scale pre-trained language models (LLMs) such as ChatGPT or GPT-4 has received significant attention in several application domains, often with an emphasis on high-level opportunities and concerns. This paper is the first examination regarding the use of LLMs for scientific simulations. We focus on four modeling and simulation tasks, each time assessing the expected benefits and limitations of LLMs while providing practical guidance for modelers regarding the steps involved. The first task is devoted to explaining the structure of a conceptual model to promote the engagement of participants in the modeling process. The second task focuses on summarizing simulation outputs, so that model users can identify a preferred scenario. The third task seeks to broaden accessibility to simulation platforms by conveying the insights of simulation visualizations via text. Finally, the last task evokes the possibility of explaining simulation errors and providing guidance to resolve them.},
  eventtitle = {2023 {{Winter Simulation Conference}} ({{WSC}})},
  isbn       = {9798350369663},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\KW9DP2XQ\Giabbanelli - 2023 - GPT-Based Models Meet Simulation How to Efficient.pdf}
}

@article{gibsonTeachingStatisticsStudent1999,
  title        = {Teaching {{Statistics}} to a {{Student}} Who Is {{Blind}}},
  author       = {Gibson, W.e. and Darron, C.},
  date         = {1999-01-01},
  journaltitle = {Teaching of Psychology},
  volume       = {26},
  number       = {2},
  pages        = {130--131},
  issn         = {00986283},
  doi          = {10.1207/s15328023top2602_13},
  url          = {https://proxy2.library.illinois.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=edselc&AN=edselc.2-52.0-0347091372&site=eds-live&scope=site},
  urldate      = {2024-02-06},
  langid       = {english},
  pagetotal    = {131},
  annotation   = {130},
  file         = {C:\Users\jseo1005\Zotero\storage\Y2KRHJXX\Gibson and Darron - 1999 - Teaching Statistics to a Student who is Blind.pdf}
}

@inproceedings{giudiceLearningNonvisualGraphical2012,
  title      = {Learning Non-Visual Graphical Information Using a Touch-Based Vibro-Audio Interface},
  booktitle  = {Proceedings of the 14th International {{ACM SIGACCESS}} Conference on {{Computers}} and Accessibility - {{ASSETS}} '12},
  author     = {Giudice, Nicholas A. and Palani, Hari Prasath and Brenner, Eric and Kramer, Kevin M.},
  date       = {2012},
  pages      = {103},
  publisher  = {ACM Press},
  location   = {Boulder, Colorado, USA},
  doi        = {10.1145/2384916.2384935},
  url        = {http://dl.acm.org/citation.cfm?doid=2384916.2384935},
  urldate    = {2022-08-21},
  eventtitle = {The 14th International {{ACM SIGACCESS}} Conference},
  isbn       = {978-1-4503-1321-6},
  langid     = {english}
}

@inproceedings{gleasonMakingMemesAccessible2019,
  title      = {Making {{Memes Accessible}}},
  booktitle  = {The 21st {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author     = {Gleason, Cole and Pavel, Amy and Liu, Xingyu and Carrington, Patrick and Chilton, Lydia B. and Bigham, Jeffrey P.},
  date       = {2019-10-24},
  pages      = {367--376},
  publisher  = {ACM},
  location   = {Pittsburgh PA USA},
  doi        = {10.1145/3308561.3353792},
  url        = {https://dl.acm.org/doi/10.1145/3308561.3353792},
  urldate    = {2024-01-19},
  eventtitle = {{{ASSETS}} '19: {{The}} 21st {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  isbn       = {978-1-4503-6676-2},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\I2V6JKXP\Gleason et al. - 2019 - Making Memes Accessible.pdf}
}

@inproceedings{godfreyAccessibleInteractionModel2018,
  title     = {An {{Accessible Interaction Model}} for {{Data Visualisation}} in {{Statistics}}},
  booktitle = {Computers {{Helping People}} with {{Special Needs}}},
  author    = {Godfrey, A. Jonathan R. and Murrell, Paul and Sorge, Volker},
  editor    = {Miesenberger, Klaus and Kouroupetroglou, Georgios},
  date      = {2018},
  series    = {Lecture {{Notes}} in {{Computer Science}}},
  pages     = {590--597},
  publisher = {Springer International Publishing},
  location  = {Cham},
  doi       = {10.1007/978-3-319-94277-3_92},
  abstract  = {Data is everywhere and its communication and understanding is an important pre-requisite for the full participation of individuals in the information age. Good data visualisation is commonly used to great effect for the sighted world, but are practically useless to a blind audience. Blind people are at risk of being left behind if efforts are not made to improve the access to information that is not traditionally conveyed in text, whether that text be accessed in braille, audio, or a computer’s screen reading software. Our work aims to provide an accessible way for blind users to easily, efficiently, and most importantly accurately, explore and query the data contained in diagrams such as bar charts, box plots, time series, and many more. We employ the statistical software environment R not only as a means to generate accessible diagrams, but also as a way for blind users to directly interact with data in the same way as their sighted peers by supporting immediate data visualisation via screen reading and interactive exploration.},
  isbn      = {978-3-319-94277-3},
  langid    = {english},
  keywords  = {Blind People,Blind Users,Data Visualisation,Screen Reader,Statistical Software Application},
  file      = {C:\Users\jseo1005\Zotero\storage\H8QE92CU\Godfrey et al. - 2018 - An Accessible Interaction Model for Data Visualisa.pdf}
}

@inproceedings{godfreyAccessibleInteractionModel2018b,
  title     = {An {{Accessible Interaction Model}} for {{Data Visualisation}} in {{Statistics}}},
  booktitle = {Computers {{Helping People}} with {{Special Needs}}},
  author    = {Godfrey, A. Jonathan R. and Murrell, Paul and Sorge, Volker},
  editor    = {Miesenberger, Klaus and Kouroupetroglou, Georgios},
  date      = {2018},
  series    = {Lecture {{Notes}} in {{Computer Science}}},
  pages     = {590--597},
  publisher = {Springer International Publishing},
  location  = {Cham},
  doi       = {10.1007/978-3-319-94277-3_92},
  abstract  = {Data is everywhere and its communication and understanding is an important pre-requisite for the full participation of individuals in the information age. Good data visualisation is commonly used to great effect for the sighted world, but are practically useless to a blind audience. Blind people are at risk of being left behind if efforts are not made to improve the access to information that is not traditionally conveyed in text, whether that text be accessed in braille, audio, or a computer’s screen reading software. Our work aims to provide an accessible way for blind users to easily, efficiently, and most importantly accurately, explore and query the data contained in diagrams such as bar charts, box plots, time series, and many more. We employ the statistical software environment R not only as a means to generate accessible diagrams, but also as a way for blind users to directly interact with data in the same way as their sighted peers by supporting immediate data visualisation via screen reading and interactive exploration.},
  isbn      = {978-3-319-94277-3},
  langid    = {english},
  file      = {C:\Users\jseo1005\Zotero\storage\IGI8EIC7\Godfrey et al. - 2018 - An Accessible Interaction Model for Data Visualisa.pdf}
}

@article{godfreyAdviceBlindTeachers2015,
  title        = {Advice {{From Blind Teachers}} on {{How}} to {{Teach Statistics}} to {{Blind Students}}},
  author       = {Godfrey, A. Jonathan R. and Loots, M. Theodor},
  date         = {2015-11-01},
  journaltitle = {Journal of Statistics Education},
  volume       = {23},
  number       = {3},
  pages        = {null},
  publisher    = {Taylor \& Francis},
  issn         = {null},
  doi          = {10.1080/10691898.2015.11889746},
  url          = {https://doi.org/10.1080/10691898.2015.11889746},
  urldate      = {2023-01-05},
  abstract     = {Blind students are bound to make up a very small part of the population most university lecturers will encounter during their careers. Research to date shows that good communication between staff and student improves the chances of a successful outcome for both parties. The research does show, however, that the exercise seems to be one of re-inventing the wheel, perhaps with a less than fully informed blueprint to work from.The authors use their own experiences as blind students who progressed beyond research methods or first year introductory courses into careers as teachers and researchers of statistical methods to provide guidance for their sighted colleagues. Our principle point of difference to the existing research work is that we rely on the experience of our statistical education for our current livelihoods; we were not one-off students taking a research methodology course or first year introductory course. We benefitted from the successful (and possibly the not so successful) interactions we had with our sighted teachers. It is our hope that by saving staff from wasted effort, we can spare students from unnecessary discomfort in classes that could improve their future employment prospects. Our aim is therefore to provide practical support for our sighted colleagues and blind peers as we work together towards the empowerment of blind students in becoming competent producers of statistical information, not just consumers who interpret that information.},
  keywords     = {Braille,Low vision,Speech output,Tactile images},
  file         = {C:\Users\jseo1005\Zotero\storage\8GHY4ISY\Godfrey and Loots - 2015 - Advice From Blind Teachers on How to Teach Statist.pdf}
}

@article{godfreyStatisticalSoftwareBlind2013,
  title        = {Statistical {{Software}} from a {{Blind Person}}'s {{Perspective}}},
  author       = {Godfrey, Jonathan,R., A.},
  date         = {2013},
  journaltitle = {The R Journal},
  shortjournal = {The R Journal},
  volume       = {5},
  number       = {1},
  pages        = {73},
  issn         = {2073-4859},
  doi          = {10.32614/RJ-2013-007},
  url          = {https://journal.r-project.org/archive/2013/RJ-2013-007/index.html},
  urldate      = {2022-08-22},
  abstract     = {Blind people have experienced access issues to many software applications since the advent of the Windows operating system; statistical software has proven to follow the rule and not be an exception. The ability to use R within minutes of download with next to no adaptation has opened doors for accessible production of statistical analyses for this author (himself blind) and blind students around the world. This article shows how little is required to make R the most accessible statistical software available today. There is any number of ramifications that this opportunity creates for blind students, especially in terms of their future research and employment prospects. There is potential for making R even better for blind users. The extensibility of R makes this possible through added functionality being made available in an add-on package called BrailleR. Functions in this package are intended to make graphical information available in text form.},
  langid       = {english},
  file         = {C:\Users\jseo1005\Zotero\storage\HGNVE9WY\Godfrey - 2013 - Statistical Software from a Blind Person's Perspec.pdf}
}

@article{godfreyStatisticalSoftwareBlind2013a,
  title        = {Statistical {{Software}} from a {{Blind Person}}'s {{Perspective}}},
  author       = {Godfrey, Jonathan,R., A.},
  date         = {2013},
  journaltitle = {The R Journal},
  shortjournal = {The R Journal},
  volume       = {5},
  number       = {1},
  pages        = {73},
  issn         = {2073-4859},
  doi          = {10.32614/RJ-2013-007},
  url          = {https://journal.r-project.org/archive/2013/RJ-2013-007/index.html},
  urldate      = {2024-02-06},
  abstract     = {Blind people have experienced access issues to many software applications since the advent of the Windows operating system; statistical software has proven to follow the rule and not be an exception. The ability to use R within minutes of download with next to no adaptation has opened doors for accessible production of statistical analyses for this author (himself blind) and blind students around the world. This article shows how little is required to make R the most accessible statistical software available today. There is any number of ramifications that this opportunity creates for blind students, especially in terms of their future research and employment prospects. There is potential for making R even better for blind users. The extensibility of R makes this possible through added functionality being made available in an add-on package called BrailleR. Functions in this package are intended to make graphical information available in text form.},
  langid       = {english},
  file         = {C:\Users\jseo1005\Zotero\storage\H8GEUPYU\Godfrey - 2013 - Statistical Software from a Blind Person's Perspec.pdf}
}

@inproceedings{gorniakVizAbilityMultimodalAccessible2023,
  title      = {{{VizAbility}}: {{Multimodal Accessible Data Visualization}} with {{Keyboard Navigation}} and {{Conversational Interaction}}},
  shorttitle = {{{VizAbility}}},
  booktitle  = {Adjunct {{Proceedings}} of the 36th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author     = {Gorniak, Joshua and Ottiger, Jacob and Wei, Donglai and Kim, Nam Wook},
  date       = {2023-10-29},
  pages      = {1--3},
  publisher  = {ACM},
  location   = {San Francisco CA USA},
  doi        = {10.1145/3586182.3616669},
  url        = {https://dl.acm.org/doi/10.1145/3586182.3616669},
  urldate    = {2024-03-25},
  eventtitle = {{{UIST}} '23: {{The}} 36th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  isbn       = {9798400700965},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\PHEJDQFH\Gorniak et al. - 2023 - VizAbility Multimodal Accessible Data Visualizati.pdf}
}

@inproceedings{gotzelmannLucentMaps3DPrinted2016,
  title      = {{{LucentMaps}}: {{3D Printed Audiovisual Tactile Maps}} for {{Blind}} and {{Visually Impaired People}}},
  shorttitle = {{{LucentMaps}}},
  booktitle  = {Proceedings of the 18th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author     = {Götzelmann, Timo},
  date       = {2016-10-23},
  series     = {{{ASSETS}} '16},
  pages      = {81--90},
  publisher  = {Association for Computing Machinery},
  location   = {New York, NY, USA},
  doi        = {10.1145/2982142.2982163},
  url        = {https://dl.acm.org/doi/10.1145/2982142.2982163},
  urldate    = {2023-09-09},
  abstract   = {Tactile maps support blind and visually impaired people in orientation and to familiarize with unfamiliar environments. Interactive approaches complement these maps with auditory feedback. However, commonly these approaches focus on blind people. We present an approach which incorporates visually impaired people by visually augmenting relevant parts of tactile maps. These audiovisual tactile maps can be used in conjunction with common tablet computers and smartphones. By integrating conductive elements into 3D printed tactile maps, they can be recognized by a single touch on the mobile device's display, which eases the handling for blind and visually impaired people. To allow multiple elevation levels in our transparent tactile maps, we conducted a study to reconcile technical and physiological requirements of off-the-shelf 3D printers, capacitive touch inputs and the human tactile sense. We propose an interaction concept for 3D printed audiovisual tactile maps, verify its feasibility and test it with a user study. Our discussion includes economic considerations crucial for a broad dissemination of tactile maps for both blind and visually impaired people.},
  isbn       = {978-1-4503-4124-0},
  keywords   = {3d printing,accessibility,audio-tactile,blind,capacitive,capacitive sensing,functional,global,marker,orientation,tactile maps,tangible user interfaces,touch screen},
  file       = {C:\Users\jseo1005\Zotero\storage\CE9AZRH9\Götzelmann - 2016 - LucentMaps 3D Printed Audiovisual Tactile Maps fo.pdf}
}

@article{gotzelmannVisuallyAugmentedAudioTactile2018,
  title        = {Visually {{Augmented Audio-Tactile Graphics}} for {{Visually Impaired People}}},
  author       = {Götzelmann, T.},
  date         = {2018-06-08},
  journaltitle = {ACM Transactions on Accessible Computing},
  shortjournal = {ACM Trans. Access. Comput.},
  volume       = {11},
  number       = {2},
  pages        = {8:1--8:31},
  issn         = {1936-7228},
  doi          = {10.1145/3186894},
  url          = {https://dl.acm.org/doi/10.1145/3186894},
  urldate      = {2023-09-09},
  abstract     = {Tactile graphics play an essential role in knowledge transfer for blind people. The tactile exploration of these graphics is often challenging because of the cognitive load caused by physiological constraints and their complexity. The coupling of physical tactile graphics with electronic devices offers to support the tactile exploration by auditory feedback. Often, these systems have strict constraints regarding their mobility or the process of coupling both components. Additionally, visually impaired people cannot appropriately benefit from their residual vision. This article presents a concept for 3D printed tactile graphics, which offers to use audio-tactile graphics with usual smartphones or tablet-computers. By using capacitive markers, the coupling of the tactile graphics with the mobile device is simplified. These tactile graphics integrating these markers can be printed in one turn by off-the-shelf 3D printers without any post-processing and allows us to use multiple elevation levels for graphical elements. Based on the developed generic concept on visually augmented audio-tactile graphics, we presented a case study for maps. A prototypical implementation was tested by a user study with visually impaired people. All the participants were able to interact with the 3D printed tactile maps using a standard tablet computer. To study the effect of visual augmentation of graphical elements, we conducted another comprehensive user study. We tested multiple types of graphics and obtained evidence that visual augmentation may offer clear advantages for the exploration of tactile graphics. Even participants with a minor residual vision could solve the tasks with visual augmentation more quickly and accurately.},
  keywords     = {3D printing,accessibility,audio-tactile,augmented,blind,capacitive,capacitive sensing,global,marker,orientation,Tactile graphics,tangible user interfaces,touch screen,visually impaired},
  file         = {C:\Users\jseo1005\Zotero\storage\6T38EXCR\Götzelmann - 2018 - Visually Augmented Audio-Tactile Graphics for Visu.pdf}
}

@online{gouldEffectivePracticesDescription2008,
  title   = {Effective {{Practices}} for {{Description}} of {{Science Content}} within {{Digital Talking Books}}},
  author  = {Gould, Bryan and O'Connell, Trisha and Freed, Geoff},
  date    = {2008-12},
  url     = {http://ncamftp.wgbh.org/ncam-old-site/experience_learn/educational_media/stemdx.html},
  urldate = {2023-08-27},
  file    = {C:\Users\jseo1005\Zotero\storage\UL4K86A9\Experience + Learn  Educational Media  Effective Practices for Description of Science Content with.pdf}
}

@online{GraphitiBreakthroughNonVisual,
  title        = {{{Graphiti}}® - a {{Breakthrough}} in {{Non-Visual Access}} to {{All Forms}} of {{Graphical Information}}},
  url          = {https://www.orbitresearch.com/product/graphiti/},
  urldate      = {2023-12-10},
  abstract     = {Graphiti® is an Interactive Tactile Graphics Display based on revolutionary Tactuator™ technology from Orbit Research. It represents a breakthrough in non-visual access to any form of graphical information such as charts, drawings, flowcharts, floorplans, images and photographs, through an array of moving pins.},
  langid       = {american},
  organization = {Orbit Research},
  file         = {C:\Users\jseo1005\Zotero\storage\HZLAS6TR\graphiti.html}
}

@inproceedings{guinnessCaptionCrawlerEnabling2018,
  title      = {Caption {{Crawler}}: {{Enabling Reusable Alternative Text Descriptions}} Using {{Reverse Image Search}}},
  shorttitle = {Caption {{Crawler}}},
  booktitle  = {Proceedings of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author     = {Guinness, Darren and Cutrell, Edward and Morris, Meredith Ringel},
  date       = {2018-04-21},
  pages      = {1--11},
  publisher  = {ACM},
  location   = {Montreal QC Canada},
  doi        = {10.1145/3173574.3174092},
  url        = {https://dl.acm.org/doi/10.1145/3173574.3174092},
  urldate    = {2024-01-19},
  eventtitle = {{{CHI}} '18: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn       = {978-1-4503-5620-6},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\34JCZQEJ\Guinness et al. - 2018 - Caption Crawler Enabling Reusable Alternative Tex.pdf}
}

@article{halkosaariDesigningMultichannelMap2013,
  title        = {Designing a {{Multichannel Map Service Concept}}},
  author       = {Halkosaari, Hanna-Marika and Sarjakoski, L. and Ylirisku, Salu and Sarjakoski, Tapani},
  date         = {2013-05-31},
  journaltitle = {Human Technology},
  shortjournal = {Human Technology},
  volume       = {9},
  pages        = {72--91},
  doi          = {10.17011/ht/urn.201305211723},
  abstract     = {This paper introduces a user-centered design process for developing a multichannel map service. The aim of the service is to provide hikers with interactive maps through several channels. In a multichannel map service, the same spatial information is available through various channels, such as printed maps, Web maps, mobile maps, and other interactive media. When properly networked, the channels share a uniform identity so that the user experiences the different channels as a part of a single map service. The traditional methods of user-centered design, such as design probes, personas, and scenarios, proved useful even in the emerging field of developing multichannel map services. The findings emphasize the need to involve users and multidisciplinary teams in the conceptual phases of designing complex services aimed at serving various kinds of users.},
  file         = {C:\Users\jseo1005\Zotero\storage\9JUH8KDH\Halkosaari et al. - 2013 - Designing a Multichannel Map Service Concept.pdf}
}

@incollection{hartDevelopmentNASATLXTask1988,
  title      = {Development of {{NASA-TLX}} ({{Task Load Index}}): {{Results}} of {{Empirical}} and {{Theoretical Research}}},
  shorttitle = {Development of {{NASA-TLX}} ({{Task Load Index}})},
  booktitle  = {Advances in {{Psychology}}},
  author     = {Hart, Sandra G. and Staveland, Lowell E.},
  editor     = {Hancock, Peter A. and Meshkati, Najmedin},
  date       = {1988-01-01},
  series     = {Human {{Mental Workload}}},
  volume     = {52},
  pages      = {139--183},
  publisher  = {North-Holland},
  doi        = {10.1016/S0166-4115(08)62386-9},
  url        = {https://www.sciencedirect.com/science/article/pii/S0166411508623869},
  urldate    = {2023-03-19},
  abstract   = {The results of a multi-year research program to identify the factors associated with variations in subjective workload within and between different types of tasks are reviewed. Subjective evaluations of 10 workload-related factors were obtained from 16 different experiments. The experimental tasks included simple cognitive and manual control tasks, complex laboratory and supervisory control tasks, and aircraft simulation. Task-, behavior-, and subject-related correlates of subjective workload experiences varied as a function of difficulty manipulations within experiments, different sources of workload between experiments, and individual differences in workload definition. A multi-dimensional rating scale is proposed in which information about the magnitude and sources of six workload-related factors are combined to derive a sensitive and reliable estimate of workload.},
  langid     = {english},
  file       = {C\:\\Users\\jseo1005\\Zotero\\storage\\G9FV42GA\\Hart and Staveland - 1988 - Development of NASA-TLX (Task Load Index) Results.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\C4YNUMGI\\S0166411508623869.html}
}

@software{hassakuAudioplotlib2022,
  title    = {Audio-Plot-Lib},
  author   = {{hassaku}},
  date     = {2022-07-19T09:41:21Z},
  origdate = {2020-12-13T07:16:03Z},
  url      = {https://github.com/hassaku/audio-plot-lib},
  urldate  = {2022-08-21},
  abstract = {This library provides graph sonification functions and has been developed for a project named "Data science and machine learning resources for screen reader users". Please refer to the project page for more details.},
  keywords = {audio,data-science,google-colab,graphs,machine-learning,python,sonification,visually-impaired}
}

@inproceedings{headleyDisplayingBrailleGraphics2011,
  title     = {Displaying Braille and Graphics on a Mouse-like Tactile Display},
  booktitle = {The Proceedings of the 13th International {{ACM SIGACCESS}} Conference on {{Computers}} and Accessibility},
  author    = {Headley, Patrick C. and Hribar, Victoria E. and Pawluk, Dianne T.V.},
  date      = {2011-10-24},
  series    = {{{ASSETS}} '11},
  pages     = {235--236},
  publisher = {Association for Computing Machinery},
  location  = {New York, NY, USA},
  doi       = {10.1145/2049536.2049584},
  url       = {https://dl.acm.org/doi/10.1145/2049536.2049584},
  urldate   = {2023-11-13},
  abstract  = {For presenting graphics on small, moveable tactile displays such as those that resemble computer mice, word labels can be as important as the diagram itself. In addition, the ability to present Braille with these displays offers an alternative for accessing full pages of Braille with a cost effective system. In this work, we consider the inherent difficulties arising from presenting Braille on these displays and propose algorithms to circumvent these problems. Lastly, we present preliminary results from individuals who are visually impaired that suggests the promise of this approach.},
  isbn      = {978-1-4503-0920-2},
  keywords  = {braille,haptic mouse,haptics,tactile graphics,visually impaired},
  file      = {C:\Users\jseo1005\Zotero\storage\2ACZ2HJW\Headley et al. - 2011 - Displaying braille and graphics on a mouse-like tactile display.pdf}
}

@article{heerenInfluenceDynamicBinaural2016,
  title    = {The {{Influence}} of {{Dynamic Binaural Cues}} on {{Speech Intelligibility}} at {{Low}} and {{High Frequencies}}},
  author   = {Heeren, J and Grimm, G and Hohmann, V},
  date     = {2016},
  abstract = {The amount of spatial release from masking is mainly determined by the change in interaural time difference (ITD) of the noise relative to the ITD of the signal [12]. Accordingly, speech-in-noise with frontal speech presentation and noise from the front or back (S0N0 and S0N180 conditions) lead to similar detection and speech intelligibility thresholds. However, head movements can introduce dynamic binaural cues that may lead to a release from masking (RFM). In this study the effect of dynamic binaural cues on speech intelligibility was investigated for lowpass and highpass filtered signals to assess the influence of ITDs and interaural level differences. Movements were implemented as modulations of the nominal azimuths of the sound sources (S0N180, S0N0). These modulations were either in-phase or anti-phase for S and N. The stimuli were rendered using eleventh order ambisonics with 'basic' decoding, and presented via loudspeakers. Speech and noise signals were filtered at 1000 Hz (lowpass), 1500 - Hz (highpass) or unfiltered. Results show a significant RFM with dynamic binaural cues for S0N0 in all filter conditions. For S0N180 only the unfiltered condition shows a significant RFM.},
  langid   = {english},
  file     = {C:\Users\jseo1005\Zotero\storage\PMRSE4X9\Heeren et al. - 2016 - The Influence of Dynamic Binaural Cues on Speech I.pdf}
}

@inproceedings{heTacTILEPreliminaryToolchain2017,
  title      = {{{TacTILE}}: {{A Preliminary Toolchain}} for {{Creating Accessible Graphics}} with {{3D-Printed Overlays}} and {{Auditory Annotations}}},
  shorttitle = {{{TacTILE}}},
  booktitle  = {Proceedings of the 19th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author     = {He, Liang and Wan, Zijian and Findlater, Leah and Froehlich, Jon E.},
  date       = {2017-10-19},
  series     = {{{ASSETS}} '17},
  pages      = {397--398},
  publisher  = {Association for Computing Machinery},
  location   = {New York, NY, USA},
  doi        = {10.1145/3132525.3134818},
  url        = {https://dl.acm.org/doi/10.1145/3132525.3134818},
  urldate    = {2023-09-09},
  abstract   = {Tactile overlays with audio annotations can increase the accessibility of touchscreens for blind users; however, preparing these overlays is complex and labor intensive. We introduce TacTILE, a novel toolchain to more easily create tactile overlays with audio annotations for arbitrary touchscreen graphics (e.g., graphs, pictures, maps). The workflow includes: (i) an annotation tool to add audio to graphical elements, (ii) a fabrication process that generates 3D-printed tactile overlays, and (iii) a custom app for the user to explore graphics with these overlays. We close with a pilot study with one blind participant who explores three examples (floor plan, photo, and chart), and a discussion of future work.},
  isbn       = {978-1-4503-4926-0},
  keywords   = {3d printing,accessible graphics,blind users,speech,tactile overlays,touchscreens.,visual impairments},
  file       = {C:\Users\jseo1005\Zotero\storage\T355EZW5\He et al. - 2017 - TacTILE A Preliminary Toolchain for Creating Acce.pdf}
}

@software{HighchartsHighcharts2022,
  title        = {Highcharts/Highcharts},
  date         = {2022-08-21T14:58:48Z},
  origdate     = {2010-06-11T12:23:53Z},
  url          = {https://github.com/highcharts/highcharts},
  urldate      = {2022-08-21},
  abstract     = {Highcharts JS, the JavaScript charting framework},
  organization = {Highcharts}
}

@software{HighchartsSonificationStudio2022,
  title        = {Highcharts {{Sonification Studio}}},
  date         = {2022-11-18T10:46:22Z},
  origdate     = {2019-08-26T11:20:08Z},
  url          = {https://github.com/highcharts/sonification-studio},
  urldate      = {2023-03-11},
  organization = {Highcharts}
}

@inproceedings{hollowayAnimationsYourFingertips2022,
  title      = {Animations at {{Your Fingertips}}: {{Using}} a {{Refreshable Tactile Display}} to {{Convey Motion Graphics}} for {{People}} Who Are {{Blind}} or Have {{Low Vision}}},
  shorttitle = {Animations at {{Your Fingertips}}},
  booktitle  = {The 24th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author     = {Holloway, Leona and Ananthanarayan, Swamy and Butler, Matthew and De Silva, Madhuka Thisuri and Ellis, Kirsten and Goncu, Cagatay and Stephens, Kate and Marriott, Kim},
  date       = {2022-10-22},
  pages      = {1--16},
  publisher  = {ACM},
  location   = {Athens Greece},
  doi        = {10.1145/3517428.3544797},
  url        = {https://dl.acm.org/doi/10.1145/3517428.3544797},
  urldate    = {2023-09-07},
  eventtitle = {{{ASSETS}} '22: {{The}} 24th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  isbn       = {978-1-4503-9258-7},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\HWAUDDT7\Holloway et al. - 2022 - Animations at Your Fingertips Using a Refreshable Tactile Display to Convey Motion Graphics for Peo.pdf}
}

@inproceedings{hollowayAnimationsYourFingertips2022a,
  title      = {Animations at {{Your Fingertips}}: {{Using}} a {{Refreshable Tactile Display}} to {{Convey Motion Graphics}} for {{People}} Who Are {{Blind}} or Have {{Low Vision}}},
  shorttitle = {Animations at {{Your Fingertips}}},
  booktitle  = {Proceedings of the 24th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author     = {Holloway, Leona and Ananthanarayan, Swamy and Butler, Matthew and De Silva, Madhuka Thisuri and Ellis, Kirsten and Goncu, Cagatay and Stephens, Kate and Marriott, Kim},
  date       = {2022-10-22},
  series     = {{{ASSETS}} '22},
  pages      = {1--16},
  publisher  = {Association for Computing Machinery},
  location   = {New York, NY, USA},
  doi        = {10.1145/3517428.3544797},
  url        = {https://dl.acm.org/doi/10.1145/3517428.3544797},
  urldate    = {2023-11-13},
  abstract   = {People who are blind rely on touch and hearing to understand the world around them, however it is extremely difficult to understand movement through these modes. The advent of refreshable tactile displays (RTDs) offers the potential for blind people to access tactile animations for the very first time. A survey of touch readers and vision accessibility experts revealed a high level of enthusiasm for tactile animations, particularly those relating to education, mapping and concept development. Based on these suggestions, a range of tactile animations were developed and four were presented to 12 touch readers. The RTD held advantages over traditional tactile graphics for conveying movement, depth and height, however there were trade-offs in terms of resolution and textural properties. This work offers a first glimpse into how refreshable tactile displays can best be utilised to convey animated graphics for people who are blind.},
  isbn       = {978-1-4503-9258-7},
  keywords   = {accessible graphics,blind,refreshable displays,tactile graphics},
  file       = {C:\Users\jseo1005\Zotero\storage\AB8262ZY\Holloway et al. - 2022 - Animations at Your Fingertips Using a Refreshable Tactile Display to Convey Motion Graphics for Peo.pdf}
}

@article{hooperDesigningMoreEffective2011,
  title        = {Towards Designing More Effective Systems by Understanding User Experiences},
  author       = {Hooper, Clare J.},
  date         = {2011-09},
  journaltitle = {ACM SIGWEB Newsletter},
  shortjournal = {SIGWEB Newsl.},
  pages        = {1--3},
  issn         = {1931-1745, 1931-1435},
  doi          = {10.1145/2020936.2020940},
  url          = {https://dl.acm.org/doi/10.1145/2020936.2020940},
  urldate      = {2022-08-21},
  abstract     = {This work is about social technologies, user experiences and the problems of creative design. It is motivated by a desire to give people who are offline --- whether for reasons of poverty, disability, infrastructure or cultural background --- the access to social technologies that is currently provided via the web, letting them access the online content and communication facilities that so many of us take for granted. There exist simple technologically-oriented approaches to this problem, such as identifying functional requirements and prototyping tools. This focus on technology, however, comes at a cost of neglecting the experiential aspects which motivate the work, and can result in systems that are functional but unappealing to (or even unusable by) their target audiences.},
  issue        = {Autumn},
  langid       = {english},
  file         = {C:\Users\jseo1005\Zotero\storage\JF48F5CD\Hooper - 2011 - Towards designing more effective systems by unders.pdf}
}

@inproceedings{hoqueAccessibleDataRepresentation2023,
  title     = {Accessible {{Data Representation}} with {{Natural Sound}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author    = {Hoque, Md Naimul and Ehtesham-Ul-Haque, Md and Elmqvist, Niklas and Billah, Syed Masum},
  date      = {2023-04-19},
  series    = {{{CHI}} '23},
  pages     = {1--19},
  publisher = {Association for Computing Machinery},
  location  = {New York, NY, USA},
  doi       = {10.1145/3544548.3581087},
  url       = {https://dl.acm.org/doi/10.1145/3544548.3581087},
  urldate   = {2023-09-10},
  abstract  = {Sonification translates data into non-speech audio. Such auditory representations can make data visualization accessible to people who are blind or have low vision (BLV). This paper presents a sonification method for translating common data visualization into a blend of natural sounds. We hypothesize that people’s familiarity with sounds drawn from nature, such as birds singing in a forest, and their ability to listen to these sounds in parallel, will enable BLV users to perceive multiple data points being sonified at the same time. Informed by an extensive literature review and a preliminary study with 5 BLV participants, we designed an accessible data representation tool, Susurrus, that combines our sonification method with other accessibility features, such as keyboard interaction and text-to-speech feedback. Finally, we conducted a user study with 12 BLV participants and report the potential and application of natural sounds for sonification compared to existing sonification tools.},
  isbn      = {978-1-4503-9421-5},
  keywords  = {Accessibility,Data visualization,Natural sound,Sonification},
  file      = {C:\Users\jseo1005\Zotero\storage\Q5SR9E3T\Hoque et al. - 2023 - Accessible Data Representation with Natural Sound.pdf}
}

@article{hunterMatplotlib2DGraphics2007,
  title        = {Matplotlib: {{A 2D Graphics Environment}}},
  shorttitle   = {Matplotlib},
  author       = {Hunter, John D.},
  date         = {2007-05-01},
  journaltitle = {Computing in Science \& Engineering},
  volume       = {9},
  number       = {03},
  pages        = {90--95},
  publisher    = {IEEE Computer Society},
  issn         = {1521-9615},
  doi          = {10.1109/MCSE.2007.55},
  url          = {https://www.computer.org/csdl/magazine/cs/2007/03/c3090/13rRUwbJD0A},
  urldate      = {2023-01-24},
  abstract     = {Matplotlib is a 2D graphics package for Python for application development, interactive scripting, and publication-quality image generation across user interfaces and operating systems.},
  langid       = {english}
}

@article{huntInteractiveSonification2011,
  title        = {Interactive {{Sonification}}},
  author       = {Hunt, Andy and Hermann, Thomas},
  date         = {2011},
  journaltitle = {The Sonification Handbook},
  url          = {https://pub.uni-bielefeld.de/record/2935181},
  urldate      = {2023-09-09},
  abstract     = {This chapter places a special focus on those situations where there is a tight control loop (a real-time interactive collaboration) between the human user and the system producing the sonification. It explains the background (why humans appear to use interactive sonification as a natural tool for exploring the world) as well as describing the different methods and application domains.},
  isbn         = {9783832528195},
  langid       = {english},
  file         = {C:\Users\jseo1005\Zotero\storage\ERJKZBBJ\2935181.html}
}

@online{IDEODesignThinking,
  title        = {{{IDEO Design Thinking}}},
  url          = {https://designthinking.ideo.com},
  urldate      = {2024-04-23},
  abstract     = {IDEO introduces design thinking, how it came to be, how it is being used, and steps and tools for mastering it.},
  organization = {IDEO | Design Thinking},
  file         = {C:\Users\jseo1005\Zotero\storage\FN9I7GKY\designthinking.ideo.com.html}
}

@inproceedings{ivanchevPrejourneyVisualizationTravel2014,
  title     = {Pre-Journey {{Visualization}} of {{Travel Routes}} for the {{Blind}} on {{Refreshable Interactive Tactile Displays}}},
  booktitle = {Computers {{Helping People}} with {{Special Needs}}},
  author    = {Ivanchev, Mihail and Zinke, Francis and Lucke, Ulrike},
  editor    = {Miesenberger, Klaus and Fels, Deborah and Archambault, Dominique and Peňáz, Petr and Zagler, Wolfgang},
  date      = {2014},
  series    = {Lecture {{Notes}} in {{Computer Science}}},
  pages     = {81--88},
  publisher = {Springer International Publishing},
  location  = {Cham},
  doi       = {10.1007/978-3-319-08599-9_13},
  abstract  = {In this paper we report on our continuing research of an audio-tactile system for visualizing travel routes on modern interactive refreshable tactile displays for blind users. The system is especially well suited for pre-journey route learning. Similar to systems for sighted users, e. g. online map services like Google Maps, we utilize an audio-tactile interactive map based on a concept from third-party research work and freely available geographic data. The system was implemented as a prototype for a touch-sensitive tactile display. Our main research interest is to explore audio-tactile concepts for displaying routes on a slippy map. We therefore developed a catalogue of ideas currently featuring tactile textures and indications for the route’s course, waypoint symbols, audio indications etc. We summarize the results of an initial user test which indicates that the route visualization with our set of strategies is feasible and justifies further research.},
  isbn      = {978-3-319-08599-9},
  langid    = {english},
  keywords  = {Accessible Geographic Routes,Blind,GIS,Visually Impaired},
  file      = {C:\Users\jseo1005\Zotero\storage\WWNGN2AF\Ivanchev et al. - 2014 - Pre-journey Visualization of Travel Routes for the Blind on Refreshable Interactive Tactile Displays.pdf}
}

@inproceedings{jayantAutomatedTactileGraphics2007,
  title      = {Automated Tactile Graphics Translation: In the Field},
  shorttitle = {Automated Tactile Graphics Translation},
  booktitle  = {Proceedings of the 9th International {{ACM SIGACCESS}} Conference on {{Computers}} and Accessibility},
  author     = {Jayant, Chandrika and Renzelmann, Matt and Wen, Dana and Krisnandi, Satria and Ladner, Richard and Comden, Dan},
  date       = {2007-10-15},
  series     = {Assets '07},
  pages      = {75--82},
  publisher  = {Association for Computing Machinery},
  location   = {New York, NY, USA},
  doi        = {10.1145/1296843.1296858},
  url        = {https://dl.acm.org/doi/10.1145/1296843.1296858},
  urldate    = {2023-11-13},
  abstract   = {We address the practical problem of automating the process of translating figures from mathematics, science, and engineering textbooks to a tactile form suitable for blind students. The Tactile Graphics Assistant (TGA) and accompanying workflow is described. Components of the TGA that identify text and replace it with Braille use machine learning, computational geometry, and optimization algorithms. We followed through with the ideas in our 2005 paper by creating a more detailed workflow, translating actual images, and analyzing the translation time. Our experience in translating more than 2,300 figures from 4 textbooks demonstrates that figures can be translated in ten minutes or less of human time on average. We describe our experience with training tactile graphics specialists to use the new TGA technology.},
  isbn       = {978-1-59593-573-1},
  keywords   = {accessibility,braille,disability,image processing,machine learning,tactile graphics,user study},
  file       = {C:\Users\jseo1005\Zotero\storage\83HJG9KW\Jayant et al. - 2007 - Automated tactile graphics translation in the field.pdf}
}

@article{jooyoungseoTeachingVisualAccessibility2023,
  title        = {Teaching {{Visual Accessibility}} in {{Introductory Data Science Classes}} with {{Multi-Modal Data Representations}}},
  author       = {{Jooyoung Seo} and Dogucu, Mine},
  date         = {2023-04},
  journaltitle = {Journal of Data Science},
  volume       = {21},
  number       = {2},
  pages        = {428--441},
  publisher    = {National University of Kaohsiung, Department of Applied Mathematics},
  issn         = {1680743X},
  doi          = {10.6339/23-JDS1095},
  url          = {https://proxy2.library.illinois.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=asn&AN=164408757&site=eds-live&scope=site},
  urldate      = {2024-02-09},
  abstract     = {Although there are various ways to represent data patterns and models, visualization has been primarily taught in many data science courses for its efficiency. Such vision-dependent output may cause critical barriers against those who are blind and visually impaired and people with learning disabilities. We argue that instructors need to teach multiple data representation methods so that all students can produce data products that are more accessible. In this paper, we argue that accessibility should be taught as early as the introductory course as part of the data science curriculum so that regardless of whether learners major in data science or not, they can have foundational exposure to accessibility. As data science educators who teach accessibility as part of our lower-division courses in two different institutions, we share specific examples that can be utilized by other data science instructors.},
  keywords     = {curriculum,data representations,DATA science,MEDICAL quality control,PEOPLE with learning disabilities,PEOPLE with visual disabilities,R},
  file         = {C:\Users\jseo1005\Zotero\storage\SWJPLNWK\Jooyoung Seo and Dogucu - 2023 - Teaching Visual Accessibility in Introductory Data.pdf}
}

@inproceedings{joynerVisualizationAccessibilityWild2022,
  title      = {Visualization {{Accessibility}} in the {{Wild}}: {{Challenges Faced}} by {{Visualization Designers}}},
  shorttitle = {Visualization {{Accessibility}} in the {{Wild}}},
  booktitle  = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author     = {Joyner, Shakila Cherise S and Riegelhuth, Amalia and Garrity, Kathleen and Kim, Yea-Seul and Kim, Nam Wook},
  date       = {2022-04-27},
  pages      = {1--19},
  publisher  = {ACM},
  location   = {New Orleans LA USA},
  doi        = {10.1145/3491102.3517630},
  url        = {https://dl.acm.org/doi/10.1145/3491102.3517630},
  urldate    = {2022-12-29},
  abstract   = {Data visualizations are now widely used across many disciplines. However, many of them are not easily accessible for visually impaired people. In this work, we use three-staged mixed methods to understand the current practice of accessible visualization design for visually impaired people. We analyzed 95 visualizations from various venues to inspect how they are made inaccessible. To understand the rationale and context behind the design choices, we also conducted surveys with 144 practitioners in the U.S. and follow-up interviews with ten selected survey participants. Our findings include the difficulties of handling modern complex and interactive visualizations and the lack of accessibility support from visualization tools in addition to personal and organizational factors making it challenging to perform accessible design practices.},
  eventtitle = {{{CHI}} '22: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn       = {978-1-4503-9157-3},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\B63HSLUG\Joyner et al. - 2022 - Visualization Accessibility in the Wild Challenge.pdf}
}

@software{julianna-langstonChart2Music2022,
  title    = {{{Chart2Music}}},
  author   = {family=langston, prefix=julianna-, useprefix=true},
  date     = {2022-08-15T04:28:31Z},
  origdate = {2022-06-12T02:59:46Z},
  url      = {https://github.com/julianna-langston/chart2music},
  urldate  = {2022-08-20},
  abstract = {Turns charts into music so the blind can hear data}
}

@article{jungCommunicatingVisualizationsVisuals2022,
  title        = {Communicating {{Visualizations}} without {{Visuals}}: {{Investigation}} of {{Visualization Alternative Text}} for {{People}} with {{Visual Impairments}}},
  shorttitle   = {Communicating {{Visualizations}} without {{Visuals}}},
  author       = {Jung, Crescentia and Mehta, Shubham and Kulkarni, Atharva and Zhao, Yuhang and Kim, Yea-Seul},
  date         = {2022-01},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  volume       = {28},
  number       = {1},
  pages        = {1095--1105},
  issn         = {1941-0506},
  doi          = {10.1109/TVCG.2021.3114846},
  url          = {https://ieeexplore.ieee.org/abstract/document/9552938?casa_token=FAQvoeKbqDcAAAAA:CRNX1Jjc1TBazkS6k19cP-Zp-WzQ952yug9YG4yuv82mrsnhzU8MrUkSleBy2Vhn2NhaNVXSxJo},
  urldate      = {2024-04-21},
  abstract     = {Alternative text is critical in communicating graphics to people who are blind or have low vision. Especially for graphics that contain rich information, such as visualizations, poorly written or an absence of alternative texts can worsen the information access inequality for people with visual impairments. In this work, we consolidate existing guidelines and survey current practices to inspect to what extent current practices and recommendations are aligned. Then, to gain more insight into what people want in visualization alternative texts, we interviewed 22 people with visual impairments regarding their experience with visualizations and their information needs in alternative texts. The study findings suggest that participants actively try to construct an image of visualizations in their head while listening to alternative texts and wish to carry out visualization tasks (e.g., retrieve specific values) as sighted viewers would. The study also provides ample support for the need to reference the underlying data instead of visual elements to reduce users' cognitive burden. Informed by the study, we provide a set of recommendations to compose an informative alternative text.},
  eventtitle   = {{{IEEE Transactions}} on {{Visualization}} and {{Computer Graphics}}},
  keywords     = {accessible visualization,alternative text for graphics,assistive technologies,Data visualization,Guidelines,Image color analysis,Social networking (online),Software,Visualization,Web pages},
  file         = {C:\Users\jseo1005\Zotero\storage\CPKIEZMU\9552938.html}
}

@inproceedings{kadayatImpactSentenceLength2020,
  title     = {Impact of {{Sentence Length}} on the {{Readability}} of {{Web}} for {{Screen Reader Users}}},
  booktitle = {Universal {{Access}} in {{Human-Computer Interaction}}. {{Design Approaches}} and {{Supporting Technologies}}},
  author    = {Kadayat, Bam Bahadur and Eika, Evelyn},
  editor    = {Antona, Margherita and Stephanidis, Constantine},
  date      = {2020},
  series    = {Lecture {{Notes}} in {{Computer Science}}},
  pages     = {261--271},
  publisher = {Springer International Publishing},
  location  = {Cham},
  doi       = {10.1007/978-3-030-49282-3_18},
  abstract  = {Readability of text is generally believed to be connected to sentence length. Most studies on readability are based on visual reading. Less is known about text readability for users relying on screen readers, such as users who are blind. This study therefore set out to investigate the effect of sentence length on the readability of web texts accessed using screen readers. A controlled within-subjects experiment was performed with twenty-one participants. Participants used a screen reader to read five texts with different sentence lengths. The participants’ comprehension and perceived workload were measured. The findings reveal that there is a significant effect of sentence length and most participants exhibit the highest comprehension and lowest workload with sentences comprising 16–20 words. Implications of these results are that web content providers should strive for sentence length of 16–20 words to maximize readability.},
  isbn      = {978-3-030-49282-3},
  langid    = {english},
  keywords  = {Accessibility,Blind,Readability,Screen reader,Sentence length,Universal design,Workload},
  file      = {C:\Users\jseo1005\Zotero\storage\RG3JKF76\Kadayat and Eika - 2020 - Impact of Sentence Length on the Readability of We.pdf}
}

@article{kavazChatbotBasedNaturalLanguage2023,
  title        = {Chatbot-{{Based Natural Language Interfaces}} for {{Data Visualisation}}: {{A Scoping Review}}},
  shorttitle   = {Chatbot-{{Based Natural Language Interfaces}} for {{Data Visualisation}}},
  author       = {Kavaz, Ecem and Puig, Anna and Rodríguez, Inmaculada},
  date         = {2023-01},
  journaltitle = {Applied Sciences},
  volume       = {13},
  number       = {12},
  pages        = {7025},
  publisher    = {Multidisciplinary Digital Publishing Institute},
  issn         = {2076-3417},
  doi          = {10.3390/app13127025},
  url          = {https://www.mdpi.com/2076-3417/13/12/7025},
  urldate      = {2024-03-25},
  abstract     = {Rapid growth in the generation of data from various sources has made data visualisation a valuable tool for analysing data. However, visual analysis can be a challenging task, not only due to intricate dashboards but also when dealing with complex and multidimensional data. In this context, advances in Natural Language Processing technologies have led to the development of Visualisation-oriented Natural Language Interfaces (V-NLIs). In this paper, we carry out a scoping review that analyses synergies between the fields of Data Visualisation and Natural Language Interaction. Specifically, we focus on chatbot-based V-NLI approaches and explore and discuss three research questions. The first two research questions focus on studying how chatbot-based V-NLIs contribute to interactions with the Data and Visual Spaces of the visualisation pipeline, while the third seeks to know how chatbot-based V-NLIs enhance users’ interaction with visualisations. Our findings show that the works in the literature put a strong focus on exploring tabular data with basic visualisations, with visual mapping primarily reliant on fixed layouts. Moreover, V-NLIs provide users with restricted guidance strategies, and few of them support high-level and follow-up queries. We identify challenges and possible research opportunities for the V-NLI community such as supporting high-level queries with complex data, integrating V-NLIs with more advanced systems such as Augmented Reality (AR) or Virtual Reality (VR), particularly for advanced visualisations, expanding guidance strategies beyond current limitations, adopting intelligent visual mapping techniques, and incorporating more sophisticated interaction methods.},
  issue        = {12},
  langid       = {english},
  keywords     = {chatbot,data visualisation,natural language interface,survey},
  file         = {C:\Users\jseo1005\Zotero\storage\MMMAM5EZ\Kavaz et al. - 2023 - Chatbot-Based Natural Language Interfaces for Data.pdf}
}

@inproceedings{kierasGeneralizedTransitionNetwork1983,
  title     = {A Generalized Transition Network Representation for Interactive Systems},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author    = {Kieras, David and Polson, Peter G.},
  date      = {1983-12-12},
  series    = {{{CHI}} '83},
  pages     = {103--106},
  publisher = {Association for Computing Machinery},
  location  = {New York, NY, USA},
  doi       = {10.1145/800045.801590},
  url       = {https://dl.acm.org/doi/10.1145/800045.801590},
  urldate   = {2023-09-07},
  abstract  = {A general method for describing the behavior of an interactive system is presented which is based on transition networks generalized enough to describe even very complex systems easily, as shown by an example description of a word processor. The key feature is the ability to easily describe hierarchies of modes or states of the system. The representation system is especially valuable as a design tool when used in a simulation of a proposed user interface. In order to characterize the interaction between a user and a system, an explicit and formal representation of the behavior of the system itself is needed. To be of value in the design of user interfaces, the representation should be independent of the actual implementation of the system, but also reflect the structural properties of the system's behavior, such as its hierarchical form, the possible modes, and the consistent patterns of interaction. At the same time, the representation must be easy to define and understand. This paper presents a representation notation with these properties.},
  isbn      = {978-0-89791-121-4},
  file      = {C:\Users\jseo1005\Zotero\storage\TGI29GKW\Kieras and Polson - 1983 - A generalized transition network representation for interactive systems.pdf}
}

@article{kimAccessibleVisualizationDesign2021,
  title        = {Accessible {{Visualization}}: {{Design Space}}, {{Opportunities}}, and {{Challenges}}},
  shorttitle   = {Accessible {{Visualization}}},
  author       = {Kim, N. W. and Joyner, S. C. and Riegelhuth, A. and Kim, Y.},
  date         = {2021-06},
  journaltitle = {Computer Graphics Forum},
  shortjournal = {Computer Graphics Forum},
  volume       = {40},
  number       = {3},
  pages        = {173--188},
  issn         = {0167-7055, 1467-8659},
  doi          = {10.1111/cgf.14298},
  url          = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.14298},
  urldate      = {2022-02-09},
  abstract     = {Visualizations are now widely used across disciplines to understand and communicate data. The benefit of visualizations lies in leveraging our natural visual perception. However, the sole dependency on vision can produce unintended discrimination against people with visual impairments. While the visualization field has seen enormous growth in recent years, supporting people with disabilities is much less explored. In this work, we examine approaches to support this marginalized user group, focusing on visual disabilities. We collected and analyzed papers published for the last 20 years on visualization accessibility. We mapped a design space for accessible visualization that includes seven dimensions: user group, literacy task, chart type, interaction, information granularity, sensory modality, assistive technology. We described the current knowledge gap in light of the latest advances in visualization and presented a preliminary accessibility model by synthesizing findings from existing research. Finally, we reflected on the dimensions and discussed opportunities and challenges for future research.},
  langid       = {english},
  file         = {C:\Users\jseo1005\Zotero\storage\E42LXLJL\Kim et al. - 2021 - Accessible Visualization Design Space, Opportunit.pdf}
}

@article{kimAccessibleVisualizationDesign2021a,
  title        = {Accessible {{Visualization}}: {{Design Space}}, {{Opportunities}}, and {{Challenges}}},
  shorttitle   = {Accessible {{Visualization}}},
  author       = {Kim, N. W. and Joyner, S. C. and Riegelhuth, A. and Kim, Y.},
  date         = {2021},
  journaltitle = {Computer Graphics Forum},
  volume       = {40},
  number       = {3},
  pages        = {173--188},
  issn         = {1467-8659},
  doi          = {10.1111/cgf.14298},
  url          = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14298},
  urldate      = {2023-11-14},
  abstract     = {Visualizations are now widely used across disciplines to understand and communicate data. The benefit of visualizations lies in leveraging our natural visual perception. However, the sole dependency on vision can produce unintended discrimination against people with visual impairments. While the visualization field has seen enormous growth in recent years, supporting people with disabilities is much less explored. In this work, we examine approaches to support this marginalized user group, focusing on visual disabilities. We collected and analyzed papers published for the last 20 years on visualization accessibility. We mapped a design space for accessible visualization that includes seven dimensions: user group, literacy task, chart type, interaction, information granularity, sensory modality, assistive technology. We described the current knowledge gap in light of the latest advances in visualization and presented a preliminary accessibility model by synthesizing findings from existing research. Finally, we reflected on the dimensions and discussed opportunities and challenges for future research.},
  langid       = {english},
  keywords     = {• Human-centered computing → Visualization,Accessibility,CCS Concepts},
  file         = {C:\Users\jseo1005\Zotero\storage\MALSXL65\Kim et al. - 2021 - Accessible Visualization Design Space, Opportunities, and Challenges.pdf}
}

@inproceedings{kimAnsweringQuestionsCharts2020,
  title      = {Answering {{Questions}} about {{Charts}} and {{Generating Visual Explanations}}},
  booktitle  = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author     = {Kim, Dae Hyun and Hoque, Enamul and Agrawala, Maneesh},
  date       = {2020-04-21},
  pages      = {1--13},
  publisher  = {ACM},
  location   = {Honolulu HI USA},
  doi        = {10.1145/3313831.3376467},
  url        = {https://dl.acm.org/doi/10.1145/3313831.3376467},
  urldate    = {2022-08-21},
  eventtitle = {{{CHI}} '20: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn       = {978-1-4503-6708-0},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\PB8PFLZX\Kim et al. - 2020 - Answering Questions about Charts and Generating Vi.pdf}
}

@inproceedings{kimAnsweringQuestionsCharts2020a,
  title      = {Answering {{Questions}} about {{Charts}} and {{Generating Visual Explanations}}},
  booktitle  = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author     = {Kim, Dae Hyun and Hoque, Enamul and Agrawala, Maneesh},
  date       = {2020-04-21},
  pages      = {1--13},
  publisher  = {ACM},
  location   = {Honolulu HI USA},
  doi        = {10.1145/3313831.3376467},
  url        = {https://dl.acm.org/doi/10.1145/3313831.3376467},
  urldate    = {2023-12-11},
  eventtitle = {{{CHI}} '20: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn       = {978-1-4503-6708-0},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\A8Z6UXG8\Kim et al. - 2020 - Answering Questions about Charts and Generating Visual Explanations.pdf}
}

@inproceedings{kimAnsweringQuestionsCharts2020b,
  title      = {Answering {{Questions}} about {{Charts}} and {{Generating Visual Explanations}}},
  booktitle  = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author     = {Kim, Dae Hyun and Hoque, Enamul and Agrawala, Maneesh},
  date       = {2020-04-21},
  pages      = {1--13},
  publisher  = {ACM},
  location   = {Honolulu HI USA},
  doi        = {10.1145/3313831.3376467},
  url        = {https://dl.acm.org/doi/10.1145/3313831.3376467},
  urldate    = {2024-03-25},
  eventtitle = {{{CHI}} '20: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn       = {978-1-4503-6708-0},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\ER5WHFZZ\Kim et al. - 2020 - Answering Questions about Charts and Generating Vi.pdf}
}

@online{kimErieDeclarativeGrammar2024,
  title       = {Erie: {{A Declarative Grammar}} for {{Data Sonification}}},
  shorttitle  = {Erie},
  author      = {Kim, Hyeok and Kim, Yea-Seul and Hullman, Jessica},
  date        = {2024-02-08},
  eprint      = {2402.00156},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  doi         = {10.1145/3613904.3642442},
  url         = {http://arxiv.org/abs/2402.00156},
  urldate     = {2024-03-25},
  abstract    = {Data sonification-mapping data variables to auditory variables, such as pitch or volume-is used for data accessibility, scientific exploration, and data-driven art (e.g., museum exhibitions) among others. While a substantial amount of research has been made on effective and intuitive sonification design, software support is not commensurate, limiting researchers from fully exploring its capabilities. We contribute Erie, a declarative grammar for data sonification, that enables abstractly expressing auditory mappings. Erie supports specifying extensible tone designs (e.g., periodic wave, sampling, frequency/amplitude modulation synthesizers), various encoding channels, auditory legends, and composition options like sequencing and overlaying. Using standard Web Audio and Web Speech APIs, we provide an Erie compiler for web environments. We demonstrate the expressiveness and feasibility of Erie by replicating research prototypes presented by prior work and provide a sonification design gallery. We discuss future steps to extend Erie toward other audio computing environments and support interactive data sonification.},
  pubstate    = {preprint},
  keywords    = {Computer Science - Human-Computer Interaction},
  file        = {C\:\\Users\\jseo1005\\Zotero\\storage\\C7HKZDME\\Kim et al. - 2024 - Erie A Declarative Grammar for Data Sonification.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\GTG8ES4C\\2402.html}
}

@inproceedings{kimExploringChartQuestion2023,
  title     = {Exploring {{Chart Question Answering}} for {{Blind}} and {{Low Vision Users}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author    = {Kim, Jiho and Srinivasan, Arjun and Kim, Nam Wook and Kim, Yea-Seul},
  date      = {2023-04-19},
  series    = {{{CHI}} '23},
  pages     = {1--15},
  publisher = {Association for Computing Machinery},
  location  = {New York, NY, USA},
  doi       = {10.1145/3544548.3581532},
  url       = {https://dl.acm.org/doi/10.1145/3544548.3581532},
  urldate   = {2023-05-05},
  abstract  = {Data visualizations can be complex or involve numerous data points, making them impractical to navigate using screen readers alone. Question answering (QA) systems have the potential to support visualization interpretation and exploration without overwhelming blind and low vision (BLV) users. To investigate if and how QA systems can help BLV users in working with visualizations, we conducted a Wizard of Oz study with 24 BLV people where participants freely posed queries about four visualizations. We collected 979 queries and mapped them to popular analytic task taxonomies. We found that retrieving value and finding extremum were the most common tasks, participants often made complex queries and used visual references, and the data topic notably influenced the queries. We compile a list of design considerations for accessible chart QA systems and make our question corpus publicly available to guide future research and development.},
  isbn      = {978-1-4503-9421-5},
  keywords  = {Accessibility,Design Considerations,Human-Subjects Qualitative Studies,Question Answering,Visualization},
  file      = {C:\Users\jseo1005\Zotero\storage\XWHZ35QQ\Kim et al. - 2023 - Exploring Chart Question Answering for Blind and L.pdf}
}

@inproceedings{kimExploringChartQuestion2023a,
  title      = {Exploring {{Chart Question Answering}} for {{Blind}} and {{Low Vision Users}}},
  booktitle  = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author     = {Kim, Jiho and Srinivasan, Arjun and Kim, Nam Wook and Kim, Yea-Seul},
  date       = {2023-04-19},
  pages      = {1--15},
  publisher  = {ACM},
  location   = {Hamburg Germany},
  doi        = {10.1145/3544548.3581532},
  url        = {https://dl.acm.org/doi/10.1145/3544548.3581532},
  urldate    = {2023-12-11},
  eventtitle = {{{CHI}} '23: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn       = {978-1-4503-9421-5},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\QBDRYS75\Kim et al. - 2023 - Exploring Chart Question Answering for Blind and Low Vision Users.pdf}
}

@inproceedings{kimExploringChartQuestion2023b,
  title      = {Exploring {{Chart Question Answering}} for {{Blind}} and {{Low Vision Users}}},
  booktitle  = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author     = {Kim, Jiho and Srinivasan, Arjun and Kim, Nam Wook and Kim, Yea-Seul},
  date       = {2023-04-19},
  pages      = {1--15},
  publisher  = {ACM},
  location   = {Hamburg Germany},
  doi        = {10.1145/3544548.3581532},
  url        = {https://dl.acm.org/doi/10.1145/3544548.3581532},
  urldate    = {2024-02-08},
  eventtitle = {{{CHI}} '23: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn       = {978-1-4503-9421-5},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\HL7TDVPA\Kim et al. - 2023 - Exploring Chart Question Answering for Blind and L.pdf}
}

@inproceedings{kimHandscopeEnablingBlind2011,
  title      = {Handscope: Enabling Blind People to Experience Statistical Graphics on Websites through Haptics},
  shorttitle = {Handscope},
  booktitle  = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author     = {Kim, Da-jung and Lim, Youn-kyung},
  date       = {2011-05-07},
  series     = {{{CHI}} '11},
  pages      = {2039--2042},
  publisher  = {Association for Computing Machinery},
  location   = {New York, NY, USA},
  doi        = {10.1145/1978942.1979237},
  url        = {https://dl.acm.org/doi/10.1145/1978942.1979237},
  urldate    = {2023-11-13},
  abstract   = {Statistical graphics on the web such as a tag cloud visually represent statistical data which are generated by website users. While sighted people can scan the latest information through the dynamic changes of statistical graphics, blind people, who cannot perceive them, lose opportunities to keep up to date in this quickly-changing society. In order to enable blind people to experience socially-generated statistical graphics, we propose a new assistive device, namely, Handscope, which translates statistical graphics on websites into simple height changes of its haptic pole. We conducted a two-phase user study with blind people in order to test its usability and explore its effects on the quality of blind users' web experiences. The results show the meaningful contribution of Handscope in extending the area of blind people's web experiences.},
  isbn       = {978-1-4503-0228-9},
  keywords   = {blind users,haptic,statistical graphics,web accessibility},
  file       = {C:\Users\jseo1005\Zotero\storage\YJ4UGMIT\Kim and Lim - 2011 - Handscope enabling blind people to experience statistical graphics on websites through haptics.pdf}
}

@online{koNaturalLanguageDataset2024,
  title       = {Natural {{Language Dataset Generation Framework}} for {{Visualizations Powered}} by {{Large Language Models}}},
  author      = {Ko, Hyung-Kwon and Jeon, Hyeon and Park, Gwanmo and Kim, Dae Hyun and Kim, Nam Wook and Kim, Juho and Seo, Jinwook},
  date        = {2024-01-21},
  eprint      = {2309.10245},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  doi         = {10.1145/3613904.3642943},
  url         = {http://arxiv.org/abs/2309.10245},
  urldate     = {2024-03-25},
  abstract    = {We introduce VL2NL, a Large Language Model (LLM) framework that generates rich and diverse NL datasets using only Vega-Lite specifications as input, thereby streamlining the development of Natural Language Interfaces (NLIs) for data visualization. To synthesize relevant chart semantics accurately and enhance syntactic diversity in each NL dataset, we leverage 1) a guided discovery incorporated into prompting so that LLMs can steer themselves to create faithful NL datasets in a self-directed manner; 2) a score-based paraphrasing to augment NL syntax along with four language axes. We also present a new collection of 1,981 real-world Vega-Lite specifications that have increased diversity and complexity than existing chart collections. When tested on our chart collection, VL2NL extracted chart semantics and generated L1/L2 captions with 89.4\% and 76.0\% accuracy, respectively. It also demonstrated generating and paraphrasing utterances and questions with greater diversity compared to the benchmarks. Last, we discuss how our NL datasets and framework can be utilized in real-world scenarios. The codes and chart collection are available at https://github.com/hyungkwonko/chart-llm.},
  pubstate    = {preprint},
  keywords    = {Computer Science - Human-Computer Interaction},
  file        = {C\:\\Users\\jseo1005\\Zotero\\storage\\L9B5NL6Y\\Ko et al. - 2024 - Natural Language Dataset Generation Framework for Visualizations Powered by Large Language Models.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\BVUEW8FJ\\2309.html}
}

@online{koNaturalLanguageDataset2024a,
  title       = {Natural {{Language Dataset Generation Framework}} for {{Visualizations Powered}} by {{Large Language Models}}},
  author      = {Ko, Hyung-Kwon and Jeon, Hyeon and Park, Gwanmo and Kim, Dae Hyun and Kim, Nam Wook and Kim, Juho and Seo, Jinwook},
  date        = {2024-01-21},
  eprint      = {2309.10245},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  doi         = {10.1145/3613904.3642943},
  url         = {http://arxiv.org/abs/2309.10245},
  urldate     = {2024-03-25},
  abstract    = {We introduce VL2NL, a Large Language Model (LLM) framework that generates rich and diverse NL datasets using only Vega-Lite specifications as input, thereby streamlining the development of Natural Language Interfaces (NLIs) for data visualization. To synthesize relevant chart semantics accurately and enhance syntactic diversity in each NL dataset, we leverage 1) a guided discovery incorporated into prompting so that LLMs can steer themselves to create faithful NL datasets in a self-directed manner; 2) a score-based paraphrasing to augment NL syntax along with four language axes. We also present a new collection of 1,981 real-world Vega-Lite specifications that have increased diversity and complexity than existing chart collections. When tested on our chart collection, VL2NL extracted chart semantics and generated L1/L2 captions with 89.4\% and 76.0\% accuracy, respectively. It also demonstrated generating and paraphrasing utterances and questions with greater diversity compared to the benchmarks. Last, we discuss how our NL datasets and framework can be utilized in real-world scenarios. The codes and chart collection are available at https://github.com/hyungkwonko/chart-llm.},
  pubstate    = {preprint},
  keywords    = {Computer Science - Human-Computer Interaction},
  file        = {C\:\\Users\\jseo1005\\Zotero\\storage\\L8YJSQYB\\Ko et al. - 2024 - Natural Language Dataset Generation Framework for .pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\XAMTQJ4T\\2309.html}
}

@book{kressMultimodalitySocialSemiotic2010,
  title      = {Multimodality: {{A Social Semiotic Approach}} to {{Contemporary Communication}}},
  shorttitle = {Multimodality},
  author     = {Kress, Gunther R.},
  date       = {2010},
  eprint     = {ihTm_cI58JQC},
  eprinttype = {googlebooks},
  publisher  = {Taylor \& Francis},
  abstract   = {The 21st century is awash with ever more mixed and remixed images, writing, layout, sound, gesture, speech, and 3D objects. Multimodality looks beyond language and examines these multiple modes of communication and meaning making.   Multimodality: A Social Semiotic Approach to Contemporary Communication represents a long-awaited and much anticipated addition to the study of multimodality from the scholar who pioneered and continues to play a decisive role in shaping the field. Written in an accessible manner and illustrated with a wealth of photos and illustrations to clearly demonstrate the points made, Multimodality: A Social Semiotic Approach to Contemporary Communication deliberately sets out to locate communication in the everyday, covering topics and issues not usually discussed in books of this kind, from traffic signs to mobile phones.   In this book, Gunther Kress presents a contemporary, distinctive and widely applicable approach to communication. He provides the framework necessary for understanding the attempt to bring all modes of meaning-making together under one unified theoretical roof.  This exploration of an increasingly vital area of language and communication studies will be of interest to advanced undergraduate and postgraduate students in the fields of English language and applied linguistics, media and communication studies and education.},
  isbn       = {978-0-415-32060-3},
  langid     = {english},
  pagetotal  = {234},
  keywords   = {Language Arts & Disciplines / Communication Studies}
}

@article{krossDemocratizationDataScience2020,
  title        = {The {{Democratization}} of {{Data Science Education}}},
  author       = {Kross, Sean and Peng, Roger D. and Caffo, Brian S. and Gooding, Ira and Leek, Jeffrey T.},
  date         = {2020-02},
  journaltitle = {American Statistician},
  volume       = {74},
  number       = {1},
  pages        = {1--7},
  publisher    = {Taylor \& Francis Ltd},
  issn         = {00031305},
  doi          = {10.1080/00031305.2019.1668849},
  url          = {https://proxy2.library.illinois.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=asn&AN=141377224&site=eds-live&scope=site},
  urldate      = {2024-02-06},
  abstract     = {Over the last three decades, data have become ubiquitous and cheap. This transition has accelerated over the last five years and training in statistics, machine learning, and data analysis has struggled to keep up. In April 2014, we launched a program of nine courses, the Johns Hopkins Data Science Specialization, which has now had more than 4 million enrollments over the past five years. Here, the program is described and compared to standard data science curricula as they were organized in 2014 and 2015. We show that novel pedagogical and administrative decisions introduced in our program are now standard in online data science programs. The impact of the Data Science Specialization on data science education in the U.S. is also discussed. Finally, we conclude with some thoughts about the future of data science education in a data democratized world.},
  keywords     = {Applications and case studies,DATA science,DEMOCRATIZATION,Education,JOHNS Hopkins University,METADATA,SCIENCE education,Statistical computing},
  file         = {C:\Users\jseo1005\Zotero\storage\F5VDBUH7\Kross et al. - 2020 - The Democratization of Data Science Education.pdf}
}

@article{ladnerDesignUserEmpowerment2015,
  title        = {Design for User Empowerment},
  author       = {Ladner, Richard E.},
  date         = {2015-02-25},
  journaltitle = {Interactions},
  shortjournal = {interactions},
  volume       = {22},
  number       = {2},
  pages        = {24--29},
  issn         = {1072-5520},
  doi          = {10.1145/2723869},
  url          = {https://dl.acm.org/doi/10.1145/2723869},
  urldate      = {2023-11-27},
  file         = {C:\Users\jseo1005\Zotero\storage\IGF3L2R7\Ladner - 2015 - Design for user empowerment.pdf}
}

@inproceedings{leeCollabAllyAccessibleCollaboration2022,
  title      = {{{CollabAlly}}: {{Accessible Collaboration Awareness}} in {{Document Editing}}},
  shorttitle = {{{CollabAlly}}},
  booktitle  = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author     = {Lee, Cheuk Yin Phipson and Zhang, Zhuohao and Herskovitz, Jaylin and Seo, JooYoung and Guo, Anhong},
  date       = {2022-04-29},
  pages      = {1--17},
  publisher  = {ACM},
  location   = {New Orleans LA USA},
  doi        = {10.1145/3491102.3517635},
  url        = {https://dl.acm.org/doi/10.1145/3491102.3517635},
  urldate    = {2023-12-13},
  abstract   = {Collaborative document editing tools are widely used in professional and academic workplaces. While these tools provide basic accessibility support, it is challenging for blind users to gain collaboration awareness that sighted people can easily obtain using visual cues (e.g., who is editing where and what). Through a series of co-design sessions with a blind coauthor, we identifed the current practices and challenges in collaborative editing, and iteratively designed CollabAlly, a system that makes collaboration awareness in document editing accessible to blind users. CollabAlly extracts collaborator, comment, and text-change information and their context from a document and presents them in a dialog box to provide easy access and navigation. CollabAlly uses earcons to communicate background events unobtrusively, voice fonts to diferentiate collaborators, and spatial audio to convey the location of document activity. In a study with 11 blind participants, we demonstrate that CollabAlly provides improved access to collaboration awareness by centralizing scattered information, sonifying visual information, and simplifying complex operations.},
  eventtitle = {{{CHI}} '22: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn       = {978-1-4503-9157-3},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\GDMXHKZF\CHI22-CollabAlly.pdf}
}

@article{leeDataUseMiddle2018,
  title        = {Data {{Use}} by {{Middle}} and {{Secondary Students}} in the {{Digital Age}}: {{A Status Report}} and {{Future Prospects}}},
  shorttitle   = {Data {{Use}} by {{Middle}} and {{Secondary Students}} in the {{Digital Age}}},
  author       = {Lee, Victor and Wilkerson, Michelle},
  date         = {2018-01-01},
  journaltitle = {Instructional Technology and Learning Sciences Faculty Publications},
  pages        = {1--43},
  url          = {https://digitalcommons.usu.edu/itls_facpub/634},
  file         = {C:\Users\jseo1005\Zotero\storage\PR22UQB9\634.html}
}

@article{leeHowPeopleMake2016,
  title        = {How Do {{People Make Sense}} of {{Unfamiliar Visualizations}}?: {{A Grounded Model}} of {{Novice}}'s {{Information Visualization Sensemaking}}},
  shorttitle   = {How Do {{People Make Sense}} of {{Unfamiliar Visualizations}}?},
  author       = {Lee, Sukwon and Kim, Sung-Hee and Hung, Ya-Hsin and Lam, Heidi and Kang, Youn-Ah and Yi, Ji Soo},
  date         = {2016-01},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  volume       = {22},
  number       = {1},
  pages        = {499--508},
  issn         = {1941-0506},
  doi          = {10.1109/TVCG.2015.2467195},
  abstract     = {In this paper, we would like to investigate how people make sense of unfamiliar information visualizations. In order to achieve the research goal, we conducted a qualitative study by observing 13 participants when they endeavored to make sense of three unfamiliar visualizations (i.e., a parallel-coordinates plot, a chord diagram, and a treemap) that they encountered for the first time. We collected data including audio/video record of think-aloud sessions and semi-structured interview; and analyzed the data using the grounded theory method. The primary result of this study is a grounded model of NOvice's information VIsualization Sensemaking (NOVIS model), which consists of the five major cognitive activities: 1 encountering visualization, 2 constructing a frame, 3 exploring visualization, 4 questioning the frame, and 5 floundering on visualization. We introduce the NOVIS model by explaining the five activities with representative quotes from our participants. We also explore the dynamics in the model. Lastly, we compare with other existing models and share further research directions that arose from our observations.},
  eventtitle   = {{{IEEE Transactions}} on {{Visualization}} and {{Computer Graphics}}},
  keywords     = {Data visualization,Encoding,grounded theory,Hidden Markov models,Image color analysis,information visualization,Interviews,novice users,qualitative study,Sensemaking model,Vehicles,Visualization},
  file         = {C:\Users\jseo1005\Zotero\storage\5U3YM9W5\Lee et al. - 2016 - How do People Make Sense of Unfamiliar Visualizati.pdf}
}

@article{leeReachingBroaderAudiences2020,
  title        = {Reaching {{Broader Audiences With Data Visualization}}},
  author       = {Lee, Bongshin and Choe, Eun Kyoung and Isenberg, Petra and Marriott, Kim and Stasko, John},
  date         = {2020-03-01},
  journaltitle = {IEEE Computer Graphics and Applications},
  shortjournal = {IEEE Comput. Grap. Appl.},
  volume       = {40},
  number       = {2},
  pages        = {82--90},
  issn         = {0272-1716, 1558-1756},
  doi          = {10.1109/MCG.2020.2968244},
  url          = {https://ieeexplore.ieee.org/document/9023497/},
  urldate      = {2023-01-12},
  abstract     = {The visualization research community can and should reach broader audiences beyond data-savvy groups of people, because these audiences could also greatly benefit from visual access to data. In this paper, we discuss four research topics—personal data visualization, data visualization on mobile devices, inclusive data visualization, and multimodal interaction for data visualization—that, individually and collaboratively, would help us reach broader audiences with data visualization, making data more accessible.},
  langid       = {english},
  file         = {C:\Users\jseo1005\Zotero\storage\M7C3NLRW\Lee et al. - 2020 - Reaching Broader Audiences With Data Visualization.pdf}
}

@inproceedings{liuCrossA11yIdentifyingVideo2022,
  title       = {{{CrossA11y}}: {{Identifying Video Accessibility Issues}} via {{Cross-modal Grounding}}},
  shorttitle  = {{{CrossA11y}}},
  booktitle   = {The 35th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author      = {Liu, Xingyu "Bruce" and Wang, Ruolin and Li, Dingzeyu and Chen, Xiang 'Anthony' and Pavel, Amy},
  date        = {2022-10-29},
  eprint      = {2208.11144},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  pages       = {1--14},
  doi         = {10.1145/3526113.3545703},
  url         = {http://arxiv.org/abs/2208.11144},
  urldate     = {2023-01-10},
  abstract    = {Authors make their videos visually accessible by adding audio descriptions (AD), and auditorily accessible by adding closed captions (CC). However, creating AD and CC is challenging and tedious, especially for non-professional describers and captioners, due to the difficulty of identifying accessibility problems in videos. A video author will have to watch the video through and manually check for inaccessible information frame-by-frame, for both visual and auditory modalities. In this paper, we present CrossA11y, a system that helps authors efficiently detect and address visual and auditory accessibility issues in videos. Using cross-modal grounding analysis, CrossA11y automatically measures accessibility of visual and audio segments in a video by checking for modality asymmetries. CrossA11y then displays these segments and surfaces visual and audio accessibility issues in a unified interface, making it intuitive to locate, review, script AD/CC in-place, and preview the described and captioned video immediately. We demonstrate the effectiveness of CrossA11y through a lab study with 11 participants, comparing to existing baseline.},
  keywords    = {Computer Science - Human-Computer Interaction},
  file        = {C\:\\Users\\jseo1005\\Zotero\\storage\\XF5LI8D7\\Liu et al. - 2022 - CrossA11y Identifying Video Accessibility Issues .pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\9V9UW4AL\\2208.html}
}

@online{liuImprovedBaselinesVisual2023,
  title       = {Improved {{Baselines}} with {{Visual Instruction Tuning}}},
  author      = {Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  date        = {2023-10-05},
  eprint      = {2310.03744},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2310.03744},
  url         = {http://arxiv.org/abs/2310.03744},
  urldate     = {2024-04-20},
  abstract    = {Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in \textasciitilde 1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available.},
  pubstate    = {preprint},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file        = {C:\Users\jseo1005\Zotero\storage\H8LENWA5\2310.html}
}

@article{luLargeDisplacementRelaxor2015,
  title        = {Large {{Displacement}} in {{Relaxor Ferroelectric Terpolymer Blend Derived Actuators Using Al Electrode}} for {{Braille Displays}}},
  author       = {Lu, S. G. and Chen, X. and Levard, T. and Diglio, P. J. and Gorny, L. J. and Rahn, C. D. and Zhang, Q. M.},
  date         = {2015-06-16},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume       = {5},
  number       = {1},
  pages        = {11361},
  publisher    = {Nature Publishing Group},
  issn         = {2045-2322},
  doi          = {10.1038/srep11361},
  url          = {https://www.nature.com/articles/srep11361},
  urldate      = {2023-12-10},
  abstract     = {Poly(vinylidene fluoride) (PVDF) based polymers are attractive for applications for artificial muscles, high energy density storage devices etc. Recently these polymers have been found great potential for being used as actuators for refreshable full-page Braille displays for visually impaired people in terms of light weight, miniaturized size and larger displacement, compared with currently used lead zirconate titanate ceramic actuators. The applied voltages of published polymer actuators, however, cannot be reduced to meet the requirements of using city power. Here, we report the polymer actuator generating quite large displacement and blocking force at a voltage close to the city power. Our embodiments also show good self-healing performance and disuse of lead-containing material, which makes the Braille device safer, more reliable and more environment-friendly.},
  issue        = {1},
  langid       = {english},
  keywords     = {Actuators,Electronic devices},
  file         = {C:\Users\jseo1005\Zotero\storage\6NC8FG2W\Lu et al. - 2015 - Large Displacement in Relaxor Ferroelectric Terpol.pdf}
}

@article{lundgardAccessibleVisualizationNatural2022,
  title        = {Accessible {{Visualization}} via {{Natural Language Descriptions}}: {{A Four-Level Model}} of {{Semantic Content}}},
  shorttitle   = {Accessible {{Visualization}} via {{Natural Language Descriptions}}},
  author       = {Lundgard, Alan and Satyanarayan, Arvind},
  date         = {2022-01},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  shortjournal = {IEEE Trans. Visual. Comput. Graphics},
  volume       = {28},
  number       = {1},
  pages        = {1073--1083},
  issn         = {1077-2626, 1941-0506, 2160-9306},
  doi          = {10.1109/TVCG.2021.3114770},
  url          = {https://ieeexplore.ieee.org/document/9555469/},
  urldate      = {2022-07-28},
  abstract     = {Natural language descriptions sometimes accompany visualizations to better communicate and contextualize their insights, and to improve their accessibility for readers with disabilities. However, it is difficult to evaluate the usefulness of these descriptions, and how effectively they improve access to meaningful information, because we have little understanding of the semantic content they convey, and how different readers receive this content. In response, we introduce a conceptual model for the semantic content conveyed by natural language descriptions of visualizations. Developed through a grounded theory analysis of 2,147 sentences, our model spans four levels of semantic content: enumerating visualization construction properties (e.g., marks and encodings); reporting statistical concepts and relations (e.g., extrema and correlations); identifying perceptual and cognitive phenomena (e.g., complex trends and patterns); and elucidating domain-specific insights (e.g., social and political context). To demonstrate how our model can be applied to evaluate the effectiveness of visualization descriptions, we conduct a mixed-methods evaluation with 30 blind and 90 sighted readers, and find that these reader groups differ significantly on which semantic content they rank as most useful. Together, our model and findings suggest that access to meaningful information is strongly reader-specific, and that research in automatic visualization captioning should orient toward descriptions that more richly communicate overall trends and statistics, sensitive to reader preferences. Our work further opens a space of research on natural language as a data interface coequal with visualization. Index Terms—Visualization, natural language, description, caption, semantic, model, theory, alt text, blind, disability, accessibility.},
  langid       = {english},
  file         = {C:\Users\jseo1005\Zotero\storage\QTJXT9NN\Lundgard and Satyanarayan - 2022 - Accessible Visualization via Natural Language Desc.pdf}
}

@inproceedings{mackWhatWeMean2021,
  title      = {What {{Do We Mean}} by “{{Accessibility Research}}”?: {{A Literature Survey}} of {{Accessibility Papers}} in {{CHI}} and {{ASSETS}} from 1994 to 2019},
  shorttitle = {What {{Do We Mean}} by “{{Accessibility Research}}”?},
  booktitle  = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author     = {Mack, Kelly and McDonnell, Emma and Jain, Dhruv and Lu Wang, Lucy and E. Froehlich, Jon and Findlater, Leah},
  date       = {2021-05-06},
  pages      = {1--18},
  publisher  = {ACM},
  location   = {Yokohama Japan},
  doi        = {10.1145/3411764.3445412},
  url        = {https://dl.acm.org/doi/10.1145/3411764.3445412},
  urldate    = {2023-05-17},
  eventtitle = {{{CHI}} '21: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn       = {978-1-4503-8096-6},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\6CJFVKZF\Mack et al. - 2021 - What Do We Mean by “Accessibility Research” A Li.pdf}
}

@article{marriottInclusiveDataVisualization2021,
  title        = {Inclusive Data Visualization for People with Disabilities: A Call to Action},
  shorttitle   = {Inclusive Data Visualization for People with Disabilities},
  author       = {Marriott, Kim and Lee, Bongshin and Butler, Matthew and Cutrell, Ed and Ellis, Kirsten and Goncu, Cagatay and Hearst, Marti and McCoy, Kathleen and Szafir, Danielle Albers},
  date         = {2021-04-27},
  journaltitle = {Interactions},
  shortjournal = {interactions},
  volume       = {28},
  number       = {3},
  pages        = {47--51},
  issn         = {1072-5520},
  doi          = {10.1145/3457875},
  url          = {https://doi.org/10.1145/3457875},
  urldate      = {2022-02-09},
  file         = {C:\Users\jseo1005\Zotero\storage\GKVYCZ3W\Marriott et al. - 2021 - Inclusive data visualization for people with disab.pdf}
}

@article{marsonTeachingIntroductoryStatistics2013,
  title        = {Teaching Introductory Statistics to Blind Students},
  author       = {Marson, Stephen M. and Harrington, Charles F. and Walls, Adam},
  date         = {2013-21},
  journaltitle = {Teaching Statistics},
  volume       = {35},
  number       = {1},
  pages        = {21--25},
  publisher    = {Wiley-Blackwell},
  issn         = {0141982X},
  doi          = {10.1111/j.1467-9639.2012.00510.x},
  url          = {https://proxy2.library.illinois.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=asn&AN=85017778&site=eds-live&scope=site},
  urldate      = {2024-02-06},
  abstract     = {The challenges of learning statistics, particularly distributions and their characteristics, can be potentially monumental for vision impaired and blind students. The authors provide some practical advice for teaching these students.},
  keywords     = {BLIND students,COLLABORATIVE learning,COUNTERTRANSFERENCE (Psychology),Kinesthetic explanations,KINESTHETIC method (Education),Nemeth Code,STUDENT participation,Teaching,Test autopsy},
  file         = {C:\Users\jseo1005\Zotero\storage\GDI9SLYB\Marson et al. - 2013 - Teaching introductory statistics to blind students.pdf}
}

@online{masryChartInstructInstructionTuning2024,
  title       = {{{ChartInstruct}}: {{Instruction Tuning}} for {{Chart Comprehension}} and {{Reasoning}}},
  shorttitle  = {{{ChartInstruct}}},
  author      = {Masry, Ahmed and Shahmohammadi, Mehrad and Parvez, Md Rizwan and Hoque, Enamul and Joty, Shafiq},
  date        = {2024-03-13},
  eprint      = {2403.09028},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2403.09028},
  url         = {http://arxiv.org/abs/2403.09028},
  urldate     = {2024-03-25},
  abstract    = {Charts provide visual representations of data and are widely used for analyzing information, addressing queries, and conveying insights to others. Various chart-related downstream tasks have emerged recently, such as question-answering and summarization. A common strategy to solve these tasks is to fine-tune various models originally trained on vision tasks language. However, such task-specific models are not capable of solving a wide range of chart-related tasks, constraining their real-world applicability. To overcome these challenges, we introduce ChartInstruct: a novel chart-specific vision-language Instruction-following dataset comprising 191K instructions generated with 71K charts. We then present two distinct systems for instruction tuning on such datasets: (1) an end-to-end model that connects a vision encoder for chart understanding with a LLM; and (2) a pipeline model that employs a two-step approach to extract chart data tables and input them into the LLM. In experiments on four downstream tasks, we first show the effectiveness of our model--achieving a new set of state-of-the-art results. Further evaluation shows that our instruction-tuning approach supports a wide array of real-world chart comprehension and reasoning scenarios, thereby expanding the scope and applicability of our models to new kinds of tasks.},
  pubstate    = {preprint},
  keywords    = {Computer Science - Computation and Language},
  file        = {C\:\\Users\\jseo1005\\Zotero\\storage\\CVCL5XGL\\Masry et al. - 2024 - ChartInstruct Instruction Tuning for Chart Compre.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\HMEVG4NX\\2403.html}
}

@online{masryUniChartUniversalVisionlanguage2023,
  title       = {{{UniChart}}: {{A Universal Vision-language Pretrained Model}} for {{Chart Comprehension}} and {{Reasoning}}},
  shorttitle  = {{{UniChart}}},
  author      = {Masry, Ahmed and Kavehzadeh, Parsa and Do, Xuan Long and Hoque, Enamul and Joty, Shafiq},
  date        = {2023-10-10},
  eprint      = {2305.14761},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2305.14761},
  url         = {http://arxiv.org/abs/2305.14761},
  urldate     = {2024-03-25},
  abstract    = {Charts are very popular for analyzing data, visualizing key insights and answering complex reasoning questions about data. To facilitate chart-based data analysis using natural language, several downstream tasks have been introduced recently such as chart question answering and chart summarization. However, most of the methods that solve these tasks use pretraining on language or vision-language tasks that do not attempt to explicitly model the structure of the charts (e.g., how data is visually encoded and how chart elements are related to each other). To address this, we first build a large corpus of charts covering a wide variety of topics and visual styles. We then present UniChart, a pretrained model for chart comprehension and reasoning. UniChart encodes the relevant text, data, and visual elements of charts and then uses a chart-grounded text decoder to generate the expected output in natural language. We propose several chart-specific pretraining tasks that include: (i) low-level tasks to extract the visual elements (e.g., bars, lines) and data from charts, and (ii) high-level tasks to acquire chart understanding and reasoning skills. We find that pretraining the model on a large corpus with chart-specific low- and high-level tasks followed by finetuning on three down-streaming tasks results in state-of-the-art performance on three downstream tasks.},
  pubstate    = {preprint},
  keywords    = {Computer Science - Computation and Language},
  file        = {C\:\\Users\\jseo1005\\Zotero\\storage\\ZGJU2XHY\\Masry et al. - 2023 - UniChart A Universal Vision-language Pretrained M.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\4B7KPLG7\\2305.html}
}

@incollection{mayerCognitiveTheoryMultimedia2014,
  title     = {Cognitive {{Theory}} of {{Multimedia Learning}}},
  booktitle = {The {{Cambridge Handbook}} of {{Multimedia Learning}}},
  author    = {Mayer, Richard E.},
  editor    = {Mayer, Richard E.},
  date      = {2014},
  series    = {Cambridge {{Handbooks}} in {{Psychology}}},
  edition   = {2},
  pages     = {43--71},
  publisher = {Cambridge University Press},
  location  = {Cambridge},
  doi       = {10.1017/CBO9781139547369.005},
  url       = {https://www.cambridge.org/core/books/cambridge-handbook-of-multimedia-learning/cognitive-theory-of-multimedia-learning/24E5AEDEC8F4137E37E15BD2BCA91326},
  urldate   = {2022-08-19},
  abstract  = {AbstractA fundamental hypothesis underlying research on multimedia learning is that multimedia instructional messages that are designed in light of how the human mind works are more likely to lead to meaningful learning than those that are not so designed. The cognitive theory of multimedia learning is based on three cognitive science principles of learning: the human information processing system includes dual channels for visual/pictorial and auditory/verbal processing (i.e., dual-channel assumption), each channel has a limited capacity for processing (i.e., limited-capacity assumption), and active learning entails carrying out a coordinated set of cognitive processes during learning (i.e., active processing assumption). The cognitive theory of multimedia learning specifies five cognitive processes in multimedia learning: selecting relevant words from the presented text or narration, selecting relevant images from the presented graphics, organizing the selected words into a coherent verbal representation, organizing selected images into a coherent pictorial representation, and integrating the pictorial and verbal representations and prior knowledge. Three demands on the learner’s cognitive capacity during learning are extraneous processing (which is not related to the instructional objective), essential processing (which is needed to mentally represent the essential material as presented), and generative processing (which is aimed at making sense of the material). Three instructional goals are to reduce extraneous processing (for extraneous overload situations), manage essential processing (for essential overload situations), and foster generative processing (for generative underuse situations). Multimedia instructional messages should be designed to guide appropriate cognitive processing during learning without overloading the learner’s cognitive system.},
  isbn      = {978-1-139-54736-9},
  file      = {C:\Users\jseo1005\Zotero\storage\RCS6J7M7\24E5AEDEC8F4137E37E15BD2BCA91326.html}
}

@article{mcdermottCocktailPartyProblem2009,
  title        = {The Cocktail Party Problem},
  author       = {McDermott, Josh H.},
  date         = {2009-12-01},
  journaltitle = {Current biology: CB},
  shortjournal = {Curr Biol},
  volume       = {19},
  number       = {22},
  eprint       = {19948136},
  eprinttype   = {pmid},
  pages        = {R1024-1027},
  issn         = {1879-0445},
  doi          = {10.1016/j.cub.2009.09.005},
  langid       = {english},
  keywords     = {Animal Communication,Animals,Humans,Noise,Sound},
  file         = {C:\Users\jseo1005\Zotero\storage\VRU3LMMP\McDermott - 2009 - The cocktail party problem.pdf}
}

@inproceedings{mcgookinSoundBarExploitingMultiple2006,
  title      = {{{SoundBar}}: Exploiting Multiple Views in Multimodal Graph Browsing},
  shorttitle = {{{SoundBar}}},
  booktitle  = {Proceedings of the 4th {{Nordic}} Conference on {{Human-computer}} Interaction Changing Roles - {{NordiCHI}} '06},
  author     = {McGookin, David K. and Brewster, Stephen A.},
  date       = {2006},
  pages      = {145--154},
  publisher  = {ACM Press},
  location   = {Oslo, Norway},
  doi        = {10.1145/1182475.1182491},
  url        = {http://portal.acm.org/citation.cfm?doid=1182475.1182491},
  urldate    = {2022-08-21},
  eventtitle = {The 4th {{Nordic}} Conference},
  isbn       = {978-1-59593-325-6},
  langid     = {english}
}

@article{medlockUsingRITEMethod2007,
  title    = {Using the {{RITE}} Method to Improve Products; a Definition and a Case Study},
  author   = {Medlock, Michael C. and Wixon, Dennis and Terrano, Mark and Romero, Ramon L.},
  date     = {2007-01-01},
  url      = {https://www.scinapse.io/papers/48202622},
  urldate  = {2023-03-29},
  abstract = {This paper defines and evaluates a method that some practitioners are using but has not been formally discusse},
  langid   = {english}
}

@online{metaIntroducingMetaLlama2024,
  title        = {Introducing {{Meta Llama}} 3: {{The}} Most Capable Openly Available {{LLM}} to Date},
  shorttitle   = {Introducing {{Meta Llama}} 3},
  author       = {{Meta}},
  date         = {2024},
  url          = {https://ai.meta.com/blog/meta-llama-3/},
  urldate      = {2024-04-20},
  abstract     = {Today, we’re introducing Meta Llama 3, the next generation of our state-of-the-art open source large language model. In the coming months, we expect to share new capabilities, additional model sizes, and more.},
  langid       = {english},
  organization = {Meta AI},
  file         = {C:\Users\jseo1005\Zotero\storage\HH9AM4RJ\meta-llama-3.html}
}

@inproceedings{mirriAccessibleGraphsHTMLbased2017,
  title      = {Towards Accessible Graphs in {{HTML-based}} Scientific Articles},
  booktitle  = {2017 14th {{IEEE Annual Consumer Communications}} \& {{Networking Conference}} ({{CCNC}})},
  author     = {Mirri, Silvia and Peroni, Silvio and Salomoni, Paola and Vitali, Fabio and Rubano, Vincenzo},
  date       = {2017-01},
  pages      = {1067--1072},
  publisher  = {IEEE},
  location   = {Las Vegas, NV},
  doi        = {10.1109/CCNC.2017.7983287},
  url        = {https://ieeexplore.ieee.org/document/7983287/},
  urldate    = {2022-08-21},
  eventtitle = {2017 14th {{IEEE Annual Consumer Communications}} \& {{Networking Conference}} ({{CCNC}})},
  isbn       = {978-1-5090-6196-9}
}

@article{morashGuidingNoviceWeb2015,
  title        = {Guiding {{Novice Web Workers}} in {{Making Image Descriptions Using Templates}}},
  author       = {Morash, Valerie S. and Siu, Yue-Ting and Miele, Joshua A. and Hasty, Lucia and Landau, Steven},
  date         = {2015-11-19},
  journaltitle = {ACM Transactions on Accessible Computing},
  shortjournal = {ACM Trans. Access. Comput.},
  volume       = {7},
  number       = {4},
  pages        = {12:1--12:21},
  issn         = {1936-7228},
  doi          = {10.1145/2764916},
  url          = {https://dl.acm.org/doi/10.1145/2764916},
  urldate      = {2023-09-10},
  abstract     = {This article compares two methods of employing novice Web workers to author descriptions of science, technology, engineering, and mathematics images to make them accessible to individuals with visual and print-reading disabilities. The goal is to identify methods of creating image descriptions that are inexpensive, effective, and follow established accessibility guidelines. The first method explicitly presented the guidelines to the worker, then the worker constructed the image description in an empty text box and table. The second method queried the worker for image information and then used responses to construct a template-based description according to established guidelines. The descriptions generated through queried image description (QID) were more likely to include information on the image category, title, caption, and units. They were also more similar to one another, based on Jaccard distances of q-grams, indicating that their word usage and structure were more standardized. Last, the workers preferred describing images using QID and found the task easier. Therefore, explicit instruction on image-description guidelines is not sufficient to produce quality image descriptions when using novice Web workers. Instead, it is better to provide information about images, then generate descriptions from responses using templates.},
  keywords     = {access technology,Accessibility (blind and visually impaired),crowdsourcing,human computation,image description},
  file         = {C:\Users\jseo1005\Zotero\storage\RH4YUIIR\Morash et al. - 2015 - Guiding Novice Web Workers in Making Image Descrip.pdf}
}

@article{morashGuidingNoviceWeb2015a,
  title        = {Guiding {{Novice Web Workers}} in {{Making Image Descriptions Using Templates}}},
  author       = {Morash, Valerie S. and Siu, Yue-Ting and Miele, Joshua A. and Hasty, Lucia and Landau, Steven},
  date         = {2015-11-19},
  journaltitle = {ACM Transactions on Accessible Computing},
  shortjournal = {ACM Trans. Access. Comput.},
  volume       = {7},
  number       = {4},
  pages        = {12:1--12:21},
  issn         = {1936-7228},
  doi          = {10.1145/2764916},
  url          = {https://dl.acm.org/doi/10.1145/2764916},
  urldate      = {2023-12-11},
  abstract     = {This article compares two methods of employing novice Web workers to author descriptions of science, technology, engineering, and mathematics images to make them accessible to individuals with visual and print-reading disabilities. The goal is to identify methods of creating image descriptions that are inexpensive, effective, and follow established accessibility guidelines. The first method explicitly presented the guidelines to the worker, then the worker constructed the image description in an empty text box and table. The second method queried the worker for image information and then used responses to construct a template-based description according to established guidelines. The descriptions generated through queried image description (QID) were more likely to include information on the image category, title, caption, and units. They were also more similar to one another, based on Jaccard distances of q-grams, indicating that their word usage and structure were more standardized. Last, the workers preferred describing images using QID and found the task easier. Therefore, explicit instruction on image-description guidelines is not sufficient to produce quality image descriptions when using novice Web workers. Instead, it is better to provide information about images, then generate descriptions from responses using templates.},
  keywords     = {access technology,Accessibility (blind and visually impaired),crowdsourcing,human computation,image description},
  file         = {C:\Users\jseo1005\Zotero\storage\EEEHUELH\Morash et al. - 2015 - Guiding Novice Web Workers in Making Image Descriptions Using Templates.pdf}
}

@article{muttaqinaInteractionDesignRefreshable2019,
  title        = {Interaction {{Design}} of {{Refreshable Braille Display}} to {{Support Learning}} for {{The Visually Impaired Kids}}},
  author       = {Muttaqina, Anisa Ayu and Estiyono, Andhika and Krisbianto, Ari Dwi},
  date         = {2019-05-31},
  journaltitle = {Jurnal Sains dan Seni ITS},
  shortjournal = {JSSITS},
  volume       = {8},
  number       = {1},
  pages        = {23--27},
  issn         = {2337-3520, 2301-928X},
  doi          = {10.12962/j23373520.v8i1.41405},
  url          = {http://ejurnal.its.ac.id/index.php/sains_seni/article/view/41405},
  urldate      = {2023-11-14},
  abstract     = {Impaired vision is a condition where someone is unable to see clearly, even when they are using glasses and in condition where there is enough amount of light. Impaired vision in Indonesia is not only become health problem, but has already become social problem. One of the social needs is education. However, people with impaired vision have differences way in how they learn, because they use braille characters in their education process. There is a device to support the braille learning which name is refreshable braille display. However, the devices are usually sold in a very expensive price. Based on the background and the above phenomenon, this research is about to do the design of low-cost refreshable braille display for educational needs.},
  langid       = {english},
  file         = {C:\Users\jseo1005\Zotero\storage\X5MCM725\Muttaqina et al. - 2019 - Interaction Design of Refreshable Braille Display to Support Learning for The Visually Impaired Kids.pdf}
}

@inproceedings{mynattNonvisualPresentationGraphical1994,
  title      = {Nonvisual Presentation of Graphical User Interfaces: Contrasting Two Approaches},
  shorttitle = {Nonvisual Presentation of Graphical User Interfaces},
  booktitle  = {Conference {{Companion}} on {{Human Factors}} in {{Computing Systems}}},
  author     = {Mynatt, Elizabeth D. and Weber, Gerhard},
  date       = {1994-04-28},
  series     = {{{CHI}} '94},
  pages      = {211},
  publisher  = {Association for Computing Machinery},
  location   = {New York, NY, USA},
  doi        = {10.1145/259963.260338},
  url        = {https://dl.acm.org/doi/10.1145/259963.260338},
  urldate    = {2023-11-13},
  isbn       = {978-0-89791-651-6},
  file       = {C:\Users\jseo1005\Zotero\storage\IPANYNMJ\Mynatt and Weber - 1994 - Nonvisual presentation of graphical user interfaces contrasting two approaches.pdf}
}

@inproceedings{niemannReflectionsTeachingData2023,
  title      = {Reflections on {{Teaching}} ‘{{Data Exploration}} and {{Visualisation}}’ in {{Multiple Modes}}},
  booktitle  = {2023 {{IEEE VIS Workshop}} on {{Visualization Education}}, {{Literacy}}, and {{Activities}} ({{EduVis}})},
  author     = {Niemann, Michael and Goodwin, Sarah and Marriott, Kim},
  date       = {2023-10-22},
  pages      = {28--33},
  publisher  = {IEEE},
  location   = {Melbourne, Australia},
  doi        = {10.1109/EduVis60792.2023.00011},
  url        = {https://ieeexplore.ieee.org/document/10343935/},
  urldate    = {2024-01-15},
  abstract   = {Whilst the visual arts is a very practical and tangible field, data visualisation is a combination of programming skills, data science and design theory. Each of these can be taught in a variety of ways, from traditional methods of lectures to technical programming classes or even asynchronous worksheets. The challenge is how to cover all of the required content within the constraints of the teaching period, environment, delivery method and ensuring the appropriate activities and assessment tasks. In this paper we reflect on the past 8 years of delivering the postgraduate unit ‘Data Exploration and Visualisation’ at Monash University. We present the four different ways that the same content has been taught to different cohorts, through a combination of on-campus lectures, large-format workshops, small group tutorials and online or hybrid tutorials and workshops, as well as various readings, videos and asynchronous activities. We explain the different external and internal requirements set for the teaching and describe the adaption of the teaching methods and assessments in order to meet these conditions. We reflect on our experiences of teaching multiple methods across different formats from very small to very large student cohorts.},
  eventtitle = {2023 {{IEEE VIS Workshop}} on {{Visualization Education}}, {{Literacy}}, and {{Activities}} ({{EduVis}})},
  isbn       = {9798350330304},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\2FUBWDIV\Niemann et al. - 2023 - Reflections on Teaching ‘Data Exploration and Visualisation’ in Multiple Modes.pdf}
}

@online{obeidCharttoTextGeneratingNatural2020,
  title       = {Chart-to-{{Text}}: {{Generating Natural Language Descriptions}} for {{Charts}} by {{Adapting}} the {{Transformer Model}}},
  shorttitle  = {Chart-to-{{Text}}},
  author      = {Obeid, Jason and Hoque, Enamul},
  date        = {2020-11-29},
  eprint      = {2010.09142},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2010.09142},
  url         = {http://arxiv.org/abs/2010.09142},
  urldate     = {2024-03-25},
  abstract    = {Information visualizations such as bar charts and line charts are very popular for exploring data and communicating insights. Interpreting and making sense of such visualizations can be challenging for some people, such as those who are visually impaired or have low visualization literacy. In this work, we introduce a new dataset and present a neural model for automatically generating natural language summaries for charts. The generated summaries provide an interpretation of the chart and convey the key insights found within that chart. Our neural model is developed by extending the state-of-the-art model for the data-to-text generation task, which utilizes a transformer-based encoder-decoder architecture. We found that our approach outperforms the base model on a content selection metric by a wide margin (55.42\% vs. 8.49\%) and generates more informative, concise, and coherent summaries.},
  pubstate    = {preprint},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file        = {C\:\\Users\\jseo1005\\Zotero\\storage\\66Q3QHSV\\Obeid and Hoque - 2020 - Chart-to-Text Generating Natural Language Descrip.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\5RC7CN4K\\2010.html}
}

@software{OlliScreenReader2022,
  title        = {Olli - {{Screen Reader Accessibility}} for {{Data Visualization}}},
  date         = {2022-10-05T01:58:04Z},
  origdate     = {2022-05-10T18:27:58Z},
  url          = {https://github.com/mitvis/olli},
  urldate      = {2022-10-05},
  abstract     = {A library for converting web visualizations into accessible text structures for blind and low-vision screen reader users.},
  organization = {MIT Visualization Group}
}

@article{omodhrainDesigningMediaVisuallyImpaired2015,
  title        = {Designing {{Media}} for {{Visually-Impaired Users}} of {{Refreshable Touch Displays}}: {{Possibilities}} and {{Pitfalls}}},
  shorttitle   = {Designing {{Media}} for {{Visually-Impaired Users}} of {{Refreshable Touch Displays}}},
  author       = {O’Modhrain, Sile and Giudice, Nicholas A. and Gardner, John A. and Legge, Gordon E.},
  date         = {2015-07},
  journaltitle = {IEEE Transactions on Haptics},
  volume       = {8},
  number       = {3},
  pages        = {248--257},
  issn         = {2329-4051},
  doi          = {10.1109/TOH.2015.2466231},
  abstract     = {This paper discusses issues of importance to designers of media for visually impaired users. The paper considers the influence of human factors on the effectiveness of presentation as well as the strengths and weaknesses of tactile, vibrotactile, haptic, and multimodal methods of rendering maps, graphs, and models. The authors, all of whom are visually impaired researchers in this domain, present findings from their own work and work of many others who have contributed to the current understanding of how to prepare and render images for both hard-copy and technology-mediated presentation of Braille and tangible graphics.},
  eventtitle   = {{{IEEE Transactions}} on {{Haptics}}},
  keywords     = {haptic display,Haptic interfaces,Media,refreshible braille.,Rendering (computer graphics),Shape,Solid modeling,tactile display,Tangible graphics,touch display,vibrotactile display,Visualization,visually impaired},
  file         = {C\:\\Users\\jseo1005\\Zotero\\storage\\4B8UTWDV\\O’Modhrain et al. - 2015 - Designing Media for Visually-Impaired Users of Ref.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\UPVMSZ6B\\7182782.html}
}

@online{openaiGPT4TechnicalReport2023,
  title       = {{{GPT-4 Technical Report}}},
  author      = {OpenAI},
  date        = {2023-03-27},
  eprint      = {2303.08774},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2303.08774},
  url         = {http://arxiv.org/abs/2303.08774},
  urldate     = {2023-11-17},
  abstract    = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  pubstate    = {preprint},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file        = {C\:\\Users\\jseo1005\\Zotero\\storage\\X7LBZXJ2\\OpenAI - 2023 - GPT-4 Technical Report.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\ZR7NRMMJ\\2303.html}
}

@online{organIncompleteGuideAccessible2021,
  title        = {An {{Incomplete Guide}} to {{Accessible Data Visualization}}},
  author       = {Organ, Nancy},
  date         = {2021-08-05T16:43:46},
  url          = {https://towardsdatascience.com/an-incomplete-guide-to-accessible-data-visualization-33f15bfcc400},
  urldate      = {2022-08-21},
  abstract     = {Practical tips for doing even a little bit better.},
  langid       = {english},
  organization = {Medium},
  file         = {C:\Users\jseo1005\Zotero\storage\9SC33Q59\an-incomplete-guide-to-accessible-data-visualization-33f15bfcc400.html}
}

@article{panditAgileUATFrameworkUser2015,
  title        = {{{AgileUAT}}: {{A Framework}} for {{User Acceptance Testing}} Based on {{User Stories}} and {{Acceptance Criteria}}},
  shorttitle   = {{{AgileUAT}}},
  author       = {Pandit, Pallavi and Tahiliani, Swati},
  date         = {2015-06-18},
  journaltitle = {International Journal of Computer Applications},
  shortjournal = {IJCA},
  volume       = {120},
  number       = {10},
  pages        = {16--21},
  issn         = {09758887},
  doi          = {10.5120/21262-3533},
  url          = {http://research.ijcaonline.org/volume120/number10/pxc3903533.pdf},
  urldate      = {2023-03-13},
  abstract     = {User Acceptance Testing (UAT) has widespread implications in the software community. It involves not only the end-user, but the Quality Assurance (QA) team, developers, business analysts and top level management. UAT is conducted with the aim of developing confidence of the user in the software product. UAT is generally performed manually and not preferred to be automated. UAT frameworks exist for Agile methodologies such as Scrum. We propose a UAT process model which adapts the generic agile process model. Hence, it is able to encompass every agile methodology. AgileUAT, aims at generation of exhaustive acceptance test cases in natural language, based on acceptance criteria. It indicates whether the acceptance criteria is fulfilled or not, as a percentage value. The tool illustrates traceability among epics, user stories, acceptance criteria and acceptance test cases. We explore several different templates for user stories and acceptance criteria. In the future, we aim to provide a direct mapping between the acceptance criteria and acceptance test cases based on permutations and combinations using decision tables.},
  langid       = {english},
  file         = {C:\Users\jseo1005\Zotero\storage\CGHSKN8Y\Pandit and Tahiliani - 2015 - AgileUAT A Framework for User Acceptance Testing .pdf}
}

@article{paneelsReviewDesignsHaptic2010,
  title        = {Review of {{Designs}} for {{Haptic Data Visualization}}},
  author       = {Paneels, Sabrina and Roberts, Jonathan C.},
  date         = {2010-04},
  journaltitle = {IEEE Transactions on Haptics},
  volume       = {3},
  number       = {2},
  pages        = {119--137},
  issn         = {2329-4051},
  doi          = {10.1109/TOH.2009.44},
  abstract     = {There are many different uses for haptics, such as training medical practitioners, teleoperation, or navigation of virtual environments. This review focuses on haptic methods that display data. The hypothesis is that haptic devices can be used to present information, and consequently, the user gains quantitative, qualitative, or holistic knowledge about the presented data. Not only is this useful for users who are blind or partially sighted (who can feel line graphs, for instance), but also the haptic modality can be used alongside other modalities, to increase the amount of variables being presented, or to duplicate some variables to reinforce the presentation. Over the last 20 years, a significant amount of research has been done in haptic data presentation; e.g., researchers have developed force feedback line graphs, bar charts, and other forms of haptic representations. However, previous research is published in different conferences and journals, with different application emphases. This paper gathers and collates these various designs to provide a comprehensive review of designs for haptic data visualization. The designs are classified by their representation: Charts, Maps, Signs, Networks, Diagrams, Images, and Tables. This review provides a comprehensive reference for researchers and learners, and highlights areas for further research.},
  eventtitle   = {{{IEEE Transactions}} on {{Haptics}}},
  keywords     = {Biomedical imaging,Data visualization,Displays,Force feedback,Frequency,Haptic data visualization,haptic design.,Haptic interfaces,haptic visualization,haptics,Mathematical model,Navigation,non-visual visualization,Rendering (computer graphics),Virtual environment},
  file         = {C:\Users\jseo1005\Zotero\storage\XW9SILTL\Paneels and Roberts - 2010 - Review of Designs for Haptic Data Visualization.pdf}
}

@article{parkImpactDataVisualization2022,
  title        = {Impact of Data Visualization on Decision-Making and Its Implications for Public Health Practice: A Systematic Literature Review},
  shorttitle   = {Impact of Data Visualization on Decision-Making and Its Implications for Public Health Practice},
  author       = {Park, Seungeun and Bekemeier, Betty and Flaxman, Abraham and Schultz, Melinda},
  date         = {2022-04-03},
  journaltitle = {Informatics for Health and Social Care},
  volume       = {47},
  number       = {2},
  eprint       = {34582297},
  eprinttype   = {pmid},
  pages        = {175--193},
  publisher    = {Taylor \& Francis},
  issn         = {1753-8157},
  doi          = {10.1080/17538157.2021.1982949},
  url          = {https://doi.org/10.1080/17538157.2021.1982949},
  urldate      = {2024-04-23},
  abstract     = {Data visualization tools have the potential to support decision-making for public health professionals. This review summarizes the science and evidence regarding data visualization and its impact on decision-making behavior as informed by cognitive processes such as understanding, attitude, or perception. An electronic literature search was conducted using six databases, including reference list reviews. Search terms were pre-defined based on research questions. Sixteen studies were included in the final analysis. Data visualization interventions in this review were found to impact attitude, perception, and decision-making compared to controls. These relationships between the interventions and outcomes appear to be explained by mediating factors such as perceived trustworthiness and quality, domain-specific knowledge, basic beliefs shared by social groups, and political beliefs. Visualization appears to bring advantages by increasing the amount of information delivered and decreasing the cognitive and intellectual burden to interpret information for decision-making. However, understanding data visualization interventions specific to public health leaders’ decision-making is lacking, and there is little guidance for understanding a participant’s characteristics and tasks. The evidence from this review suggests positive effects of data visualization can be identified, depending on the control of confounding factors on attitude, perception, and decision-making.},
  keywords     = {Data visualization,decision-making,local health department,public health informatics,public health practice}
}

@inproceedings{pengSayItAll2021,
  title      = {Say {{It All}}: {{Feedback}} for {{Improving Non-Visual Presentation Accessibility}}},
  shorttitle = {Say {{It All}}},
  booktitle  = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author     = {Peng, Yi-Hao and Jang, JiWoong and Bigham, Jeffrey P and Pavel, Amy},
  date       = {2021-05-06},
  pages      = {1--12},
  publisher  = {ACM},
  location   = {Yokohama Japan},
  doi        = {10.1145/3411764.3445572},
  url        = {https://dl.acm.org/doi/10.1145/3411764.3445572},
  urldate    = {2022-12-07},
  abstract   = {Presenters commonly use slides as visual aids for informative talks. When presenters fail to verbally describe the content on their slides, blind and visually impaired audience members lose access to necessary content, making the presentation difcult to follow. Our analysis of 90 presentation videos revealed that 72\% of 610 visual elements (e.g., images, text) were insufciently described. To help presenters create accessible presentations, we introduce Presentation A11y, a system that provides real-time and post-presentation accessibility feedback. Our system analyzes visual elements on the slide and the transcript of the verbal presentation to provide element-level feedback on what visual content needs to be further described or even removed. Presenters using our system with their own slide-based presentations described more of the content on their slides, and identifed 3.26 times more accessibility problems to fx after the talk than when using a traditional slide-based presentation interface. Integrating accessibility feedback into content creation tools will improve the accessibility of informational content for all.},
  eventtitle = {{{CHI}} '21: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn       = {978-1-4503-8096-6},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\LI5QXB65\Peng et al. - 2021 - Say It All Feedback for Improving Non-Visual Pres.pdf}
}

@inproceedings{potluriCodeTalkImprovingProgramming2018,
  title      = {{{CodeTalk}}: {{Improving Programming Environment Accessibility}} for {{Visually Impaired Developers}}},
  shorttitle = {{{CodeTalk}}},
  booktitle  = {Proceedings of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author     = {Potluri, Venkatesh and Vaithilingam, Priyan and Iyengar, Suresh and Vidya, Y. and Swaminathan, Manohar and Srinivasa, Gopal},
  date       = {2018-04-21},
  series     = {{{CHI}} '18},
  pages      = {1--11},
  publisher  = {Association for Computing Machinery},
  location   = {New York, NY, USA},
  doi        = {10.1145/3173574.3174192},
  url        = {https://doi.org/10.1145/3173574.3174192},
  urldate    = {2022-12-09},
  abstract   = {In recent times, programming environments like Visual Studio are widely used to enhance programmer productivity. However, inadequate accessibility prevents Visually Impaired (VI) developers from taking full advantage of these environments. In this paper, we focus on the accessibility challenges faced by the VI developers in using Graphical User Interface (GUI) based programming environments. Based on a survey of VI developers and based on two of the authors' personal experiences, we categorize the accessibility difficulties into Discoverability, Glanceability, Navigability, and Alertability. We propose solutions to some of these challenges and implement these in CodeTalk, a plugin for Visual Studio. We show how CodeTalk improves developer experience and share promising early feedback from VI developers who used our plugin.},
  isbn       = {978-1-4503-5620-6},
  keywords   = {accessibility,audio debugging,programming environments,visually impaired},
  file       = {C:\Users\jseo1005\Zotero\storage\6D4HNELI\Potluri et al. - 2018 - CodeTalk Improving Programming Environment Access.pdf}
}

@inproceedings{potluriNotablyInaccessibleData2023,
  title     = {Notably {{Inaccessible}} — {{Data Driven Understanding}} of {{Data Science Notebook}} ({{In}}){{Accessibility}}},
  booktitle = {Proceedings of the 25th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author    = {Potluri, Venkatesh and Singanamalla, Sudheesh and Tieanklin, Nussara and Mankoff, Jennifer},
  date      = {2023-10-22},
  series    = {{{ASSETS}} '23},
  pages     = {1--19},
  publisher = {Association for Computing Machinery},
  location  = {New York, NY, USA},
  doi       = {10.1145/3597638.3608417},
  url       = {https://dl.acm.org/doi/10.1145/3597638.3608417},
  urldate   = {2023-11-01},
  abstract  = {Computational notebooks, tools that facilitate storytelling through exploration, data analysis, and information visualization, have become the widely accepted standard in the data science community. These notebooks have been widely adopted through notebook software such as Jupyter, Datalore and Google Colab, both in academia and industry. While there is extensive research to learn how data scientists use computational notebooks, identify their pain points, and enable collaborative data science practices, very little is known about the various accessibility barriers experienced by blind and visually impaired (BVI) users using these notebooks. BVI users are unable to use computational notebook interfaces due to (1) inaccessibility of the interface, (2) common ways in which data is represented in these interfaces, and (3) inability for popular libraries to provide accessible outputs. We perform a large scale systematic analysis of 100000 Jupyter notebooks to identify various accessibility challenges in published notebooks affecting the creation and consumption of these notebooks. Through our findings, we make recommendations to improve accessibility of the artifacts of a notebook, suggest authoring practices, and propose changes to infrastructure to make notebooks accessible.},
  isbn      = {9798400702204},
  keywords  = {Accessibility,computational notebooks,Data science,measurement},
  file      = {C:\Users\jseo1005\Zotero\storage\I5ZLZCY2\Potluri et al. - 2023 - Notably Inaccessible — Data Driven Understanding o.pdf}
}

@online{qinChatGPTGeneralPurposeNatural2023,
  title       = {Is {{ChatGPT}} a {{General-Purpose Natural Language Processing Task Solver}}?},
  author      = {Qin, Chengwei and Zhang, Aston and Zhang, Zhuosheng and Chen, Jiaao and Yasunaga, Michihiro and Yang, Diyi},
  date        = {2023-11-19},
  eprint      = {2302.06476},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2302.06476},
  url         = {http://arxiv.org/abs/2302.06476},
  urldate     = {2024-04-20},
  abstract    = {Spurred by advancements in scale, large language models (LLMs) have demonstrated the ability to perform a variety of natural language processing (NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently, the debut of ChatGPT has drawn a great deal of attention from the natural language processing (NLP) community due to the fact that it can generate high-quality responses to human input and self-correct previous mistakes based on subsequent conversations. However, it is not yet known whether ChatGPT can serve as a generalist model that can perform many NLP tasks zero-shot. In this work, we empirically analyze the zero-shot learning ability of ChatGPT by evaluating it on 20 popular NLP datasets covering 7 representative task categories. With extensive empirical studies, we demonstrate both the effectiveness and limitations of the current version of ChatGPT. We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging. We additionally provide in-depth analysis through qualitative case studies.},
  pubstate    = {preprint},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file        = {C:\Users\jseo1005\Zotero\storage\SYWLQFX9\2302.html}
}

@online{radfordLearningTransferableVisual2021,
  title       = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author      = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  date        = {2021-02-26},
  eprint      = {2103.00020},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2103.00020},
  url         = {http://arxiv.org/abs/2103.00020},
  urldate     = {2023-11-18},
  abstract    = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  pubstate    = {preprint},
  keywords    = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file        = {C\:\\Users\\jseo1005\\Zotero\\storage\\N77EIEWS\\Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\22YTVENV\\2103.html}
}

@online{radfordRobustSpeechRecognition2022,
  title       = {Robust {{Speech Recognition}} via {{Large-Scale Weak Supervision}}},
  author      = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  date        = {2022-12-06},
  eprint      = {2212.04356},
  eprinttype  = {arxiv},
  eprintclass = {cs, eess},
  doi         = {10.48550/arXiv.2212.04356},
  url         = {http://arxiv.org/abs/2212.04356},
  urldate     = {2024-04-24},
  abstract    = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
  pubstate    = {preprint},
  keywords    = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file        = {C\:\\Users\\jseo1005\\Zotero\\storage\\T57GSX2E\\Radford et al. - 2022 - Robust Speech Recognition via Large-Scale Weak Sup.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\7EB5Y2KK\\2212.html}
}

@inproceedings{reynaEnhancingFlippedClassroom2016,
  title      = {Enhancing the {{Flipped Classroom Experience}} with the {{Aid}} of {{Inclusive Design}}},
  author     = {Reyna, Jorge and Davila, Yvonne C. and Meier, Peter},
  date       = {2016-06-28},
  pages      = {1795--1807},
  publisher  = {Association for the Advancement of Computing in Education (AACE)},
  url        = {https://www.learntechlib.org/primary/p/173190/},
  urldate    = {2024-02-09},
  abstract   = {Flipped classrooms are increasingly used in tertiary institutions to engage students in active learning tasks and foster independent learning skills. The use of technology such as digital video, screencasts and interactive presentations is impacting the design of flipped classrooms. This creates an opportunity to apply the principles of Inclusive Design in the planning, development and deployment of resources used to flip the classroom. The aim of this paper is to discuss the integration of Inclusive Design into Flipped Classroom interventions to cater for a wider range of learners. For...},
  eventtitle = {{{EdMedia}} + {{Innovate Learning}}},
  isbn       = {978-1-939797-24-7},
  langid     = {english}
}

@inproceedings{robertsNewTechnologyEnables2000,
  title     = {New Technology Enables Many-Fold Reduction in the Cost of Refreshable {{Braille}} Displays},
  booktitle = {Proceedings of the Fourth International {{ACM}} Conference on {{Assistive}} Technologies},
  author    = {Roberts, John and Slattery, Oliver and Kardos, David and Swope, Brett},
  date      = {2000-11-13},
  series    = {Assets '00},
  pages     = {42--49},
  publisher = {Association for Computing Machinery},
  location  = {New York, NY, USA},
  doi       = {10.1145/354324.354335},
  url       = {https://dl.acm.org/doi/10.1145/354324.354335},
  urldate   = {2023-11-13},
  abstract  = {By analysis of the primary cost factors for existing refreshable Braille displays, a team at NIST has pioneered a new technology that can reduce the cost of the electromechanical portions of a Braille display by an extremely large factor, and the overall cost of a Braille display by as much as a factor of ten. A massive cost reduction in displays creates a new model for the purchase and use of Braille displays by individuals, by employers, and by educators. Readability and user control issues are are addressed. It is hoped that this technology will open a significant new market for low cost, high performance refreshable Braille displays.},
  isbn      = {978-1-58113-313-4},
  keywords  = {passive pin retention,refreshable Braille,wheel},
  file      = {C:\Users\jseo1005\Zotero\storage\J9ATR2JN\Roberts et al. - 2000 - New technology enables many-fold reduction in the cost of refreshable Braille displays.pdf}
}

@inproceedings{rochaSpeechtoTextInterfaceMammoClass2016,
  title      = {A {{Speech-to-Text Interface}} for {{MammoClass}}},
  booktitle  = {2016 {{IEEE}} 29th {{International Symposium}} on {{Computer-Based Medical Systems}} ({{CBMS}})},
  author     = {Rocha, Ricardo Sousa and Ferreira, Pedro and Dutra, Ines and Correia, Ricardo and Salvini, Rogerio and Burnside, Elizabeth},
  date       = {2016-06},
  pages      = {1--6},
  publisher  = {IEEE},
  location   = {Belfast and Dublin, Ireland},
  doi        = {10.1109/CBMS.2016.25},
  url        = {http://ieeexplore.ieee.org/document/7545947/},
  urldate    = {2022-08-21},
  eventtitle = {2016 {{IEEE}} 29th {{International Symposium}} on {{Computer-Based Medical Systems}} ({{CBMS}})},
  isbn       = {978-1-4673-9036-1},
  file       = {C:\Users\jseo1005\Zotero\storage\F3Q2HP3D\Rocha et al. - 2016 - A Speech-to-Text Interface for MammoClass.pdf}
}

@article{russomannoRefreshingRefreshableBraille2015,
  title        = {Refreshing {{Refreshable Braille Displays}}},
  author       = {Russomanno, Alexander and O’Modhrain, Sile and Gillespie, R. Brent and Rodger, Matthew W. M.},
  date         = {2015-07},
  journaltitle = {IEEE Transactions on Haptics},
  volume       = {8},
  number       = {3},
  pages        = {287--297},
  issn         = {2329-4051},
  doi          = {10.1109/TOH.2015.2423492},
  url          = {https://ieeexplore.ieee.org/abstract/document/7086320?casa_token=dGanPTq4ou4AAAAA:iIbtxje4WfYvG4YhQH75177iI1ez3qrBD5PHv7V2hpbRfFzEqjrA9x1zGRZsT9K6gAu4W8z9WqA},
  urldate      = {2023-11-14},
  abstract     = {The increased access to books afforded to blind people via e-publishing has given them long-sought independence for both recreational and educational reading. In most cases, blind readers access materials using speech output. For some content such as highly technical texts, music, and graphics, speech is not an appropriate access modality as it does not promote deep understanding. Therefore blind braille readers often prefer electronic braille displays. But, these are prohibitively expensive. The search is on, therefore, for a low-cost refreshable display that would go beyond current technologies and deliver graphical content as well as text. And many solutions have been proposed, some of which reduce costs by restricting the number of characters that can be displayed, even down to a single braille cell. In this paper, we demonstrate that restricting tactile cues during braille reading leads to poorer performance in a letter recognition task. In particular, we show that lack of sliding contact between the fingertip and the braille reading surface results in more errors and that the number of errors increases as a function of presentation speed. These findings suggest that single cell displays which do not incorporate sliding contact are likely to be less effective for braille reading.},
  eventtitle   = {{{IEEE Transactions}} on {{Haptics}}},
  file         = {C\:\\Users\\jseo1005\\Zotero\\storage\\255Q5D5U\\Russomanno et al. - 2015 - Refreshing Refreshable Braille Displays.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\ZQDWJ9LF\\7086320.html}
}

@article{saikotRefreshableBrailleDisplay2022,
  title        = {Refreshable {{Braille Display With Adjustable Cell Size}} for {{Learners With Different Tactile Sensitivity}}},
  author       = {Saikot, Md Mahmud Hasan and Sanim, Kazi Ragib Ishraq},
  date         = {2022-07},
  journaltitle = {IEEE Transactions on Haptics},
  volume       = {15},
  number       = {3},
  pages        = {582--591},
  issn         = {2329-4051},
  doi          = {10.1109/TOH.2022.3184265},
  url          = {https://ieeexplore.ieee.org/abstract/document/9799767?casa_token=gor2tiDtWuEAAAAA:1TGhGS8QpgOqMbLMHbSlXRY94lOx_-8p8A13aGIWgnH9oLkczUzm9QQSD_HrKAkZAKhUnHfMkfs},
  urldate      = {2023-11-14},
  abstract     = {Braille is one of the most popular mediums of education for the blind. However, learning braille requires trainers and a lot of practice. Additionally, different individuals have different levels of tactile sensitivity at their fingertips. The tactile components get often overlooked in most braille learning devices and related studies. Our solution is a single cell refreshable braille display with six custom-made electromechanical flapper actuators. It incorporates speech functionalities to facilitate self-learning and independent operation. The cell size can be adjusted according to the learner's preference by moving the actuators. The device can provide standard braille cell dimensions and elevation as well. It is designed to help learners with different tactile perceptions improve themselves through practice and adapt to standard size braille. The operational conditions and force analysis of the braille dots were performed. Two tests were also performed with two different cell sizes to evaluate the device with several blind students. The device is very affordable and easy to maintain. It can also be used to teach braille to the sighted.},
  eventtitle   = {{{IEEE Transactions}} on {{Haptics}}},
  file         = {C\:\\Users\\jseo1005\\Zotero\\storage\\PQ3GJYYV\\Saikot and Sanim - 2022 - Refreshable Braille Display With Adjustable Cell Size for Learners With Different Tactile Sensitivit.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\E26FEGLL\\9799767.html}
}

@article{saikotRefreshableBrailleDisplay2022a,
  title        = {Refreshable {{Braille Display With Adjustable Cell Size}} for {{Learners With Different Tactile Sensitivity}}},
  author       = {Saikot, Md Mahmud Hasan and Sanim, Kazi Ragib Ishraq},
  date         = {2022-07},
  journaltitle = {IEEE Transactions on Haptics},
  volume       = {15},
  number       = {3},
  pages        = {582--591},
  issn         = {2329-4051},
  doi          = {10.1109/TOH.2022.3184265},
  url          = {https://ieeexplore.ieee.org/abstract/document/9799767?casa_token=gor2tiDtWuEAAAAA:1TGhGS8QpgOqMbLMHbSlXRY94lOx_-8p8A13aGIWgnH9oLkczUzm9QQSD_HrKAkZAKhUnHfMkfs},
  urldate      = {2023-11-14},
  abstract     = {Braille is one of the most popular mediums of education for the blind. However, learning braille requires trainers and a lot of practice. Additionally, different individuals have different levels of tactile sensitivity at their fingertips. The tactile components get often overlooked in most braille learning devices and related studies. Our solution is a single cell refreshable braille display with six custom-made electromechanical flapper actuators. It incorporates speech functionalities to facilitate self-learning and independent operation. The cell size can be adjusted according to the learner's preference by moving the actuators. The device can provide standard braille cell dimensions and elevation as well. It is designed to help learners with different tactile perceptions improve themselves through practice and adapt to standard size braille. The operational conditions and force analysis of the braille dots were performed. Two tests were also performed with two different cell sizes to evaluate the device with several blind students. The device is very affordable and easy to maintain. It can also be used to teach braille to the sighted.},
  eventtitle   = {{{IEEE Transactions}} on {{Haptics}}}
}

@inproceedings{salberApplyingWizardOz1993,
  title     = {Applying the {{Wizard}} of {{Oz}} Technique to the Study of Multimodal Systems},
  booktitle = {Human-{{Computer Interaction}}},
  author    = {Salber, Daniel and Coutaz, Joëlle},
  editor    = {Bass, Leonard J. and Gornostaev, Juri and Unger, Claus},
  date      = {1993},
  series    = {Lecture {{Notes}} in {{Computer Science}}},
  pages     = {219--230},
  publisher = {Springer},
  location  = {Berlin, Heidelberg},
  doi       = {10.1007/3-540-57433-6_51},
  abstract  = {The Wizard of Oz (WOz) technique is an experimental evaluation mechanism. It allows the observation of a user operating an apparently fully functioning system whose missing services are supplemented by a hidden wizard. From our analysis of existing WOz systems, we observe that this technique has primarily been used to study natural language interfaces. With recent advances in interactive media, multimodal user interfaces are becoming popular but our current understanding on how to design such systems is still primitive. In the absence of generalizable theories and models, the WOz technique is an appropriate approach to the identification of sound design solutions. We show how the WOz technique can be extended to the analysis of multimodal interfaces and we formulate a set of requirements for a generic multimodal WOz platform. The Neimo system is presented as an illustration of our early experience in the development of such platforms.},
  isbn      = {978-3-540-48152-2},
  langid    = {english},
  keywords  = {Client Application,Evaluation Expert,Multimodal Interaction,Multimodal Interface,Multimodal System},
  file      = {C:\Users\jseo1005\Zotero\storage\28FEDWJI\Salber and Coutaz - 1993 - Applying the Wizard of Oz technique to the study o.pdf}
}

@article{sandersCocreationNewLandscapes2008,
  title        = {Co-Creation and the New Landscapes of Design},
  author       = {Sanders, Elizabeth B.-N. and Stappers, Pieter Jan},
  date         = {2008-03-01},
  journaltitle = {CoDesign},
  volume       = {4},
  number       = {1},
  pages        = {5--18},
  publisher    = {Taylor \& Francis},
  issn         = {1571-0882},
  doi          = {10.1080/15710880701875068},
  url          = {https://doi.org/10.1080/15710880701875068},
  urldate      = {2023-09-13},
  abstract     = {Designers have been moving increasingly closer to the future users of what they design and the next new thing in the changing landscape of design research has become co-designing with your users. But co-designing is actually not new at all, having taken distinctly different paths in the US and in Europe. The evolution in design research from a user-centred approach to co-designing is changing the roles of the designer, the researcher and the person formerly known as the ‘user’. The implications of this shift for the education of designers and researchers are enormous. The evolution in design research from a user-centred approach to co-designing is changing the landscape of design practice as well, creating new domains of collective creativity. It is hoped that this evolution will support a transformation toward more sustainable ways of living in the future.},
  keywords     = {co-creation,co-design,collective creativity,design research,participatory design,user-centred design},
  file         = {C:\Users\jseo1005\Zotero\storage\M3ZKHHRP\Sanders and Stappers - 2008 - Co-creation and the new landscapes of design.pdf}
}

@online{SASHelpCenter,
  title   = {{{SAS Help Center}}: {{About}} the {{SAS Graphics Accelerator}}},
  url     = {https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.5/odsacoutput/p1363trzxif9zun1rq43uo5ys7q3.htm},
  urldate = {2022-08-20},
  file    = {C:\Users\jseo1005\Zotero\storage\8GKLQ229\p1363trzxif9zun1rq43uo5ys7q3.html}
}

@online{schepersWhyAccessibilityHeart2020,
  title        = {Why {{Accessibility Is}} at the {{Heart}} of {{Data Visualization}}},
  author       = {Schepers, Doug},
  date         = {2020-05-21T17:12:50},
  url          = {https://medium.com/nightingale/accessibility-is-at-the-heart-of-data-visualization-64a38d6c505b},
  urldate      = {2023-02-06},
  abstract     = {To make data viz more accessible, we first need to understand assistive technology},
  langid       = {english},
  organization = {Nightingale}
}

@online{seoBornAccessibleData2024,
  title       = {Born {{Accessible Data Science}} and {{Visualization Courses}}: {{Challenges}} of {{Developing Curriculum}} to Be {{Taught}} by {{Blind Instructors}} to {{Blind Students}}},
  shorttitle  = {Born {{Accessible Data Science}} and {{Visualization Courses}}},
  author      = {Seo, JooYoung and O'Modhrain, Sile and Xia, Yilin and Kamath, Sanchita and Lee, Bongshin and Coughlan, James M.},
  date        = {2024-03-04},
  eprint      = {2403.02568},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/2403.02568},
  urldate     = {2024-03-08},
  abstract    = {While recent years have seen a growing interest in accessible visualization tools and techniques for blind people, little attention is paid to the learning opportunities and teaching strategies of data science and visualization tailored for blind individuals. Whereas the former focuses on the accessibility issues of data visualization tools, the latter is concerned with the learnability of concepts and skills for data science and visualization. In this paper, we present novel approaches to teaching data science and visualization to blind students in an online setting. Taught by blind instructors, nine blind learners having a wide range of professional backgrounds participated in a two-week summer course. We describe the course design, teaching strategies, and learning outcomes. We also discuss the challenges and opportunities of teaching data science and visualization to blind students. Our work contributes to the growing body of knowledge on accessible data science and visualization education, and provides insights into the design of online courses for blind students.},
  pubstate    = {preprint},
  version     = {1},
  keywords    = {Computer Science - Human-Computer Interaction},
  file        = {C\:\\Users\\jseo1005\\Zotero\\storage\\4XNL9LK9\\Seo et al. - 2024 - Born Accessible Data Science and Visualization Courses Challenges of Developing Curriculum to be Ta.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\KG4NTTUW\\2403.html}
}

@inproceedings{seoMAIDRMakingStatistical2024,
  title      = {{{MAIDR}}: {{Making Statistical Visualizations Accessible}} with {{Multimodal Data Representation}}},
  shorttitle = {{{MAIDR}}},
  booktitle  = {Proceedings of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author     = {Seo, JooYoung and Xia, Yilin and Lee, Bongshin and Mccurry, Sean and Yam, Yu Jun},
  date       = {2024-05-11},
  series     = {{{CHI}} '24},
  pages      = {1--22},
  publisher  = {Association for Computing Machinery},
  location   = {New York, NY, USA},
  doi        = {10.1145/3613904.3642730},
  url        = {https://dl.acm.org/doi/10.1145/3613904.3642730},
  urldate    = {2024-05-14},
  abstract   = {This paper investigates new data exploration experiences that enable blind users to interact with statistical data visualizations—bar plots, heat maps, box plots, and scatter plots—leveraging multimodal data representations. In addition to sonification and textual descriptions that are commonly employed by existing accessible visualizations, our MAIDR (multimodal access and interactive data representation) system incorporates two additional modalities (braille and review) that offer complementary benefits. It also provides blind users with the autonomy and control to interactively access and understand data visualizations. In a user study involving 11 blind participants, we found the MAIDR system facilitated the accurate interpretation of statistical visualizations. Participants exhibited a range of strategies in combining multiple modalities, influenced by their past interactions and experiences with data visualizations. This work accentuates the overlooked potential of combining refreshable tactile representation with other modalities and elevates the discussion on the importance of user autonomy when designing accessible data visualizations.},
  isbn       = {9798400703300},
  keywords   = {Accessibility,Blind,Braille Display,Multimodality,Screen Readers,Statistical Visualization},
  file       = {C:\Users\jseo1005\Zotero\storage\3VPFHR65\Seo et al. - 2024 - MAIDR Making Statistical Visualizations Accessible with Multimodal Data Representation.pdf}
}

@online{seoMAIDRMakingStatistical2024a,
  title       = {{{MAIDR}}: {{Making Statistical Visualizations Accessible}} with {{Multimodal Data Representation}}},
  shorttitle  = {{{MAIDR}}},
  author      = {Seo, JooYoung and Xia, Yilin and Lee, Bongshin and McCurry, Sean and Yam, Yu Jun},
  date        = {2024-03-01},
  eprint      = {2403.00717},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  doi         = {10.1145/3613904.3642730},
  url         = {http://arxiv.org/abs/2403.00717},
  urldate     = {2024-04-24},
  abstract    = {This paper investigates new data exploration experiences that enable blind users to interact with statistical data visualizations\$-\$bar plots, heat maps, box plots, and scatter plots\$-\$leveraging multimodal data representations. In addition to sonification and textual descriptions that are commonly employed by existing accessible visualizations, our MAIDR (multimodal access and interactive data representation) system incorporates two additional modalities (braille and review) that offer complementary benefits. It also provides blind users with the autonomy and control to interactively access and understand data visualizations. In a user study involving 11 blind participants, we found the MAIDR system facilitated the accurate interpretation of statistical visualizations. Participants exhibited a range of strategies in combining multiple modalities, influenced by their past interactions and experiences with data visualizations. This work accentuates the overlooked potential of combining refreshable tactile representation with other modalities and elevates the discussion on the importance of user autonomy when designing accessible data visualizations.},
  pubstate    = {preprint},
  keywords    = {Computer Science - Graphics,Computer Science - Human-Computer Interaction},
  file        = {C\:\\Users\\jseo1005\\Zotero\\storage\\AF8HWGTZ\\Seo et al. - 2024 - MAIDR Making Statistical Visualizations Accessibl.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\ZW4WFJVP\\2403.html}
}

@inproceedings{seoMAIDRMultimodalAccess2023,
  title     = {{{MAIDR}}: {{Multimodal Access}} and {{Interactive Data Representation System}} for {{Inclusive Data Science Education}}},
  booktitle = {Proceedings of the 3rd {{Annual Meeting}} of the {{International Society}} of the {{Learning Sciences}}},
  author    = {Seo, JooYoung and Xia, Yilin and Yam, Yu Jun and McCurry, Sean},
  date      = {2023},
  pages     = {51--54}
}

@article{seoTeachingVisualAccessibility2023,
  title        = {Teaching {{Visual Accessibility}} in {{Introductory Data Science Classes}} with {{Multi-Modal Data Representations}}},
  author       = {Seo, JooYoung and Dogucu, Mine},
  date         = {2023-03-21},
  journaltitle = {Journal of Data Science},
  pages        = {1--14},
  publisher    = {School of Statistics, Renmin University of China},
  issn         = {1680-743X, 1683-8602},
  doi          = {10.6339/23-JDS1095},
  url          = {https://jds-online.org/journal/JDS/article/1331},
  urldate      = {2023-04-10},
  abstract     = {Although there are various ways to represent data patterns and models, visualization has been primarily taught in many data science courses for its efficiency. Such vision-dependent output may cause critical barriers against those who are blind and visually impaired and people with learning disabilities. We argue that instructors need to teach multiple data representation methods so that all students can produce data products that are more accessible. In this paper, we argue that accessibility should be taught as early as the introductory course as part of the data science curriculum so that regardless of whether learners major in data science or not, they can have foundational exposure to accessibility. As data science educators who teach accessibility as part of our lower-division courses in two different institutions, we share specific examples that can be utilized by other data science instructors.},
  langid       = {english}
}

@inproceedings{serinDrawingUnderstandingDiagrams2022,
  title      = {Drawing and {{Understanding Diagrams}}: {{An Accessible Approach Dedicated}} to {{Blind People}}},
  shorttitle = {Drawing and {{Understanding Diagrams}}},
  booktitle  = {Universal {{Access}} in {{Human-Computer Interaction}}. {{User}} and {{Context Diversity}}},
  author     = {Serin, Frederic and Romeo, Katerine},
  editor     = {Antona, Margherita and Stephanidis, Constantine},
  date       = {2022},
  series     = {Lecture {{Notes}} in {{Computer Science}}},
  pages      = {94--109},
  publisher  = {Springer International Publishing},
  location   = {Cham},
  doi        = {10.1007/978-3-031-05039-8_7},
  abstract   = {Our research activity focuses on accessibility for document design and consultation, especially in the field of computer science. In this article, we focus on the design and reading of class diagrams according to the UML standard. Our goal is to enable collaboration between blind and sighted people, which means inclusion and accessibility. We will present the user-centered approach, regardless of the user’s profile. For the blind, we have chosen to use a screen reader, with speech synthesis. For a designer, especially for a visually impaired person, it is essential to perceive the structure of a model, to understand it in all its dimensions. In the case of a UML class diagram, we propose a pattern combining visibility and readability, which we call intelligibility. Our model aims at presenting a simple, complete and fast way to understand an object model.},
  isbn       = {978-3-031-05039-8},
  langid     = {english},
  keywords   = {Accessibility,ARIA,Blindness,Class diagram,Object-oriented design,Screen reader,SVG,UML,Visually impaired persons,WCAG},
  file       = {C:\Users\jseo1005\Zotero\storage\MYT4HSGE\Serin and Romeo - 2022 - Drawing and Understanding Diagrams An Accessible .pdf}
}

@inproceedings{sharifEvoGraphsJQueryPlugin2018,
  title      = {{{evoGraphs}} — {{A jQuery}} Plugin to Create Web Accessible Graphs},
  booktitle  = {2018 15th {{IEEE Annual Consumer Communications}} \& {{Networking Conference}} ({{CCNC}})},
  author     = {Sharif, Ather and Forouraghi, Babak},
  date       = {2018-01},
  pages      = {1--4},
  publisher  = {IEEE},
  location   = {Las Vegas, NV},
  doi        = {10.1109/CCNC.2018.8319239},
  url        = {http://ieeexplore.ieee.org/document/8319239/},
  urldate    = {2022-08-21},
  eventtitle = {2018 15th {{IEEE Annual Consumer Communications}} \& {{Networking Conference}} ({{CCNC}})},
  isbn       = {978-1-5386-4790-5}
}

@inproceedings{sharifUnderstandingScreenReaderUsers2021,
  title     = {Understanding {{Screen-Reader Users}}’ {{Experiences}} with {{Online Data Visualizations}}},
  booktitle = {The 23rd {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author    = {Sharif, Ather and Chintalapati, Sanjana Shivani and Wobbrock, Jacob O. and Reinecke, Katharina},
  date      = {2021-10-17},
  series    = {{{ASSETS}} '21},
  pages     = {1--16},
  publisher = {Association for Computing Machinery},
  location  = {New York, NY, USA},
  doi       = {10.1145/3441852.3471202},
  url       = {https://doi.org/10.1145/3441852.3471202},
  urldate   = {2022-08-21},
  abstract  = {Online data visualizations are widely used to communicate information from simple statistics to complex phenomena, supporting people in gaining important insights from data. However, due to the defining visual nature of data visualizations, extracting information from visualizations can be difficult or impossible for screen-reader users. To assess screen-reader users’ challenges with online data visualizations, we conducted two empirical studies: (1) A qualitative study with nine screen-reader users, and (2) a quantitative study with 36 screen-reader and 36 non-screen-reader users. Our results show that due to the inaccessibility of online data visualizations, screen-reader users extract information 61.48\% less accurately and spend 210.96\% more time interacting with online data visualizations compared to non-screen-reader users. Additionally, our findings show that online data visualizations are commonly indiscoverable to screen readers. In visualizations that are discoverable and comprehensible, screen-reader users suggested tabular and textual representation of data as techniques to improve the accessibility of online visualizations. Taken together, our results provide empirical evidence of the inequalities screen-readers users face in their interaction with online data visualizations.},
  isbn      = {978-1-4503-8306-6},
  keywords  = {challenges,data,screen readers,techniques,visualizations}
}

@software{sharifVoxLens2022,
  title    = {{{VoxLens}}},
  author   = {Sharif, Ather},
  date     = {2022-07-11T20:47:53Z},
  origdate = {2021-10-06T21:56:25Z},
  url      = {https://github.com/athersharif/voxlens},
  urldate  = {2022-08-21},
  abstract = {JavaScript Library to Make Online Data Visualizations Accessible to Screen-Reader Users},
  keywords = {accessibility,blind,dataviz,graph,javascript,visualization}
}

@inproceedings{sharifVoxLensMakingOnline2022,
  title      = {{{VoxLens}}: {{Making Online Data Visualizations Accessible}} with an {{Interactive JavaScript Plug-In}}},
  shorttitle = {{{VoxLens}}},
  booktitle  = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author     = {Sharif, Ather and Wang, Olivia H. and Muongchan, Alida T. and Reinecke, Katharina and Wobbrock, Jacob O.},
  date       = {2022-04-29},
  pages      = {1--19},
  publisher  = {ACM},
  location   = {New Orleans LA USA},
  doi        = {10.1145/3491102.3517431},
  url        = {https://dl.acm.org/doi/10.1145/3491102.3517431},
  urldate    = {2022-08-21},
  eventtitle = {{{CHI}} '22: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn       = {978-1-4503-9157-3},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\L9SM5CPR\Sharif et al. - 2022 - VoxLens Making Online Data Visualizations Accessi.pdf}
}

@inproceedings{siuCOVID19HighlightsIssues2021,
  title      = {{{COVID-19}} Highlights the Issues Facing Blind and Visually Impaired People in Accessing Data on the Web},
  booktitle  = {Proceedings of the 18th {{International Web}} for {{All Conference}}},
  author     = {Siu, Alexa F. and Fan, Danyang and Kim, Gene S-H and Rao, Hrishikesh V. and Vazquez, Xavier and O'Modhrain, Sile and Follmer, Sean},
  date       = {2021-04-19},
  pages      = {1--15},
  publisher  = {ACM},
  location   = {Ljubljana Slovenia},
  doi        = {10.1145/3430263.3452432},
  url        = {https://dl.acm.org/doi/10.1145/3430263.3452432},
  urldate    = {2023-01-12},
  abstract   = {During the COVID-19 pandemic, dissemination of data on the web has been vital in shaping the public’s response. We postulated the increased prominence of data might have exacerbated the accessibility gap for the Blind and Visually Impaired (BVI) community and exposed new inequities. We discuss fndings from a survey (n=127) on data accessibility followed by a contextual inquiry (n=12) of BVI people conducted between June and September 2020. 94\% of survey respondents had concerns about accessing accurate COVID-19 data in a timely manner. Participants described how they encountered broad inaccessibility at early onset of the pandemic, and how advocacy eforts and complimenting their access with a wide range of sources helped fulfll their needs. By examining how BVI users interact with accessible COVID-19 data dashboards, we observed the efect of data literacy, confdence, and modality preferences on user strategies and takeaways. Our observations during this critical period provide an understanding of the impact access or inaccess has on the BVI community in a time of crisis and important implications for improving the technologies and modalities available to disseminate data-driven information accessibly on the web.},
  eventtitle = {{{W4A}} '21: 18th {{Web}} for {{All Conference}}},
  isbn       = {978-1-4503-8212-0},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\6UTD4QYV\Siu et al. - 2021 - COVID-19 highlights the issues facing blind and vi.pdf}
}

@inproceedings{siuSupportingAccessibleData2022,
  title      = {Supporting {{Accessible Data Visualization Through Audio Data Narratives}}},
  booktitle  = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author     = {Siu, Alexa and S-H Kim, Gene and O'Modhrain, Sile and Follmer, Sean},
  date       = {2022-04-29},
  pages      = {1--19},
  publisher  = {ACM},
  location   = {New Orleans LA USA},
  doi        = {10.1145/3491102.3517678},
  url        = {https://dl.acm.org/doi/10.1145/3491102.3517678},
  urldate    = {2023-01-12},
  eventtitle = {{{CHI}} '22: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn       = {978-1-4503-9157-3},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\KG9VB5WG\Siu et al. - 2022 - Supporting Accessible Data Visualization Through A.pdf}
}

@inproceedings{siuSupportingAccessibleData2022b,
  title      = {Supporting {{Accessible Data Visualization Through Audio Data Narratives}}},
  booktitle  = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author     = {Siu, Alexa and S-H Kim, Gene and O'Modhrain, Sile and Follmer, Sean},
  date       = {2022-04-29},
  pages      = {1--19},
  publisher  = {ACM},
  location   = {New Orleans LA USA},
  doi        = {10.1145/3491102.3517678},
  url        = {https://dl.acm.org/doi/10.1145/3491102.3517678},
  urldate    = {2023-09-05},
  eventtitle = {{{CHI}} '22: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn       = {978-1-4503-9157-3},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\I2LXSHBV\Siu et al. - 2022 - Supporting Accessible Data Visualization Through A.pdf}
}

@online{SonificationHandbookEdited,
  title   = {The {{Sonification Handbook}} | Edited by {{Hermann}}, {{Hunt}}, {{Neuhoff}}},
  url     = {https://sonification.de/handbook/},
  urldate = {2022-09-20},
  langid  = {american},
  file    = {C:\Users\jseo1005\Zotero\storage\4T78LXUJ\handbook.html}
}

@online{SparkBrailleBrailleLine,
  title   = {{{SparkBraille}}: {{Braille Line Charts}}},
  url     = {https://fizzstudio.github.io/sparkbraille/},
  urldate = {2023-11-16},
  file    = {C:\Users\jseo1005\Zotero\storage\VBN5YC75\sparkbraille.html}
}

@inproceedings{srinivasanAzimuthDesigningAccessible2023,
  title      = {Azimuth: {{Designing Accessible Dashboards}} for {{Screen Reader Users}}},
  shorttitle = {Azimuth},
  booktitle  = {Proceedings of the 25th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author     = {Srinivasan, Arjun and Harshbarger, Tim and Hilliker, Darrell and Mankoff, Jennifer},
  date       = {2023-10-22},
  series     = {{{ASSETS}} '23},
  pages      = {1--16},
  publisher  = {Association for Computing Machinery},
  location   = {New York, NY, USA},
  doi        = {10.1145/3597638.3608405},
  url        = {https://dl.acm.org/doi/10.1145/3597638.3608405},
  urldate    = {2023-11-01},
  abstract   = {Dashboards are frequently used to monitor and share data across a breadth of domains including business, finance, sports, public policy, and healthcare, just to name a few. The combination of different components (e.g., key performance indicators, charts, filtering widgets) and the interactivity between components makes dashboards powerful interfaces for data monitoring and analysis. However, these very characteristics also often make dashboards inaccessible to blind and low vision (BLV) users. Through a co-design study with two screen reader users, we investigate challenges faced by BLV users and identify design goals to support effective screen reader-based interactions with dashboards. Operationalizing the findings from the co-design process, we present a prototype system, Azimuth, that generates dashboards optimized for screen reader-based navigation along with complementary descriptions to support dashboard comprehension and interaction. Based on a follow-up study with five BLV participants, we showcase how our generated dashboards support BLV users and enable them to perform both targeted and open-ended analysis. Reflecting on our design process and study feedback, we discuss opportunities for future work on supporting interactive data analysis, understanding dashboard accessibility at scale, and investigating alternative devices and modalities for designing accessible visualization dashboards.},
  isbn       = {9798400702204},
  keywords   = {Dashboards,screen readers,text generation},
  file       = {C:\Users\jseo1005\Zotero\storage\QWMZIF9G\Srinivasan et al. - 2023 - Azimuth Designing Accessible Dashboards for Scree.pdf}
}

@inproceedings{srinivasanCollectingCharacterizingNatural2021,
  title      = {Collecting and {{Characterizing Natural Language Utterances}} for {{Specifying Data Visualizations}}},
  booktitle  = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author     = {Srinivasan, Arjun and Nyapathy, Nikhila and Lee, Bongshin and Drucker, Steven M. and Stasko, John},
  date       = {2021-05-06},
  pages      = {1--10},
  publisher  = {ACM},
  location   = {Yokohama Japan},
  doi        = {10.1145/3411764.3445400},
  url        = {https://dl.acm.org/doi/10.1145/3411764.3445400},
  urldate    = {2022-08-21},
  eventtitle = {{{CHI}} '21: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn       = {978-1-4503-8096-6},
  langid     = {english},
  file       = {C:\Users\jseo1005\Zotero\storage\BLM3Z6L4\Srinivasan et al. - 2021 - Collecting and Characterizing Natural Language Utt.pdf}
}

@online{StateDiagramVisualizing,
  title        = {State {{Diagram}}: {{Visualizing}} the {{Behavior}} and {{Transitions}} of {{Systems}}},
  shorttitle   = {State {{Diagram}}},
  url          = {https://start-up.house/inventory/state-diagram},
  urldate      = {2023-09-07},
  abstract     = {State diagram: a visual representation of system behaviors and transitions.},
  langid       = {english},
  organization = {Startup House}
}

@article{stefikBESTPAPERSIGCSE2019,
  title        = {{{BEST PAPER AT SIGCSE}} 2019 {{IN THE EXPERIENCE REPORTS AND TOOLS TRACK}}: {{Computer}} Science Principles for Teachers of Blind and Visually Impaired Students},
  shorttitle   = {{{BEST PAPER AT SIGCSE}} 2019 {{IN THE EXPERIENCE REPORTS AND TOOLS TRACK}}},
  author       = {Stefik, Andreas and Ladner, Richard E. and Allee, William and Mealin, Sean},
  date         = {2019-04-25},
  journaltitle = {ACM Inroads},
  shortjournal = {ACM Inroads},
  volume       = {10},
  number       = {2},
  pages        = {50--57},
  issn         = {2153-2184, 2153-2192},
  doi          = {10.1145/3324894},
  url          = {https://dl.acm.org/doi/10.1145/3324894},
  urldate      = {2024-04-04},
  langid       = {english},
  file         = {C:\Users\jseo1005\Zotero\storage\ZKBNY8IZ\Stefik et al. - 2019 - BEST PAPER AT SIGCSE 2019 IN THE EXPERIENCE REPORTS AND TOOLS TRACK Computer science principles for.pdf}
}

@article{stephensAccessibilityParticipatoryDesign2023,
  title        = {Accessibility and Participatory Design: Time, Power, and Facilitation},
  shorttitle   = {Accessibility and Participatory Design},
  author       = {Stephens, Lindsay and Smith, Hilda and Epstein, Iris and Baljko, Melanie and Mcintosh, Ian and Dadashi, Nastaran and Prakash, Devika Narayani},
  date         = {2023-10-02},
  journaltitle = {CoDesign},
  volume       = {19},
  number       = {4},
  pages        = {287--303},
  publisher    = {Taylor \& Francis},
  issn         = {1571-0882},
  doi          = {10.1080/15710882.2023.2214145},
  url          = {https://doi.org/10.1080/15710882.2023.2214145},
  urldate      = {2024-04-23},
  abstract     = {This paper documents the goals, techniques, and outcomes of nine interventions designed to improve the accessibility of a design charette (DC). These interventions focused on Time, Power, and Facilitation and were developed based on critiques found in design literature, critical disability scholarship, and the lived expertise of disabled people. Data was collected through recording activities and outputs, recorded observations, and elicited feedback. We found that adjusting time, which is essential for access, was difficult and required trade-offs. We also suggest that the presence of a ‘vibes watch’ facilitation role to monitor participation frequency, emotional tone, and power dynamics can be useful to address uneven power relations, caucusing can also be valuable but should be used at specific moments. Non-neutral facilitation, anti-oppression training, and regular reflection can help facilitation/design teams identify and address exclusionary practices. Technology can aid but also constrain access. Finally, despite all interventions, access remains a site of friction and political choices. Stakeholders continue to participate in different and not always equally valued ways, so secondary analysis is useful for understanding charette products or outputs.},
  keywords     = {accessibility,design charette,Disability,evaluation,participatory design,practicum education}
}

@online{sturdevantDeliveringDataDifferently2022,
  title       = {Delivering Data Differently},
  author      = {Sturdevant, Gwynn and Godfrey, A. Jonathan R. and Gelman, Andrew},
  date        = {2022-04-14},
  eprint      = {2204.10854},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/2204.10854},
  urldate     = {2024-02-06},
  abstract    = {Human-computer interaction relies on mouse/touchpad, keyboard, and screen, but tools have recently been developed that engage sound, smell, touch, muscular resistance, voice dialogue, balance, and multiple senses at once. How might these improvements impact upon the practice of statistics and data science? People with low vision may be better able to grasp and explore data. More generally, methods developed to enable this have the potential to allow sighted people to use more senses and become better analysts. We would like to adapt some of the wide range of available computer and sensory input/output technologies to transform data science workflows. Here is a vision of what this synthesis might accomplish.},
  pubstate    = {preprint},
  keywords    = {Computer Science - Human-Computer Interaction},
  file        = {C\:\\Users\\jseo1005\\Zotero\\storage\\LC6MR9DI\\Sturdevant et al. - 2022 - Delivering data differently.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\I9UD6BDE\\2204.html}
}

@inproceedings{suhDevelopingValidatingUser2016,
  title      = {Developing and {{Validating}} the {{User Burden Scale}}: {{A Tool}} for {{Assessing User Burden}} in {{Computing Systems}}},
  shorttitle = {Developing and {{Validating}} the {{User Burden Scale}}},
  booktitle  = {Proceedings of the 2016 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author     = {Suh, Hyewon and Shahriaree, Nina and Hekler, Eric B. and Kientz, Julie A.},
  date       = {2016-05-07},
  series     = {{{CHI}} '16},
  pages      = {3988--3999},
  publisher  = {Association for Computing Machinery},
  location   = {New York, NY, USA},
  doi        = {10.1145/2858036.2858448},
  url        = {https://doi.org/10.1145/2858036.2858448},
  urldate    = {2022-12-16},
  abstract   = {Computing systems that place a high level of burden on their users can have a negative affect on initial adoption, retention, and overall user experience. Through an iterative process, we have developed a model for user burden that consists of six constructs: 1) difficulty of use, 2) physical, 3) time and social, 4) mental and emotional, 5) privacy, and 6) financial. If researchers and practitioners can have an understanding of the overall level of burden systems may be having on the user, they can have a better sense of whether and where to target future design efforts that can reduce those burdens. To help assist with understanding and measuring user burden, we have also developed and validated a measure of user burden in computing systems called the User Burden Scale (UBS), which is a 20-item scale with 6 individual sub-scales representing each construct. This paper presents the process we followed to develop and validate this scale for use in evaluating user burden in computing systems. Results indicate that the User Burden Scale has good overall inter-item reliability, convergent validity with similar scales, and concurrent validity when compared to systems abandoned vs. those still in use.},
  isbn       = {978-1-4503-3362-7},
  keywords   = {evaluation,measuring usability,technology abandonment,usability,user burden,user experience,validated measures},
  file       = {C:\Users\jseo1005\Zotero\storage\X4KP6UF5\Suh et al. - 2016 - Developing and Validating the User Burden Scale A.pdf}
}

@article{summersAccessibilityODSGraphics,
  title    = {Accessibility and {{ODS Graphics}}: {{Seven Simple Steps}} to {{Section}} 508 {{Compliance Using SAS}}® 9.{{4M5}}},
  author   = {Summers, Ed and Langston, Julianna and Heath, Dan},
  pages    = {13},
  abstract = {How do you create data visualizations that comply with the Section 508 amendment to the United States Workforce Rehabilitation Act, the Web Content Accessibility Guidelines (WCAG), and other accessibility standards? It’s easy when you use the new Output Delivery System (ODS) Graphics accessibility features in SAS® 9.4M5. This paper defines seven simple steps to create accessible data visualizations. The accessibility requirements that are satisfied by each step are explained, and additional references are provided. It includes sample code for real-world examples that has been tested by the SAS® accessibility team. It also includes a handy one-page checklist that you can print separately for future reference.},
  langid   = {english},
  file     = {C:\Users\jseo1005\Zotero\storage\9X2W8VD8\Summers et al. - Accessibility and ODS Graphics Seven Simple Steps.pdf}
}

@patent{summersAccessibleDataVisualizations2014,
  type     = {patentus},
  title    = {Accessible {{Data Visualizations}} for {{Visually Impaired Users}}},
  author   = {Summers, II Claude Edward and JR, Robert E. Allison and Langston, Julianna Elizabeth and Cowley, Jennifer Antonia},
  holder   = {{SAS Institute Inc}},
  date     = {2014-02-20},
  number   = {20140053055A1},
  url      = {https://patents.google.com/patent/US20140053055/en},
  urldate  = {2022-08-19},
  langid   = {english},
  keywords = {accessibility information,boundary,graphic,region,visualization},
  file     = {C:\Users\jseo1005\Zotero\storage\UGHK8CJK\Summers et al. - 2014 - Accessible Data Visualizations for Visually Impair.pdf}
}

@patent{summersConvertingGraphicalDatavisualizations2019,
  type     = {patentus},
  title    = {Converting Graphical Data-Visualizations into Sonified Output},
  author   = {Summers, II Claude Edward and Langston, Julianna Elizabeth and Sookne, Jesse Daniel and Olley, Jesse Benjamin and Trout, Kerry Leanne Smith and IV, Cleester Daniel Heath and Kalat, Samuel Edward and Layne, Paul William},
  holder   = {{SAS Institute Inc}},
  date     = {2019-01-29},
  number   = {10191979B2},
  url      = {https://patents.google.com/patent/US10191979B2/en},
  urldate  = {2022-08-19},
  langid   = {english},
  keywords = {data,data point,output,processing device,sonified},
  file     = {C:\Users\jseo1005\Zotero\storage\MGDD7QSW\Summers et al. - 2019 - Converting graphical data-visualizations into soni.pdf}
}

@patent{summersUserInterfacesConverting2022,
  type     = {patentus},
  title    = {User Interfaces for Converting Geospatial Data into Audio Outputs},
  author   = {Summers, II Claude Edward and Mealin, Sean Patrick and Langston, Julianna Elizabeth and Kraus, Gregory David and Williamson, Jonathan Tyler and Robinson, Lisa Beth Morton and Sookne, Jesse Daniel and Smith, Brice Joseph},
  holder   = {{SAS Institute Inc}},
  date     = {2022-02-22},
  number   = {11257396B2},
  url      = {https://patents.google.com/patent/US11257396B2/en/},
  urldate  = {2022-09-21},
  langid   = {english},
  keywords = {data,map,user,virtual,virtual map},
  file     = {C:\Users\jseo1005\Zotero\storage\5LC4C6VM\Summers et al. - 2022 - User interfaces for converting geospatial data int.pdf}
}

@book{SUSQuickDirty1996,
  title        = {{{SUS}}: {{A}} '{{Quick}} and {{Dirty}}' {{Usability Scale}}},
  shorttitle   = {{{SUS}}},
  date         = {1996-06-11},
  journaltitle = {Usability Evaluation In Industry},
  pages        = {207--212},
  publisher    = {CRC Press},
  doi          = {10.1201/9781498710411-35},
  url          = {https://www.taylorfrancis.com/chapters/edit/10.1201/9781498710411-35/sus-quick-dirty-usability-scale-john-brooke},
  urldate      = {2022-12-16},
  abstract     = {Usability is not a quality that exists in any real or absolute sense. Perhaps it can be best summed up as being a general quality of the appropriateness to a purpose of any particular artefact. This notion is neatly summed up by Terry Pratchett in his novel Moving Pictures:In just the same way, the usability of any tool or system has to be viewed in terms of the context in which it is used, and its appropriateness to that context. With particular reference to information systems, this view of usability is reflected in the current draft international standard ISO 9241-11 and in the European Community ESPRIT project MUSiC (Measuring Usability of Systems in Context) (e.g. Bevan et al., 1991). In general, it is impossible to specify the usability of a system (i.e. its fitness for purpose) without first defining who are the intended users of the system, the tasks those users will perform with it, and the characteristics of the physical, organizational and social environment in which it will be used.},
  isbn         = {978-0-429-15701-1},
  langid       = {english}
}

@article{swellerCognitiveLoadProblem1988,
  title        = {Cognitive Load during Problem Solving: {{Effects}} on Learning},
  shorttitle   = {Cognitive Load during Problem Solving},
  author       = {Sweller, John},
  date         = {1988-04-01},
  journaltitle = {Cognitive Science},
  shortjournal = {Cognitive Science},
  volume       = {12},
  number       = {2},
  pages        = {257--285},
  issn         = {0364-0213},
  doi          = {10.1016/0364-0213(88)90023-7},
  url          = {https://www.sciencedirect.com/science/article/pii/0364021388900237},
  urldate      = {2023-01-17},
  abstract     = {Considerable evidence indicates that domain specific knowledge in the form of schemas is the primary factor distinguishing experts from novices in problem-solving skill. Evidence that conventional problem-solving activity is not effective in schema acquisition is also accumulating. It is suggested that a major reason for the ineffectiveness of problem solving as a learning device, is that the cognitive processes required by the two activities overlap insufficiently, and that conventional problem solving in the form of means-ends analysis requires a relatively large amount of cognitive processing capacity which is consequently unavailable for schema acquisition. A computational model and experimental evidence provide support for this contention. Theoretical and practical implications are discussed.},
  langid       = {english},
  file         = {C\:\\Users\\jseo1005\\Zotero\\storage\\EKMKBBLY\\Sweller - 1988 - Cognitive load during problem solving Effects on .pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\WMBAQZBS\\0364021388900237.html}
}

@article{swellerElementInteractivityIntrinsic2010,
  title        = {Element {{Interactivity}} and {{Intrinsic}}, {{Extraneous}}, and {{Germane Cognitive Load}}},
  author       = {Sweller, John},
  date         = {2010-06-01},
  journaltitle = {Educational Psychology Review},
  shortjournal = {Educ Psychol Rev},
  volume       = {22},
  number       = {2},
  pages        = {123--138},
  issn         = {1573-336X},
  doi          = {10.1007/s10648-010-9128-5},
  url          = {https://doi.org/10.1007/s10648-010-9128-5},
  urldate      = {2023-01-18},
  abstract     = {In cognitive load theory, element interactivity has been used as the basic, defining mechanism of intrinsic cognitive load for many years. In this article, it is suggested that element interactivity underlies extraneous cognitive load as well. By defining extraneous cognitive load in terms of element interactivity, a distinct relation between intrinsic and extraneous cognitive load can be established based on whether element interactivity is essential to the task at hand or whether it is a function of instructional procedures. Furthermore, germane cognitive load can be defined in terms of intrinsic cognitive load, thus also associating germane cognitive load with element interactivity. An analysis of the consequences of explaining the various cognitive load effects in terms of element interactivity is carried out.},
  langid       = {english},
  keywords     = {Cognitive load theory,Element interactivity,Extraneous cognitive load,Germane cognitive load,Intrinsic cognitive load},
  file         = {C:\Users\jseo1005\Zotero\storage\CBDT7FY5\Sweller - 2010 - Element Interactivity and Intrinsic, Extraneous, a.pdf}
}

@online{tangVisTextBenchmarkSemantically2023,
  title       = {{{VisText}}: {{A Benchmark}} for {{Semantically Rich Chart Captioning}}},
  shorttitle  = {{{VisText}}},
  author      = {Tang, Benny J. and Boggust, Angie and Satyanarayan, Arvind},
  date        = {2023-06-28},
  eprint      = {2307.05356},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/2307.05356},
  urldate     = {2024-03-25},
  abstract    = {Captions that describe or explain charts help improve recall and comprehension of the depicted data and provide a more accessible medium for people with visual disabilities. However, current approaches for automatically generating such captions struggle to articulate the perceptual or cognitive features that are the hallmark of charts (e.g., complex trends and patterns). In response, we introduce VisText: a dataset of 12,441 pairs of charts and captions that describe the charts' construction, report key statistics, and identify perceptual and cognitive phenomena. In VisText, a chart is available as three representations: a rasterized image, a backing data table, and a scene graph -- a hierarchical representation of a chart's visual elements akin to a web page's Document Object Model (DOM). To evaluate the impact of VisText, we fine-tune state-of-the-art language models on our chart captioning task and apply prefix-tuning to produce captions that vary the semantic content they convey. Our models generate coherent, semantically rich captions and perform on par with state-of-the-art chart captioning models across machine translation and text generation metrics. Through qualitative analysis, we identify six broad categories of errors that our models make that can inform future work.},
  pubstate    = {preprint},
  keywords    = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file        = {C\:\\Users\\jseo1005\\Zotero\\storage\\Z8PRE6Y8\\Tang et al. - 2023 - VisText A Benchmark for Semantically Rich Chart C.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\Z88FWG9U\\2307.html}
}

@online{taptapseeTapTapSeeBlindVisually,
  title   = {{{TapTapSee}} - {{Blind}} and {{Visually Impaired Assistive Technology}} - Powered by {{CloudSight}}.Ai {{Image Recognition API}}},
  author  = {{TapTapSee}},
  url     = {https://taptapseeapp.com/},
  urldate = {2024-04-20},
  file    = {C:\Users\jseo1005\Zotero\storage\Y5RS2XQY\taptapseeapp.com.html}
}

@article{tennisonBridgingGapGraphical2023,
  title        = {Bridging the {{Gap}} of {{Graphical Information Accessibility}} in {{Education With Multimodal Touchscreens Among Students With Blindness}} and {{Low Vision}}},
  author       = {Tennison, Jennifer L. and Goswami, Spondita and Hairston, Jesse R. and Merlin Drews, P. and Smith, Derrick W. and Giudice, Nicholas A. and Stefik, Andreas and Gorlewicz, Jenna L.},
  date         = {2023-11-01},
  journaltitle = {Journal of Visual Impairment \& Blindness},
  volume       = {117},
  number       = {6},
  pages        = {453--466},
  publisher    = {SAGE Publications Inc},
  issn         = {0145-482X},
  doi          = {10.1177/0145482X231217496},
  url          = {https://doi.org/10.1177/0145482X231217496},
  urldate      = {2024-01-15},
  abstract     = {Introduction: Informational graphics and data representations (e.g., charts and figures) are critical for accessing educational content. Novel technologies, such as the multimodal touchscreen which displays audio, haptic, and visual information, are promising for being platforms of diverse means to access digital content. This work evaluated educational graphics rendered on a touchscreen compared to the current standard for accessing graphical content. Method: Three bar charts and geometry figures were evaluated on student (N\,=\,20) ability to orient to and extract information from the touchscreen and print. Participants explored the graphics and then were administered a set of questions (11–12 depending on graphic group). In addition, participants’ attitudes using the mediums were assessed. Results: Participants performed statistically significantly better on questions assessing information orientation using the touchscreen than print for both bar chart and geometry figures. No statistically significant difference in information extraction ability was found between mediums on either graphic type. Participants responded significantly more favorably to the touchscreen than the print graphics, indicating them as more helpful, interesting, fun, and less confusing. Discussion: Accessing and orienting to information was highly successful by participants using the touchscreen, and was the preferred means of accessing graphical information when compared to the print image for both geometry figures and bar charts. This study highlights challenges in presenting graphics both on touchscreens and in print. Implications for Practitioners: This study offers preliminary support for the use of multimodal, touchscreen tablets as educational tools. Student ability using touchscreen-based graphics seems to be comparable to traditional types of graphics (large print and embossed, tactile graphics), although further investigation may be necessary for tactile graphic users. In summary, educators of students with blindness and visual impairments should consider ways to utilize new technologies, such as touchscreens, to provide more diverse access to graphical information.},
  langid       = {english},
  file         = {C:\Users\jseo1005\Zotero\storage\SP9F7WGB\Tennison et al. - 2023 - Bridging the Gap of Graphical Information Accessibility in Education With Multimodal Touchscreens Am.pdf}
}

@online{theconsumerfinancialprotectionbureauWelcomeCFPBDesign2024,
  title   = {Welcome to the {{CFPB Design System}} - {{CFPB Design System}}},
  author  = {{The Consumer Financial Protection Bureau}},
  date    = {2024},
  url     = {https://cfpb.github.io/design-system/},
  urldate = {2024-04-21},
  file    = {C:\Users\jseo1005\Zotero\storage\844W4DWF\design-system.html}
}

@online{thediagramcenterSpecificGuidelinesGraphs2009,
  title    = {Specific {{Guidelines}} - {{Graphs}}},
  author   = {{The Diagram Center}},
  date     = {2009},
  url      = {http://diagramcenter.org/specific-guidelines-e.html/},
  urldate  = {2024-04-21},
  abstract = {{$<<$} Previous section: Diagrams (Relational) | Main Table of Contents | Next: Section F (Maps) {$>>$} Table of Contents for Section E E. Graphs 1. Bar Graphs 1a. Vertical 1b. Horizontal 1c. Double 2. Line Graphs 3. Pie Graphs 4. Scatter Plots Graphs 1. Bar Graphs 1a. Vertical Bar Graphs © NCAM 2009 Guidelines: Bar graphs should be converted into […]},
  langid   = {american},
  file     = {C:\Users\jseo1005\Zotero\storage\USDZK68X\specific-guidelines-e.html}
}

@inproceedings{thompsonChartReaderAccessible2023,
  title      = {Chart {{Reader}}: {{Accessible Visualization Experiences Designed}} with {{Screen Reader Users}}},
  shorttitle = {Chart {{Reader}}},
  booktitle  = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author     = {Thompson, John R and Martinez, Jesse J and Sarikaya, Alper and Cutrell, Edward and Lee, Bongshin},
  date       = {2023-04-19},
  series     = {{{CHI}} '23},
  pages      = {1--18},
  publisher  = {Association for Computing Machinery},
  location   = {New York, NY, USA},
  doi        = {10.1145/3544548.3581186},
  url        = {https://dl.acm.org/doi/10.1145/3544548.3581186},
  urldate    = {2023-04-25},
  abstract   = {Even though screen readers are a core accessibility tool for blind and low vision individuals (BLVIs), most visualizations are incompatible with screen readers. To improve accessible visualization experiences, we partnered with 10 BLV screen reader users (SRUs) in an iterative co-design study to design and develop accessible visualization experiences that afford SRUs the autonomy to interactively read and understand visualizations and their underlying data. During the five-month study, we explored accessible visualization prototypes with our design partners for three one-hour sessions. Our results provide feedback on the synthesized design concepts we explored, why (or why not) they aid comprehension and exploration for SRUs, and how differing design concepts can fit into cohesive accessible visualization experiences. We contribute both Chart Reader, a web-based accessibility engine resulting from our design iterations, and our distilled study findings—organized by design dimensions—in the creation of comprehensive accessible visualization experiences.},
  isbn       = {978-1-4503-9421-5},
  keywords   = {accessibility,accessibility engine,accessible visualization experiences,blind and low vision,data visualization,iterative co-design,screen readers},
  file       = {C:\Users\jseo1005\Zotero\storage\DHS55RVS\Thompson et al. - 2023 - Chart Reader Accessible Visualization Experiences.pdf}
}

@software{TidyverseDatasciencebox2024,
  title        = {Tidyverse/Datascience-Box},
  date         = {2024-04-14T02:12:54Z},
  origdate     = {2017-12-29T22:16:17Z},
  url          = {https://github.com/tidyverse/datascience-box},
  urldate      = {2024-04-14},
  abstract     = {Data Science Course in a Box},
  organization = {tidyverse},
  keywords     = {data-science,education,r,rstats,teaching}
}

@article{touvronLlamaOpenFoundation2023,
  title      = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  shorttitle = {Llama 2},
  author     = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  date       = {2023},
  publisher  = {arXiv},
  doi        = {10.48550/ARXIV.2307.09288},
  url        = {https://arxiv.org/abs/2307.09288},
  urldate    = {2023-11-18},
  abstract   = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  version    = {2},
  keywords   = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),FOS: Computer and information sciences},
  file       = {C:\Users\jseo1005\Zotero\storage\FWW6TYP6\Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf}
}

@online{UmweltAccessibleStructured,
  title   = {Umwelt: {{Accessible Structured Editing}} of {{Multi-Modal Data Representations}} | {{Proceedings}} of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  url     = {https://dl.acm.org/doi/10.1145/3613904.3641996},
  urldate = {2024-05-15},
  file    = {C:\Users\jseo1005\Zotero\storage\QCWII7KT\3613904.html}
}

@online{UmweltAccessibleStructureda,
  title   = {Umwelt: {{Accessible Structured Editing}} of {{Multi-Modal Data Representations}} | {{Proceedings}} of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  url     = {https://dl.acm.org/doi/10.1145/3613904.3641996},
  urldate = {2024-05-15},
  file    = {C:\Users\jseo1005\Zotero\storage\7HP2ACAU\3613904.html}
}

@article{UnicodeStandardVersion,
  title  = {The {{Unicode Standard}}, {{Version}} 15.0},
  langid = {english},
  file   = {C:\Users\jseo1005\Zotero\storage\3CGNFLBR\The Unicode Standard, Version 15.0.pdf}
}

@article{vinesSonificationNumericalData2019,
  title        = {Sonification of Numerical Data for Education},
  author       = {Vines, Karen and Hughes, Chris and Alexander, Laura and Calvert, Carol and Colwell, Chetz and Holmes, Hilary and Kotecki, Claire and Parks, Kaela and Pearson, Victoria},
  date         = {2019-01-13},
  journaltitle = {Open Learning: The Journal of Open, Distance and e-Learning},
  publisher    = {Routledge},
  issn         = {0268-0513},
  url          = {https://www.tandfonline.com/doi/full/10.1080/02680513.2018.1553707},
  urldate      = {2022-12-16},
  abstract     = {Sonification is the use of non-speech audio to convey information. In this article, sonifications are representations of plots aimed at improving the accessibility of teaching materials. The electr...},
  langid       = {english},
  file         = {C:\Users\jseo1005\Zotero\storage\GQVU6QIM\02680513.2018.html}
}

@software{VisaChartComponents2022,
  title        = {Visa {{Chart Components}}},
  date         = {2022-10-15T03:25:41Z},
  origdate     = {2020-11-11T01:29:46Z},
  url          = {https://github.com/visa/visa-chart-components},
  urldate      = {2022-10-20},
  abstract     = {Visa Chart Components (VCC) is an accessibility focused, framework agnostic set of data experience design systems components for the web. VCC attempts to provide a toolset to enable developers to build equal data experiences for everyone, everywhere.},
  organization = {Visa},
  keywords     = {accessibility,charts,d3,data-experience,data-visualization,graphs,stencil,visualization}
}

@online{VisualizationBlindACMInteractions,
  title   = {Visualization for the {{BlindACM Interactions}}},
  url     = {https://interactions.acm.org/archive/view/january-february-2023/visualization-for-the-blind},
  urldate = {2023-01-15},
  file    = {C:\Users\jseo1005\Zotero\storage\BH8LR49A\visualization-for-the-blind.html}
}

@article{walkerMappingsMetaphorsAuditory2005,
  title        = {Mappings and Metaphors in Auditory Displays: {{An}} Experimental Assessment},
  shorttitle   = {Mappings and Metaphors in Auditory Displays},
  author       = {Walker, Bruce N. and Kramer, Gregory},
  date         = {2005-10-01},
  journaltitle = {ACM Transactions on Applied Perception},
  shortjournal = {ACM Trans. Appl. Percept.},
  volume       = {2},
  number       = {4},
  pages        = {407--412},
  issn         = {1544-3558},
  doi          = {10.1145/1101530.1101534},
  url          = {https://doi.org/10.1145/1101530.1101534},
  urldate      = {2022-10-17},
  abstract     = {Auditory displays are becoming more and more common, but there are still no general guidelines for mapping data dimensions (e.g., temperature) onto display dimensions (e.g., pitch). This paper presents experimental research on different mappings and metaphors, in a generic process-control task environment, with reaction time and accuracy as dependent measures. It is hoped that this area of investigation will lead to the development of mapping guidelines applicable to auditory displays in a wide range of task domains.},
  keywords     = {auditory display,data mapping,guidelines,metaphors,Sonification}
}

@article{walkerSpearconsSpeechbasedEarcons2013,
  title        = {Spearcons (Speech-Based Earcons) Improve Navigation Performance in Advanced Auditory Menus},
  author       = {Walker, Bruce N. and Lindsay, Jeffrey and Nance, Amanda and Nakano, Yoko and Palladino, Dianne K. and Dingler, Tilman and Jeon, Myounghoon},
  date         = {2013-02},
  journaltitle = {Human Factors},
  shortjournal = {Hum Factors},
  volume       = {55},
  number       = {1},
  eprint       = {23516800},
  eprinttype   = {pmid},
  pages        = {157--182},
  issn         = {0018-7208},
  doi          = {10.1177/0018720812450587},
  abstract     = {OBJECTIVE: The goal of this project is to evaluate a new auditory cue, which the authors call spearcons, in comparison to other auditory cues with the aim of improving auditory menu navigation. BACKGROUND: With the shrinking displays of mobile devices and increasing technology use by visually impaired users, it becomes important to improve usability of non-graphical user interface (GUI) interfaces such as auditory menus. Using nonspeech sounds called auditory icons (i.e., representative real sounds of objects or events) or earcons (i.e., brief musical melody patterns) has been proposed to enhance menu navigation. To compensate for the weaknesses of traditional nonspeech auditory cues, the authors developed spearcons by speeding up a spoken phrase, even to the point where it is no longer recognized as speech. METHOD: The authors conducted five empirical experiments. In Experiments 1 and 2, they measured menu navigation efficiency and accuracy among cues. In Experiments 3 and 4, they evaluated learning rate of cues and speech itself. In Experiment 5, they assessed spearcon enhancements compared to plain TTS (text to speech: speak out written menu items) in a two-dimensional auditory menu. RESULTS: Spearcons outperformed traditional and newer hybrid auditory cues in navigation efficiency, accuracy, and learning rate. Moreover, spearcons showed comparable learnability as normal speech and led to better performance than speech-only auditory cues in two-dimensional menu navigation. CONCLUSION: These results show that spearcons can be more effective than previous auditory cues in menu-based interfaces. APPLICATION: Spearcons have broadened the taxonomy of nonspeech auditory cues. Users can benefit from the application of spearcons in real devices.},
  langid       = {english},
  keywords     = {Acoustic Stimulation,Adolescent,Analysis of Variance,Auditory Perception,Cell Phone,Computers Handheld,Cues,Data Display,Female,Humans,Male,Sound,Speech,User-Computer Interface,Young Adult}
}

@article{wassermanExtendingStateTransition1985,
  title        = {Extending {{State Transition Diagrams}} for the {{Specification}} of {{Human}}–{{Computer Interaction}}},
  author       = {Wasserman, A.I.},
  date         = {1985-08},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume       = {SE-11},
  number       = {8},
  pages        = {699--713},
  issn         = {1939-3520},
  doi          = {10.1109/TSE.1985.232519},
  abstract     = {User Software Engineering is a methodology for the specification and implementation of interactive information systems. An early step in the methodology is the creation of a formal executable description of the user interaction with the system, based on augmented state transition diagrams. This paper shows the derivation of the USE transition diagrams based on perceived shortcomings of the "pure" state transition diagram approach. In this way, the features of the USE specification notation are gradually presented and illustrated. The paper shows both the graphical notation and the textual equivalent of the notation, and briefly describes the automated tools that support direct execution of the specification.},
  eventtitle   = {{{IEEE Transactions}} on {{Software Engineering}}},
  keywords     = {Command languages,Database languages,Design methodology,Executable specifications,Information science,Information systems,interactive information systems,Interactive systems,Programming,rapid prototyping,software development methodology,Software engineering,Software prototyping,transition diagrams,user interfaces,User interfaces,User Software Engineering},
  file         = {C:\Users\jseo1005\Zotero\storage\HXFH2CCB\Wasserman - 1985 - Extending State Transition Diagrams for the Specif.pdf}
}

@online{webaccessibilityinitiativeComplexImages2022,
  title    = {Complex {{Images}}},
  author   = {{Web Accessibility Initiative}},
  date     = {2022},
  url      = {https://www.w3.org/WAI/tutorials/images/complex/},
  urldate  = {2024-04-21},
  abstract = {Accessibility resources free online from the international standards organization: W3C Web Accessibility Initiative (WAI).},
  langid   = {english},
  file     = {C:\Users\jseo1005\Zotero\storage\USMD4SDG\complex.html}
}

@online{WebAIMAlternativeText,
  title   = {{{WebAIM}}: {{Alternative Text}}},
  url     = {https://webaim.org/techniques/alttext/},
  urldate = {2024-04-21},
  file    = {C:\Users\jseo1005\Zotero\storage\8L94BX85\alttext.html}
}

@online{webaimWebAIMWebAIMMillion2024,
  title   = {{{WebAIM}}: {{The WebAIM Million}} - {{The}} 2024 Report on the Accessibility of the Top 1,000,000 Home Pages},
  author  = {{WebAIM}},
  date    = {2024},
  url     = {https://webaim.org/projects/million/#labels},
  urldate = {2024-04-21},
  file    = {C:\Users\jseo1005\Zotero\storage\NBSDV2E8\million.html}
}

@online{WebContentAccessibility,
  title   = {Web {{Content Accessibility Guidelines}} ({{WCAG}}) 2.1},
  url     = {https://www.w3.org/TR/WCAG21/},
  urldate = {2024-04-21},
  file    = {C:\Users\jseo1005\Zotero\storage\7FFW8GE6\WCAG21.html}
}

@online{westArtScienceAudio1995,
  type         = {webpage},
  title        = {The {{Art}} and {{Science}} of {{Audio Book Production}}},
  author       = {West, Billy},
  date         = {1995},
  url          = {https://www.loc.gov/nls/who-we-are/guidelines-and-specifications/the-art-and-science-of-audio-book-production/},
  urldate      = {2024-04-21},
  abstract     = {Over the years, many organizations have begun audio book production programs. Some have been successful, some have not advanced beyond the novice level, and some have failed completely.},
  langid       = {english},
  organization = {National Library Service for the Blind and Print Disabled (NLS) | Library of Congress},
  file         = {C:\Users\jseo1005\Zotero\storage\YCSYPFGS\the-art-and-science-of-audio-book-production.html}
}

@article{westermannEffectSpatialSeparation2015,
  title        = {The Effect of Spatial Separation in Distance on the Intelligibility of Speech in Rooms},
  author       = {Westermann, A. and Buchholz, J.M.},
  date         = {2015},
  journaltitle = {Journal of the Acoustical Society of America},
  volume       = {137},
  number       = {2},
  pages        = {757--767},
  issn         = {0001-4966},
  doi          = {10.1121/1.4906581},
  abstract     = {The influence of spatial separation in source distance on speech reception thresholds (SRTs) is investigated. In one scenario, the target was presented at 0.5 m distance, and the masker varied from 0.5 m distance up to 10 m. In a second scenario, the masker was presented at 0.5 m distance and the target distance varied. The stimuli were synthesized using convolution with binaural room impulse responses (BRIRs) measured on a dummy head in a reverberant auditorium, and were equalized to compensate for distance-dependent spectral and intensity changes. All sources were simulated directly in front of the listener. SRTs decreased monotonically when the target was at 0.5 m and the speech-masker was moved further away, resulting in a SRT improvement of up to 10 dB. When the speech masker was at 0.5 m and the target was moved away, a large variation across subjects was observed. Neither short-term signal-to-noise ratio (SNR) improvements nor cross-ear glimpsing could account for the observed improvement in intelligibility. However, the effect might be explained by an improvement in the SNR in the modulation domain and a decrease in informational masking. This study demonstrates that distance-related cues can play a significant role when listening in complex environments. © 2015 Acoustical Society of America.},
  langid       = {english}
}

@article{whiteAccessibilityMathematicalNotation2020,
  title        = {The {{Accessibility}} of {{Mathematical Notation}} on the {{Web}} and {{Beyond}}},
  author       = {White, Jason J. G.},
  date         = {2020},
  journaltitle = {Journal of Science Education for Students with Disabilities},
  volume       = {23},
  number       = {1},
  publisher    = {RIT Scholar Works},
  url          = {https://eric.ed.gov/?id=EJ1251755},
  urldate      = {2023-12-10},
  abstract     = {This paper serves two purposes. First, it offers an overview of the role of the Mathematical Markup Language (MathML) in representing mathematical notation on the Web, and its significance for accessibility. To orient the discussion, hypotheses are advanced regarding users' needs in connection with the accessibility of mathematical notation. Second, current developments in the evolution of MathML are reviewed, noting their consequences for accessibility, and commenting on prospects for future improvement in the concrete experiences of users of assistive technologies. Recommendations are advanced for further research and development activities, emphasizing the cognitive aspects of user interface design.},
  langid       = {english},
  keywords     = {Accessibility (for Disabled),Assistive Technology,Coding,Internet,Mathematics,Programming Languages,Students with Disabilities,Web Browsers},
  annotation   = {ERIC Number: EJ1251755},
  file         = {C:\Users\jseo1005\Zotero\storage\PDK2IDJM\White - 2020 - The Accessibility of Mathematical Notation on the .pdf}
}

@book{wickhamDataScienceImport2016,
  title      = {R for {{Data Science}}: {{Import}}, {{Tidy}}, {{Transform}}, {{Visualize}}, and {{Model Data}}},
  shorttitle = {R for {{Data Science}}},
  author     = {Wickham, Hadley and Grolemund, Garrett},
  date       = {2016-12-12},
  eprint     = {I6y3DQAAQBAJ},
  eprinttype = {googlebooks},
  publisher  = {"O'Reilly Media, Inc."},
  abstract   = {Learn how to use R to turn raw data into insight, knowledge, and understanding. This book introduces you to R, RStudio, and the tidyverse, a collection of R packages designed to work together to make data science fast, fluent, and fun. Suitable for readers with no previous programming experience, R for Data Science is designed to get you doing data science as quickly as possible.Authors Hadley Wickham and Garrett Grolemund guide you through the steps of importing, wrangling, exploring, and modeling your data and communicating the results. You'll get a complete, big-picture understanding of the data science cycle, along with basic tools you need to manage the details. Each section of the book is paired with exercises to help you practice what you've learned along the way.You'll learn how to:Wrangle—transform your datasets into a form convenient for analysisProgram—learn powerful R tools for solving data problems with greater clarity and easeExplore—examine your data, generate hypotheses, and quickly test themModel—provide a low-dimensional summary that captures true "signals" in your datasetCommunicate—learn R Markdown for integrating prose, code, and results},
  isbn       = {978-1-4919-1034-4},
  langid     = {english},
  pagetotal  = {474},
  keywords   = {Computers / Data Science / General,Computers / Mathematical & Statistical Software,Mathematics / Probability & Statistics / General}
}

@book{wickhamGgplot2ElegantGraphics2010,
  title      = {Ggplot2: {{Elegant Graphics}} for {{Data Analysis}}},
  shorttitle = {Ggplot2},
  author     = {Wickham, Hadley},
  date       = {2010-02-22},
  edition    = {1st ed. 2009. Corr. 3rd printing 2010 edition},
  publisher  = {Springer},
  location   = {New York},
  abstract   = {Provides both rich theory and powerful applicationsFigures are accompanied by code required to produce themFull color figures},
  isbn       = {978-0-387-98140-6},
  langid     = {english},
  pagetotal  = {213}
}

@article{wickhamLayeredGrammarGraphics2010,
  title        = {A {{Layered Grammar}} of {{Graphics}}},
  author       = {Wickham, Hadley},
  date         = {2010-01},
  journaltitle = {Journal of Computational and Graphical Statistics},
  shortjournal = {Journal of Computational and Graphical Statistics},
  volume       = {19},
  number       = {1},
  pages        = {3--28},
  issn         = {1061-8600, 1537-2715},
  doi          = {10.1198/jcgs.2009.07098},
  url          = {http://www.tandfonline.com/doi/abs/10.1198/jcgs.2009.07098},
  urldate      = {2022-09-21},
  langid       = {english},
  file         = {C:\Users\jseo1005\Zotero\storage\7MXU8HHI\Wickham - 2010 - A Layered Grammar of Graphics.pdf}
}

@article{wickhamWelcomeTidyverse2019,
  title        = {Welcome to the {{Tidyverse}}},
  author       = {Wickham, Hadley and Averick, Mara and Bryan, Jennifer and Chang, Winston and McGowan, Lucy D'Agostino and François, Romain and Grolemund, Garrett and Hayes, Alex and Henry, Lionel and Hester, Jim and Kuhn, Max and Pedersen, Thomas Lin and Miller, Evan and Bache, Stephan Milton and Müller, Kirill and Ooms, Jeroen and Robinson, David and Seidel, Dana Paige and Spinu, Vitalie and Takahashi, Kohske and Vaughan, Davis and Wilke, Claus and Woo, Kara and Yutani, Hiroaki},
  date         = {2019-11-21},
  journaltitle = {Journal of Open Source Software},
  volume       = {4},
  number       = {43},
  pages        = {1686},
  issn         = {2475-9066},
  doi          = {10.21105/joss.01686},
  url          = {https://joss.theoj.org/papers/10.21105/joss.01686},
  urldate      = {2024-04-14},
  abstract     = {Wickham et al., (2019). Welcome to the Tidyverse. Journal of Open Source Software, 4(43), 1686, https://doi.org/10.21105/joss.01686},
  langid       = {english},
  file         = {C:\Users\jseo1005\Zotero\storage\52D9DX8G\Wickham et al. - 2019 - Welcome to the Tidyverse.pdf}
}

@incollection{wilkinsonGrammarGraphics2012,
  title     = {The {{Grammar}} of {{Graphics}}},
  booktitle = {Handbook of {{Computational Statistics}}: {{Concepts}} and {{Methods}}},
  author    = {Wilkinson, Leland},
  editor    = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
  date      = {2012},
  series    = {Springer {{Handbooks}} of {{Computational Statistics}}},
  pages     = {375--414},
  publisher = {Springer},
  location  = {Berlin, Heidelberg},
  doi       = {10.1007/978-3-642-21551-3_13},
  url       = {https://doi.org/10.1007/978-3-642-21551-3_13},
  urldate   = {2023-01-24},
  abstract  = {The Grammar of Graphics, or GOG, denotes a system with seven orthogonal components. By orthogonal, we mean there are seven graphical component sets whose elements are aspects of the general system and that every combination of aspects in the product of all these sets is meaningful. This sense of the word orthogonality, a term used by computer designers to describe a combinatoric system of components or building blocks, is in some sense similar to the orthogonal factorial analysis of variance (ANOVA), where factors have levels and all possible combinations of levels exist in the ANOVA design. If we interpret each combination of features in a GOG system as a point in a network, then the world described by GOG is represented in a seven-dimensional rectangular lattice.},
  isbn      = {978-3-642-21551-3},
  langid    = {english},
  keywords  = {Algebraic Expression,Graph Function,Graphic System,Recursive Partitioning,Statistical Graphic},
  file      = {C:\Users\jseo1005\Zotero\storage\YCA8836K\Wilkinson - 2012 - The Grammar of Graphics.pdf}
}

@online{worldwidewebconsortiumUnderstandingSuccessCriterion2023,
  title   = {Understanding {{Success Criterion}} 1.1.1 | {{Understanding WCAG}} 2.0},
  author  = {{World Wide Web Consortium}},
  date    = {2023},
  url     = {https://www.w3.org/TR/UNDERSTANDING-WCAG20/text-equiv-all.html},
  urldate = {2024-04-21},
  file    = {C:\Users\jseo1005\Zotero\storage\Y9ZV9IPZ\text-equiv-all.html}
}

@inproceedings{wuDataDataEverywhere2023,
  title      = {Data, {{Data}}, {{Everywhere}}: {{Uncovering Everyday Data Experiences}} for {{People}} with {{Intellectual}} and {{Developmental Disabilities}}},
  shorttitle = {Data, {{Data}}, {{Everywhere}}},
  booktitle  = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author     = {Wu, Keke and Tran, Michelle Ho and Petersen, Emma and Koushik, Varsha and Szafir, Danielle Albers},
  date       = {2023-04-19},
  series     = {{{CHI}} '23},
  pages      = {1--17},
  publisher  = {Association for Computing Machinery},
  location   = {New York, NY, USA},
  doi        = {10.1145/3544548.3581204},
  url        = {https://dl.acm.org/doi/10.1145/3544548.3581204},
  urldate    = {2023-04-25},
  abstract   = {Data is everywhere but may not be accessible to everyone. Conventional data visualization tools and guidelines often do not actively consider the specific needs and abilities of people with Intellectual and Developmental Disabilities (IDD), leaving them excluded from data-driven activities and vulnerable to ethical issues. To understand the needs and challenges people with IDD have with data, we conducted 15 semi-structured interviews with individuals with IDD and their caregivers. Our algorithmic interview approach situated data in the lived experiences of people with IDD to uncover otherwise hidden data encounters in their everyday life. Drawing on findings and observations, we characterize how they conceptualize data, when and where they use data, and what barriers exist when they interact with data. We use our results as a lens to reimagine the role of visualization in data accessibility and establish a critical near-term research agenda for cognitively accessible visualization.},
  isbn       = {978-1-4503-9421-5},
  keywords   = {human-subjects qualitative studies,personal visual analytics},
  file       = {C:\Users\jseo1005\Zotero\storage\HUD5FMNM\Wu et al. - 2023 - Data, Data, Everywhere Uncovering Everyday Data E.pdf}
}

@online{xuLetGetVysical2023,
  title       = {Let's {{Get Vysical}}: {{Perceptual Accuracy In Visual}} and {{Tactile Encodings}}},
  shorttitle  = {Let's {{Get Vysical}}},
  author      = {Xu, Zhongzheng and Williams, Kristin and Wall, Emily},
  date        = {2023-08-08},
  eprint      = {2308.04392},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2308.04392},
  url         = {http://arxiv.org/abs/2308.04392},
  urldate     = {2023-11-06},
  abstract    = {In this paper, we explore the effectiveness of tactile data encodings using swell paper in comparison to visual encodings displayed with SVGs for data perception tasks. By replicating and adapting Cleveland and McGill's graphical perception study for the tactile modality, we establish a novel tactile encoding hierarchy. In a study with 12 university students, we found that participants perceived visual encodings more accurately when comparing values, judging their ratios with lower cognitive load, and better self-evaluated performance than tactile encodings. However, tactile encodings differed from their visual counterparts in terms of how accurately values could be decoded from them. This suggests that data physicalizations will require different design guidance than that developed for visual encodings. By providing empirical evidence for the perceptual accuracy of tactile encodings, our work contributes to foundational research on forms of data representation that prioritize tactile perception such as tactile graphics.},
  pubstate    = {preprint},
  keywords    = {Computer Science - Human-Computer Interaction},
  file        = {C\:\\Users\\jseo1005\\Zotero\\storage\\T9K8AXMG\\Xu et al. - 2023 - Let's Get Vysical Perceptual Accuracy In Visual a.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\WNE5DI6G\\2308.html}
}

@inproceedings{xuTactileDisplayVisually2011,
  title     = {Tactile Display for the Visually Impaired Using {{TeslaTouch}}},
  booktitle = {{{CHI}} '11 {{Extended Abstracts}} on {{Human Factors}} in {{Computing Systems}}},
  author    = {Xu, Cheng and Israr, Ali and Poupyrev, Ivan and Bau, Olivier and Harrison, Chris},
  date      = {2011-05-07},
  series    = {{{CHI EA}} '11},
  pages     = {317--322},
  publisher = {Association for Computing Machinery},
  location  = {New York, NY, USA},
  doi       = {10.1145/1979742.1979705},
  url       = {https://dl.acm.org/doi/10.1145/1979742.1979705},
  urldate   = {2023-11-13},
  abstract  = {TeslaTouch is a technology that provides tactile sensation to moving fingers on touch screens. Based on TeslaTouch, we have developed applications for the visually impaired to interpret and create 2D tactile information. In this paper, we demonstrate these applications, present observations from the interaction, and discuss TeslaTouch's potential in supporting communication among visually impaired individuals.},
  isbn      = {978-1-4503-0268-5},
  keywords  = {assistive technology,tactile display,TeslaTouch,visual impairment},
  file      = {C:\Users\jseo1005\Zotero\storage\6Q9YLX6J\Xu et al. - 2011 - Tactile display for the visually impaired using TeslaTouch.pdf}
}

@inproceedings{yangTactilePresentationNetwork2020,
  title      = {Tactile {{Presentation}} of {{Network Data}}: {{Text}}, {{Matrix}} or {{Diagram}}?},
  shorttitle = {Tactile {{Presentation}} of {{Network Data}}},
  booktitle  = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author     = {Yang, Yalong and Marriott, Kim and Butler, Matthew and Goncu, Cagatay and Holloway, Leona},
  date       = {2020-04-23},
  series     = {{{CHI}} '20},
  pages      = {1--12},
  publisher  = {Association for Computing Machinery},
  location   = {New York, NY, USA},
  doi        = {10.1145/3313831.3376367},
  url        = {https://doi.org/10.1145/3313831.3376367},
  urldate    = {2022-11-06},
  abstract   = {Visualisations are commonly used to understand social, biological and other kinds of networks. Currently we do not know how to effectively present network data to people who are blind or have low-vision (BLV). We ran a controlled study with 8 BLV participants comparing four tactile representations: organic node-link diagram, grid node-link diagram, adjacency matrix and braille list. We found that the node-link representations were preferred and more effective for path following and cluster identification while the matrix and list were better for adjacency tasks. This is broadly in line with findings for the corresponding visual representations.},
  isbn       = {978-1-4503-6708-0},
  keywords   = {accessibility,adjacency matrix,blindness,graphvisualization,vision impairment},
  file       = {C:\Users\jseo1005\Zotero\storage\58WSGZTQ\Yang et al. - 2020 - Tactile Presentation of Network Data Text, Matrix.pdf}
}

@incollection{yuHapticGraphsBlind2001,
  title       = {Haptic Graphs for Blind Computer Users},
  booktitle   = {Haptic {{Human-Computer Interaction}}},
  author      = {Yu, Wai and Ramloll, Ramesh and Brewster, Stephen},
  editor      = {Brewster, Stephen and Murray-Smith, Roderick},
  editora     = {Goos, Gerhard and Hartmanis, Juris and Van Leeuwen, Jan},
  editoratype = {redactor},
  date        = {2001},
  volume      = {2058},
  pages       = {41--51},
  publisher   = {Springer Berlin Heidelberg},
  location    = {Berlin, Heidelberg},
  doi         = {10.1007/3-540-44589-7_5},
  url         = {http://link.springer.com/10.1007/3-540-44589-7_5},
  urldate     = {2023-11-28},
  abstract    = {In this paper we discuss the design of computer-based haptic graphs for blind and visually impaired people with the support of our preliminary experimental results. Since visual impairment makes data visualisation techniques inappropriate for blind people, we are developing a system that can make graphs accessible through haptic and audio media. The disparity between human haptic perception and the sensation simulated by force feedback devices is discussed. Our strategies to tackle technical difficulties posed by the limitations of force feedback devices are explained. Based on the results of experiments conducted on both blind and sighted people, we suggested two techniques: engraving and the use of texture to model curved lines on haptic graphs. Integration of surface property and auditory cues in our system are proposed to assist blind users in exploring haptic graphs.},
  isbn        = {978-3-540-42356-0 978-3-540-44589-0},
  langid      = {english},
  file        = {C:\Users\jseo1005\Zotero\storage\QX4MDTLL\Yu et al. - 2001 - Haptic graphs for blind computer users.pdf}
}

@inproceedings{yuMultimodalVirtualReality2002,
  title     = {Multimodal Virtual Reality versus Printed Medium in Visualization for Blind People},
  booktitle = {Proceedings of the Fifth International {{ACM}} Conference on {{Assistive}} Technologies},
  author    = {Yu, Wai and Brewster, Stephen},
  date      = {2002-07-08},
  series    = {Assets '02},
  pages     = {57--64},
  publisher = {Association for Computing Machinery},
  location  = {New York, NY, USA},
  doi       = {10.1145/638249.638261},
  url       = {https://dl.acm.org/doi/10.1145/638249.638261},
  urldate   = {2023-11-13},
  abstract  = {In this paper, we describe a study comparing the strengths of a multimodal Virtual Reality (VR) interface against traditional tactile diagrams in conveying information to visually impaired and blind people. The multimodal VR interface consists of a force feedback device (SensAble PHANTOM), synthesized speech and non-speech audio. Potential advantages of the VR technology are well known however its real usability in comparison with the conventional paper-based medium is seldom investigated. We have addressed this issue in our evaluation. The experimental results show benefits from using the multimodal approach in terms of more accurate information about the graphs obtained by users.},
  isbn      = {978-1-58113-464-3},
  keywords  = {assistive technology,haptics,human computer interaction,multimodal interface,virtual reality},
  file      = {C:\Users\jseo1005\Zotero\storage\X7C76U32\Yu and Brewster - 2002 - Multimodal virtual reality versus printed medium in visualization for blind people.pdf}
}

@incollection{yuWebbasedMultimodalGraphs2002,
  title     = {Web-Based {{Multimodal Graphs}} for {{Visually Impaired People}}},
  booktitle = {Universal {{Access}} and {{Assistive Technology}}},
  author    = {Yu, W. and Reid, D. and Brewster, S.},
  editor    = {Keates, Simeon and Langdon, Patrick and Clarkson, P. John and Robinson, Peter},
  date      = {2002},
  pages     = {97--108},
  publisher = {Springer London},
  location  = {London},
  doi       = {10.1007/978-1-4471-3719-1_10},
  url       = {http://link.springer.com/10.1007/978-1-4471-3719-1_10},
  urldate   = {2023-11-28},
  isbn      = {978-1-4471-3721-4 978-1-4471-3719-1},
  langid    = {english},
  file      = {C:\Users\jseo1005\Zotero\storage\DGQPT5N4\Yu et al. - 2002 - Web-based Multimodal Graphs for Visually Impaired People.pdf}
}

@article{zanellaSonificationSoundDesign2022,
  title        = {Sonification and Sound Design for Astronomy Research, Education and Public Engagement},
  author       = {Zanella, A. and Harrison, C. M. and Lenzi, S. and Cooke, J. and Damsma, P. and Fleming, S. W.},
  date         = {2022-11},
  journaltitle = {Nature Astronomy},
  shortjournal = {Nat Astron},
  volume       = {6},
  number       = {11},
  pages        = {1241--1248},
  publisher    = {Nature Publishing Group},
  issn         = {2397-3366},
  doi          = {10.1038/s41550-022-01721-z},
  url          = {https://www.nature.com/articles/s41550-022-01721-z},
  urldate      = {2023-03-11},
  abstract     = {Over the past ten years there has been a large increase in the number of projects using sound to represent astronomical data and concepts. Motivation for these projects includes the potential to enhance scientific discovery within complex datasets, by utilizing the inherent multidimensionality of sound and the ability of our hearing to filter signals from noise. Other motivations include creating engaging multisensory resources, for education and public engagement, and making astronomy more accessible to people who are blind or have low vision, promoting their participation in science and related careers. We describe potential benefits of sound within these contexts and provide an overview of the nearly 100 sound-based astronomy projects that we have identified. We discuss current limitations and challenges of the approaches taken. Finally, we suggest future directions to help realize the full potential of sound-based techniques in general and to widen their application within the astronomy community.},
  issue        = {11},
  langid       = {english},
  keywords     = {Astronomy and astrophysics,Interdisciplinary studies},
  file         = {C:\Users\jseo1005\Zotero\storage\S5RZB7RG\Zanella et al. - 2022 - Sonification and sound design for astronomy resear.pdf}
}

@article{zekveldCognitiveProcessingLoad2014,
  title        = {Cognitive Processing Load during Listening Is Reduced More by Decreasing Voice Similarity than by Increasing Spatial Separation between Target and Masker Speech},
  author       = {Zekveld, Adriana A. and Rudner, Mary and Kramer, Sophia E. and Lyzenga, Johannes and Rönnberg, Jerker},
  date         = {2014-04-29},
  journaltitle = {Frontiers in Neuroscience},
  shortjournal = {Front Neurosci},
  volume       = {8},
  eprint       = {24808818},
  eprinttype   = {pmid},
  pages        = {88},
  issn         = {1662-4548},
  doi          = {10.3389/fnins.2014.00088},
  url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4010736/},
  urldate      = {2024-02-12},
  abstract     = {We investigated changes in speech recognition and cognitive processing load due to the masking release attributable to decreasing similarity between target and masker speech. This was achieved by using masker voices with either the same (female) gender as the target speech or different gender (male) and/or by spatially separating the target and masker speech using HRTFs. We assessed the relation between the signal-to-noise ratio required for 50\% sentence intelligibility, the pupil response and cognitive abilities. We hypothesized that the pupil response, a measure of cognitive processing load, would be larger for co-located maskers and for same-gender compared to different-gender maskers. We further expected that better cognitive abilities would be associated with better speech perception and larger pupil responses as the allocation of larger capacity may result in more intense mental processing. In line with previous studies, the performance benefit from different-gender compared to same-gender maskers was larger for co-located masker signals. The performance benefit of spatially-separated maskers was larger for same-gender maskers. The pupil response was larger for same-gender than for different-gender maskers, but was not reduced by spatial separation. We observed associations between better perception performance and better working memory, better information updating, and better executive abilities when applying no corrections for multiple comparisons. The pupil response was not associated with cognitive abilities. Thus, although both gender and location differences between target and masker facilitate speech perception, only gender differences lower cognitive processing load. Presenting a more dissimilar masker may facilitate target-masker separation at a later (cognitive) processing stage than increasing the spatial separation between the target and masker. The pupil response provides information about speech perception that complements intelligibility data.},
  pmcid        = {PMC4010736},
  file         = {C:\Users\jseo1005\Zotero\storage\CFTDN3N5\Zekveld et al. - 2014 - Cognitive processing load during listening is redu.pdf}
}

@online{zhu-tianGeneratingCodeEvaluating2023,
  title       = {Beyond {{Generating Code}}: {{Evaluating GPT}} on a {{Data Visualization Course}}},
  shorttitle  = {Beyond {{Generating Code}}},
  author      = {Zhu-Tian, Chen and Zhang, Chenyang and Wang, Qianwen and Troidl, Jakob and Warchol, Simon and Beyer, Johanna and Gehlenborg, Nils and Pfister, Hanspeter},
  date        = {2023-10-06},
  eprint      = {2306.02914},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2306.02914},
  url         = {http://arxiv.org/abs/2306.02914},
  urldate     = {2024-01-15},
  abstract    = {This paper presents an empirical evaluation of the performance of the Generative Pre-trained Transformer (GPT) model in Harvard's CS171 data visualization course. While previous studies have focused on GPT's ability to generate code for visualizations, this study goes beyond code generation to evaluate GPT's abilities in various visualization tasks, such as data interpretation, visualization design, visual data exploration, and insight communication. The evaluation utilized GPT-3.5 and GPT-4 to complete assignments of CS171, and included a quantitative assessment based on the established course rubrics, a qualitative analysis informed by the feedback of three experienced graders, and an exploratory study of GPT's capabilities in completing border visualization tasks. Findings show that GPT-4 scored 80\% on quizzes and homework, and TFs could distinguish between GPT- and human-generated homework with 70\% accuracy. The study also demonstrates GPT's potential in completing various visualization tasks, such as data cleanup, interaction with visualizations, and insight communication. The paper concludes by discussing the strengths and limitations of GPT in data visualization, potential avenues for incorporating GPT in broader visualization tasks, and the need to redesign visualization education.},
  pubstate    = {preprint},
  keywords    = {Computer Science - Graphics,Computer Science - Human-Computer Interaction},
  file        = {C\:\\Users\\jseo1005\\Zotero\\storage\\FAG3NAGK\\Zhu-Tian et al. - 2023 - Beyond Generating Code Evaluating GPT on a Data Visualization Course.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\2NVA2ZJC\\2306.html}
}

@article{zongRichScreenReader2022,
  title        = {Rich {{Screen Reader Experiences}} for {{Accessible Data Visualization}}},
  author       = {Zong, Jonathan and Lee, Crystal and Lundgard, Alan and Jang, JiWoong and Hajas, Daniel and Satyanarayan, Arvind},
  date         = {2022},
  journaltitle = {Computer Graphics Forum},
  volume       = {41},
  number       = {3},
  pages        = {15--27},
  issn         = {1467-8659},
  doi          = {10.1111/cgf.14519},
  url          = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14519},
  urldate      = {2024-02-25},
  abstract     = {Current web accessibility guidelines ask visualization designers to support screen readers via basic non-visual alternatives like textual descriptions and access to raw data tables. But charts do more than summarize data or reproduce tables; they afford interactive data exploration at varying levels of granularity—from fine-grained datum-by-datum reading to skimming and surfacing high-level trends. In response to the lack of comparable non-visual affordances, we present a set of rich screen reader experiences for accessible data visualization and exploration. Through an iterative co-design process, we identify three key design dimensions for expressive screen reader accessibility: structure, or how chart entities should be organized for a screen reader to traverse; navigation, or the structural, spatial, and targeted operations a user might perform to step through the structure; and, description, or the semantic content, composition, and verbosity of the screen reader's narration. We operationalize these dimensions to prototype screen-reader-accessible visualizations that cover a diverse range of chart types and combinations of our design dimensions. We evaluate a subset of these prototypes in a mixed-methods study with 13 blind and visually impaired readers. Our findings demonstrate that these designs help users conceptualize data spatially, selectively attend to data of interest at different levels of granularity, and experience control and agency over their data analysis process. An accessible HTML version of this paper is available at: http://vis.csail.mit.edu/pubs/rich-screen-reader-vis-experiences.},
  langid       = {english},
  keywords     = {• Human-centered computing → Visualization design and evaluation methods,Accessibility design and evaluation methods,CCS Concepts},
  file         = {C\:\\Users\\jseo1005\\Zotero\\storage\\IFSDZU5R\\Zong et al. - 2022 - Rich Screen Reader Experiences for Accessible Data Visualization.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\SGDWXKUI\\cgf.html}
}
